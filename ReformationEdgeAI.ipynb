{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhRFUJjUBdbE"
      },
      "source": [
        "# Plotting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-w7h8fSh-wIT"
      },
      "outputs": [],
      "source": [
        "def xyxy2xywh(x):\n",
        "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
        "    \n",
        "    # if is tensor, clonek else use numpy copy\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    \n",
        "\n",
        "    # [row, col]\n",
        "\n",
        "    \n",
        "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n",
        "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n",
        "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
        "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
        "    return y\n",
        "\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
        "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
        "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
        "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
        "    return y\n",
        "\n",
        "def fitness(x):\n",
        "    # Model fitness as a weighted combination of metrics\n",
        "    w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\n",
        "    return (x[:, :4] * w).sum(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "wWoutvHr9cWw"
      },
      "outputs": [],
      "source": [
        "# Plotting utils\n",
        "\n",
        "import glob\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "from copy import copy\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import yaml\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "\n",
        "# from utils.general import xywh2xyxy, xyxy2xywh\n",
        "# from utils.metrics import fitness\n",
        "\n",
        "# Settings\n",
        "matplotlib.rc('font', **{'size': 11})\n",
        "\n",
        "# for writing to files only\n",
        "# writing the result of matplotlib into the file\n",
        "matplotlib.use('Agg')\n",
        "\n",
        "\n",
        "class Colors:\n",
        "    # Ultralytics color palette https://ultralytics.com/\n",
        "    def __init__(self):\n",
        "        # hex = matplotlib.colors.TABLEAU_COLORS.values()\n",
        "        hex = ('FF3838', 'FF9D97', 'FF701F', 'FFB21D', 'CFD231', '48F90A', '92CC17', '3DDB86', '1A9334', '00D4BB',\n",
        "               '2C99A8', '00C2FF', '344593', '6473FF', '0018EC', '8438FF', '520085', 'CB38FF', 'FF95C8', 'FF37C7')\n",
        "        self.palette = [self.hex2rgb('#' + c) for c in hex]\n",
        "        self.n = len(self.palette)\n",
        "\n",
        "    def __call__(self, i, bgr=False):\n",
        "        c = self.palette[int(i) % self.n]\n",
        "        return (c[2], c[1], c[0]) if bgr else c\n",
        "\n",
        "    @staticmethod\n",
        "    def hex2rgb(h):  # rgb order (PIL)\n",
        "        return tuple(int(h[1 + i:1 + i + 2], 16) for i in (0, 2, 4))\n",
        "\n",
        "\n",
        "colors = Colors()  # create instance for 'from utils.plots import colors'\n",
        "\n",
        "\n",
        "def hist2d(x, y, n=100):\n",
        "    # 2d histogram used in labels.png and evolve.png\n",
        "\n",
        "    # Return evenly spaced numbers over a specified interval.Returns num evenly spaced samples, calculated over the interval [start, stop].The endpoint of the interval can optionally be excluded.\n",
        "    #     np.linspace(2.0, 3.0, num=5)\n",
        "    # input  : start=2, end=3, size=5\n",
        "    # output : [2.  , 2.25, 2.5 , 2.75, 3.  ]\n",
        "    # array([2.  , 2.25, 2.5 , 2.75, 3.  ])\n",
        "\n",
        "    xedges, yedges = np.linspace(x.min(), x.max(), n), np.linspace(y.min(), y.max(), n)\n",
        "\n",
        "    \n",
        "    hist, xedges, yedges = np.histogram2d(x, y, (xedges, yedges))\n",
        "    xidx = np.clip(np.digitize(x, xedges) - 1, 0, hist.shape[0] - 1)\n",
        "    yidx = np.clip(np.digitize(y, yedges) - 1, 0, hist.shape[1] - 1)\n",
        "    return np.log(hist[xidx, yidx])\n",
        "\n",
        "\n",
        "def butter_lowpass_filtfilt(data, cutoff=1500, fs=50000, order=5):\n",
        "    from scipy.signal import butter, filtfilt\n",
        "\n",
        "    # https://stackoverflow.com/questions/28536191/how-to-filter-smooth-with-scipy-numpy\n",
        "    def butter_lowpass(cutoff, fs, order):\n",
        "        nyq = 0.5 * fs\n",
        "        normal_cutoff = cutoff / nyq\n",
        "        return butter(order, normal_cutoff, btype='low', analog=False)\n",
        "\n",
        "    b, a = butter_lowpass(cutoff, fs, order=order)\n",
        "    return filtfilt(b, a, data)  # forward-backward filter\n",
        "\n",
        "\n",
        "def plot_one_box(x, im, color=None, label=None, line_thickness=3):\n",
        "    # Plots one bounding box on image 'im' using OpenCV\n",
        "    assert im.data.contiguous, 'Image not contiguous. Apply np.ascontiguousarray(im) to plot_on_box() input image.'\n",
        "    tl = line_thickness or round(0.002 * (im.shape[0] + im.shape[1]) / 2) + 1  # line/font thickness\n",
        "    color = color or [random.randint(0, 255) for _ in range(3)]\n",
        "    c1, c2 = (int(x[0]), int(x[1])), (int(x[2]), int(x[3]))\n",
        "    cv2.rectangle(im, c1, c2, color, thickness=tl, lineType=cv2.LINE_AA)\n",
        "    if label:\n",
        "        tf = max(tl - 1, 1)  # font thickness\n",
        "        t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "        c2 = c1[0] + t_size[0], c1[1] - t_size[1] - 3\n",
        "        cv2.rectangle(im, c1, c2, color, -1, cv2.LINE_AA)  # filled\n",
        "        cv2.putText(im, label, (c1[0], c1[1] - 2), 0, tl / 3, [225, 255, 255], thickness=tf, lineType=cv2.LINE_AA)\n",
        "\n",
        "\n",
        "def plot_one_box_PIL(box, im, color=None, label=None, line_thickness=None):\n",
        "    # Plots one bounding box on image 'im' using PIL\n",
        "    im = Image.fromarray(im)\n",
        "    draw = ImageDraw.Draw(im)\n",
        "    line_thickness = line_thickness or max(int(min(im.size) / 200), 2)\n",
        "    draw.rectangle(box, width=line_thickness, outline=tuple(color))  # plot\n",
        "    if label:\n",
        "        fontsize = max(round(max(im.size) / 40), 12)\n",
        "        font = ImageFont.truetype(\"Arial.ttf\", fontsize)\n",
        "        txt_width, txt_height = font.getsize(label)\n",
        "        draw.rectangle([box[0], box[1] - txt_height + 4, box[0] + txt_width, box[1]], fill=tuple(color))\n",
        "        draw.text((box[0], box[1] - txt_height + 1), label, fill=(255, 255, 255), font=font)\n",
        "    return np.asarray(im)\n",
        "\n",
        "\n",
        "def plot_wh_methods():  # from utils.plots import *; plot_wh_methods()\n",
        "    # Compares the two methods for width-height anchor multiplication\n",
        "    # https://github.com/ultralytics/yolov3/issues/168\n",
        "    x = np.arange(-4.0, 4.0, .1)\n",
        "    ya = np.exp(x)\n",
        "    yb = torch.sigmoid(torch.from_numpy(x)).numpy() * 2\n",
        "\n",
        "    fig = plt.figure(figsize=(6, 3), tight_layout=True)\n",
        "    plt.plot(x, ya, '.-', label='YOLOv3')\n",
        "    plt.plot(x, yb ** 2, '.-', label='YOLOv5 ^2')\n",
        "    plt.plot(x, yb ** 1.6, '.-', label='YOLOv5 ^1.6')\n",
        "    plt.xlim(left=-4, right=4)\n",
        "    plt.ylim(bottom=0, top=6)\n",
        "    plt.xlabel('input')\n",
        "    plt.ylabel('output')\n",
        "    plt.grid()\n",
        "    plt.legend()\n",
        "    fig.savefig('comparison.png', dpi=200)\n",
        "\n",
        "\n",
        "def output_to_target(output):\n",
        "    # Convert model output to target format [batch_id, class_id, x, y, w, h, conf]\n",
        "    targets = []\n",
        "    for i, o in enumerate(output):\n",
        "        for *box, conf, cls in o.cpu().numpy():\n",
        "            targets.append([i, cls, *list(*xyxy2xywh(np.array(box)[None])), conf])\n",
        "    return np.array(targets)\n",
        "\n",
        "\n",
        "def plot_images(images, targets, paths=None, fname='images.jpg', names=None, max_size=640, max_subplots=16):\n",
        "    # Plot image grid with labels\n",
        "\n",
        "    if isinstance(images, torch.Tensor):\n",
        "        images = images.cpu().float().numpy()\n",
        "    if isinstance(targets, torch.Tensor):\n",
        "        targets = targets.cpu().numpy()\n",
        "\n",
        "    # un-normalise\n",
        "    if np.max(images[0]) <= 1:\n",
        "        images *= 255\n",
        "\n",
        "    tl = 3  # line thickness\n",
        "    tf = max(tl - 1, 1)  # font thickness\n",
        "    bs, _, h, w = images.shape  # batch size, _, height, width\n",
        "    bs = min(bs, max_subplots)  # limit plot images\n",
        "    ns = np.ceil(bs ** 0.5)  # number of subplots (square)\n",
        "\n",
        "    # Check if we should resize\n",
        "    scale_factor = max_size / max(h, w)\n",
        "    if scale_factor < 1:\n",
        "        h = math.ceil(scale_factor * h)\n",
        "        w = math.ceil(scale_factor * w)\n",
        "\n",
        "    mosaic = np.full((int(ns * h), int(ns * w), 3), 255, dtype=np.uint8)  # init\n",
        "    for i, img in enumerate(images):\n",
        "        if i == max_subplots:  # if last batch has fewer images than we expect\n",
        "            break\n",
        "\n",
        "        block_x = int(w * (i // ns))\n",
        "        block_y = int(h * (i % ns))\n",
        "\n",
        "        img = img.transpose(1, 2, 0)\n",
        "        if scale_factor < 1:\n",
        "            img = cv2.resize(img, (w, h))\n",
        "\n",
        "        mosaic[block_y:block_y + h, block_x:block_x + w, :] = img\n",
        "        if len(targets) > 0:\n",
        "            image_targets = targets[targets[:, 0] == i]\n",
        "            boxes = xywh2xyxy(image_targets[:, 2:6]).T\n",
        "            classes = image_targets[:, 1].astype('int')\n",
        "            labels = image_targets.shape[1] == 6  # labels if no conf column\n",
        "            conf = None if labels else image_targets[:, 6]  # check for confidence presence (label vs pred)\n",
        "\n",
        "            if boxes.shape[1]:\n",
        "                if boxes.max() <= 1.01:  # if normalized with tolerance 0.01\n",
        "                    boxes[[0, 2]] *= w  # scale to pixels\n",
        "                    boxes[[1, 3]] *= h\n",
        "                elif scale_factor < 1:  # absolute coords need scale if image scales\n",
        "                    boxes *= scale_factor\n",
        "            boxes[[0, 2]] += block_x\n",
        "            boxes[[1, 3]] += block_y\n",
        "            for j, box in enumerate(boxes.T):\n",
        "                cls = int(classes[j])\n",
        "                color = colors(cls)\n",
        "                cls = names[cls] if names else cls\n",
        "                if labels or conf[j] > 0.25:  # 0.25 conf thresh\n",
        "                    label = '%s' % cls if labels else '%s %.1f' % (cls, conf[j])\n",
        "                    plot_one_box(box, mosaic, label=label, color=color, line_thickness=tl)\n",
        "\n",
        "        # Draw image filename labels\n",
        "        if paths:\n",
        "            label = Path(paths[i]).name[:40]  # trim to 40 char\n",
        "            t_size = cv2.getTextSize(label, 0, fontScale=tl / 3, thickness=tf)[0]\n",
        "            cv2.putText(mosaic, label, (block_x + 5, block_y + t_size[1] + 5), 0, tl / 3, [220, 220, 220], thickness=tf,\n",
        "                        lineType=cv2.LINE_AA)\n",
        "\n",
        "        # Image border\n",
        "        cv2.rectangle(mosaic, (block_x, block_y), (block_x + w, block_y + h), (255, 255, 255), thickness=3)\n",
        "\n",
        "    if fname:\n",
        "        r = min(1280. / max(h, w) / ns, 1.0)  # ratio to limit image size\n",
        "        mosaic = cv2.resize(mosaic, (int(ns * w * r), int(ns * h * r)), interpolation=cv2.INTER_AREA)\n",
        "        # cv2.imwrite(fname, cv2.cvtColor(mosaic, cv2.COLOR_BGR2RGB))  # cv2 save\n",
        "        Image.fromarray(mosaic).save(fname)  # PIL save\n",
        "    return mosaic\n",
        "\n",
        "\n",
        "def plot_lr_scheduler(optimizer, scheduler, epochs=300, save_dir=''):\n",
        "    # Plot LR simulating training for full epochs\n",
        "    optimizer, scheduler = copy(optimizer), copy(scheduler)  # do not modify originals\n",
        "    y = []\n",
        "    for _ in range(epochs):\n",
        "        scheduler.step()\n",
        "        y.append(optimizer.param_groups[0]['lr'])\n",
        "    plt.plot(y, '.-', label='LR')\n",
        "    plt.xlabel('epoch')\n",
        "    plt.ylabel('LR')\n",
        "    plt.grid()\n",
        "    plt.xlim(0, epochs)\n",
        "    plt.ylim(0)\n",
        "    plt.savefig(Path(save_dir) / 'LR.png', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "\n",
        "def plot_test_txt():  # from utils.plots import *; plot_test()\n",
        "    # Plot test.txt histograms\n",
        "    x = np.loadtxt('test.txt', dtype=np.float32)\n",
        "    box = xyxy2xywh(x[:, :4])\n",
        "    cx, cy = box[:, 0], box[:, 1]\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 6), tight_layout=True)\n",
        "    ax.hist2d(cx, cy, bins=600, cmax=10, cmin=0)\n",
        "    ax.set_aspect('equal')\n",
        "    plt.savefig('hist2d.png', dpi=300)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 2, figsize=(12, 6), tight_layout=True)\n",
        "    ax[0].hist(cx, bins=600)\n",
        "    ax[1].hist(cy, bins=600)\n",
        "    plt.savefig('hist1d.png', dpi=200)\n",
        "\n",
        "\n",
        "def plot_targets_txt():  # from utils.plots import *; plot_targets_txt()\n",
        "    # Plot targets.txt histograms\n",
        "    x = np.loadtxt('targets.txt', dtype=np.float32).T\n",
        "    s = ['x targets', 'y targets', 'width targets', 'height targets']\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)\n",
        "    ax = ax.ravel()\n",
        "    for i in range(4):\n",
        "        ax[i].hist(x[i], bins=100, label='%.3g +/- %.3g' % (x[i].mean(), x[i].std()))\n",
        "        ax[i].legend()\n",
        "        ax[i].set_title(s[i])\n",
        "    plt.savefig('targets.jpg', dpi=200)\n",
        "\n",
        "\n",
        "def plot_study_txt(path='', x=None):  # from utils.plots import *; plot_study_txt()\n",
        "    # Plot study.txt generated by test.py\n",
        "    fig, ax = plt.subplots(2, 4, figsize=(10, 6), tight_layout=True)\n",
        "    # ax = ax.ravel()\n",
        "\n",
        "    fig2, ax2 = plt.subplots(1, 1, figsize=(8, 4), tight_layout=True)\n",
        "    # for f in [Path(path) / f'study_coco_{x}.txt' for x in ['yolov5s6', 'yolov5m6', 'yolov5l6', 'yolov5x6']]:\n",
        "    for f in sorted(Path(path).glob('study*.txt')):\n",
        "        y = np.loadtxt(f, dtype=np.float32, usecols=[0, 1, 2, 3, 7, 8, 9], ndmin=2).T\n",
        "        x = np.arange(y.shape[1]) if x is None else np.array(x)\n",
        "        s = ['P', 'R', 'mAP@.5', 'mAP@.5:.95', 't_inference (ms/img)', 't_NMS (ms/img)', 't_total (ms/img)']\n",
        "        # for i in range(7):\n",
        "        #     ax[i].plot(x, y[i], '.-', linewidth=2, markersize=8)\n",
        "        #     ax[i].set_title(s[i])\n",
        "\n",
        "        j = y[3].argmax() + 1\n",
        "        ax2.plot(y[6, 1:j], y[3, 1:j] * 1E2, '.-', linewidth=2, markersize=8,\n",
        "                 label=f.stem.replace('study_coco_', '').replace('yolo', 'YOLO'))\n",
        "\n",
        "    ax2.plot(1E3 / np.array([209, 140, 97, 58, 35, 18]), [34.6, 40.5, 43.0, 47.5, 49.7, 51.5],\n",
        "             'k.-', linewidth=2, markersize=8, alpha=.25, label='EfficientDet')\n",
        "\n",
        "    ax2.grid(alpha=0.2)\n",
        "    ax2.set_yticks(np.arange(20, 60, 5))\n",
        "    ax2.set_xlim(0, 57)\n",
        "    ax2.set_ylim(30, 55)\n",
        "    ax2.set_xlabel('GPU Speed (ms/img)')\n",
        "    ax2.set_ylabel('COCO AP val')\n",
        "    ax2.legend(loc='lower right')\n",
        "    plt.savefig(str(Path(path).name) + '.png', dpi=300)\n",
        "\n",
        "\n",
        "def plot_labels(labels, names=(), save_dir=Path(''), loggers=None):\n",
        "    # plot dataset labels\n",
        "    print('Plotting labels... ')\n",
        "    c, b = labels[:, 0], labels[:, 1:].transpose()  # classes, boxes\n",
        "    nc = int(c.max() + 1)  # number of classes\n",
        "    x = pd.DataFrame(b.transpose(), columns=['x', 'y', 'width', 'height'])\n",
        "\n",
        "    # seaborn correlogram\n",
        "    sns.pairplot(x, corner=True, diag_kind='auto', kind='hist', diag_kws=dict(bins=50), plot_kws=dict(pmax=0.9))\n",
        "    plt.savefig(save_dir / 'labels_correlogram.jpg', dpi=200)\n",
        "    plt.close()\n",
        "\n",
        "    # matplotlib labels\n",
        "    matplotlib.use('svg')  # faster\n",
        "    ax = plt.subplots(2, 2, figsize=(8, 8), tight_layout=True)[1].ravel()\n",
        "    y = ax[0].hist(c, bins=np.linspace(0, nc, nc + 1) - 0.5, rwidth=0.8)\n",
        "    [y[2].patches[i].set_color([x / 255 for x in colors(i)]) for i in range(nc)]  # update colors\n",
        "    ax[0].set_ylabel('instances')\n",
        "    if 0 < len(names) < 30:\n",
        "        ax[0].set_xticks(range(len(names)))\n",
        "        ax[0].set_xticklabels(names, rotation=90, fontsize=10)\n",
        "    else:\n",
        "        ax[0].set_xlabel('classes')\n",
        "    sns.histplot(x, x='x', y='y', ax=ax[2], bins=50, pmax=0.9)\n",
        "    sns.histplot(x, x='width', y='height', ax=ax[3], bins=50, pmax=0.9)\n",
        "\n",
        "    # rectangles\n",
        "    labels[:, 1:3] = 0.5  # center\n",
        "    labels[:, 1:] = xywh2xyxy(labels[:, 1:]) * 2000\n",
        "    img = Image.fromarray(np.ones((2000, 2000, 3), dtype=np.uint8) * 255)\n",
        "    for cls, *box in labels[:1000]:\n",
        "        ImageDraw.Draw(img).rectangle(box, width=1, outline=colors(cls))  # plot\n",
        "    ax[1].imshow(img)\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    for a in [0, 1, 2, 3]:\n",
        "        for s in ['top', 'right', 'left', 'bottom']:\n",
        "            ax[a].spines[s].set_visible(False)\n",
        "\n",
        "    plt.savefig(save_dir / 'labels.jpg', dpi=200)\n",
        "    matplotlib.use('Agg')\n",
        "    plt.close()\n",
        "\n",
        "    # loggers\n",
        "    for k, v in loggers.items() or {}:\n",
        "        if k == 'wandb' and v:\n",
        "            v.log({\"Labels\": [v.Image(str(x), caption=x.name) for x in save_dir.glob('*labels*.jpg')]}, commit=False)\n",
        "\n",
        "\n",
        "def plot_evolution(yaml_file='data/hyp.finetune.yaml'):  # from utils.plots import *; plot_evolution()\n",
        "    # Plot hyperparameter evolution results in evolve.txt\n",
        "    with open(yaml_file) as f:\n",
        "        hyp = yaml.safe_load(f)\n",
        "    x = np.loadtxt('evolve.txt', ndmin=2)\n",
        "    f = fitness(x)\n",
        "    # weights = (f - f.min()) ** 2  # for weighted results\n",
        "    plt.figure(figsize=(10, 12), tight_layout=True)\n",
        "    matplotlib.rc('font', **{'size': 8})\n",
        "    for i, (k, v) in enumerate(hyp.items()):\n",
        "        y = x[:, i + 7]\n",
        "        # mu = (y * weights).sum() / weights.sum()  # best weighted result\n",
        "        mu = y[f.argmax()]  # best single result\n",
        "        plt.subplot(6, 5, i + 1)\n",
        "        plt.scatter(y, f, c=hist2d(y, f, 20), cmap='viridis', alpha=.8, edgecolors='none')\n",
        "        plt.plot(mu, f.max(), 'k+', markersize=15)\n",
        "        plt.title('%s = %.3g' % (k, mu), fontdict={'size': 9})  # limit to 40 characters\n",
        "        if i % 5 != 0:\n",
        "            plt.yticks([])\n",
        "        print('%15s: %.3g' % (k, mu))\n",
        "    plt.savefig('evolve.png', dpi=200)\n",
        "    print('\\nPlot saved as evolve.png')\n",
        "\n",
        "\n",
        "def profile_idetection(start=0, stop=0, labels=(), save_dir=''):\n",
        "    # Plot iDetection '*.txt' per-image logs. from utils.plots import *; profile_idetection()\n",
        "    ax = plt.subplots(2, 4, figsize=(12, 6), tight_layout=True)[1].ravel()\n",
        "    s = ['Images', 'Free Storage (GB)', 'RAM Usage (GB)', 'Battery', 'dt_raw (ms)', 'dt_smooth (ms)', 'real-world FPS']\n",
        "    files = list(Path(save_dir).glob('frames*.txt'))\n",
        "    for fi, f in enumerate(files):\n",
        "        try:\n",
        "            results = np.loadtxt(f, ndmin=2).T[:, 90:-30]  # clip first and last rows\n",
        "            n = results.shape[1]  # number of rows\n",
        "            x = np.arange(start, min(stop, n) if stop else n)\n",
        "            results = results[:, x]\n",
        "            t = (results[0] - results[0].min())  # set t0=0s\n",
        "            results[0] = x\n",
        "            for i, a in enumerate(ax):\n",
        "                if i < len(results):\n",
        "                    label = labels[fi] if len(labels) else f.stem.replace('frames_', '')\n",
        "                    a.plot(t, results[i], marker='.', label=label, linewidth=1, markersize=5)\n",
        "                    a.set_title(s[i])\n",
        "                    a.set_xlabel('time (s)')\n",
        "                    # if fi == len(files) - 1:\n",
        "                    #     a.set_ylim(bottom=0)\n",
        "                    for side in ['top', 'right']:\n",
        "                        a.spines[side].set_visible(False)\n",
        "                else:\n",
        "                    a.remove()\n",
        "        except Exception as e:\n",
        "            print('Warning: Plotting error for %s; %s' % (f, e))\n",
        "\n",
        "    ax[1].legend()\n",
        "    plt.savefig(Path(save_dir) / 'idetection_profile.png', dpi=200)\n",
        "\n",
        "\n",
        "def plot_results_overlay(start=0, stop=0):  # from utils.plots import *; plot_results_overlay()\n",
        "    # Plot training 'results*.txt', overlaying train and val losses\n",
        "    s = ['train', 'train', 'train', 'Precision', 'mAP@0.5', 'val', 'val', 'val', 'Recall', 'mAP@0.5:0.95']  # legends\n",
        "    t = ['Box', 'Objectness', 'Classification', 'P-R', 'mAP-F1']  # titles\n",
        "    for f in sorted(glob.glob('results*.txt') + glob.glob('../../Downloads/results*.txt')):\n",
        "        results = np.loadtxt(f, usecols=[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin=2).T\n",
        "        n = results.shape[1]  # number of rows\n",
        "        x = range(start, min(stop, n) if stop else n)\n",
        "        fig, ax = plt.subplots(1, 5, figsize=(14, 3.5), tight_layout=True)\n",
        "        ax = ax.ravel()\n",
        "        for i in range(5):\n",
        "            for j in [i, i + 5]:\n",
        "                y = results[j, x]\n",
        "                ax[i].plot(x, y, marker='.', label=s[j])\n",
        "                # y_smooth = butter_lowpass_filtfilt(y)\n",
        "                # ax[i].plot(x, np.gradient(y_smooth), marker='.', label=s[j])\n",
        "\n",
        "            ax[i].set_title(t[i])\n",
        "            ax[i].legend()\n",
        "            ax[i].set_ylabel(f) if i == 0 else None  # add filename\n",
        "        fig.savefig(f.replace('.txt', '.png'), dpi=200)\n",
        "\n",
        "\n",
        "def plot_results(start=0, stop=0, bucket='', id=(), labels=(), save_dir=''):\n",
        "    # Plot training 'results*.txt'. from utils.plots import *; plot_results(save_dir='runs/train/exp')\n",
        "    fig, ax = plt.subplots(2, 5, figsize=(12, 6), tight_layout=True)\n",
        "    ax = ax.ravel()\n",
        "    s = ['Box', 'Objectness', 'Classification', 'Precision', 'Recall',\n",
        "         'val Box', 'val Objectness', 'val Classification', 'mAP@0.5', 'mAP@0.5:0.95']\n",
        "    if bucket:\n",
        "        # files = ['https://storage.googleapis.com/%s/results%g.txt' % (bucket, x) for x in id]\n",
        "        files = ['results%g.txt' % x for x in id]\n",
        "        c = ('gsutil cp ' + '%s ' * len(files) + '.') % tuple('gs://%s/results%g.txt' % (bucket, x) for x in id)\n",
        "        os.system(c)\n",
        "    else:\n",
        "        files = list(Path(save_dir).glob('results*.txt'))\n",
        "    assert len(files), 'No results.txt files found in %s, nothing to plot.' % os.path.abspath(save_dir)\n",
        "    for fi, f in enumerate(files):\n",
        "        try:\n",
        "            results = np.loadtxt(f, usecols=[2, 3, 4, 8, 9, 12, 13, 14, 10, 11], ndmin=2).T\n",
        "            n = results.shape[1]  # number of rows\n",
        "            x = range(start, min(stop, n) if stop else n)\n",
        "            for i in range(10):\n",
        "                y = results[i, x]\n",
        "                if i in [0, 1, 2, 5, 6, 7]:\n",
        "                    y[y == 0] = np.nan  # don't show zero loss values\n",
        "                    # y /= y[0]  # normalize\n",
        "                label = labels[fi] if len(labels) else f.stem\n",
        "                ax[i].plot(x, y, marker='.', label=label, linewidth=2, markersize=8)\n",
        "                ax[i].set_title(s[i])\n",
        "                # if i in [5, 6, 7]:  # share train and val loss y axes\n",
        "                #     ax[i].get_shared_y_axes().join(ax[i], ax[i - 5])\n",
        "        except Exception as e:\n",
        "            print('Warning: Plotting error for %s; %s' % (f, e))\n",
        "\n",
        "    ax[1].legend()\n",
        "    fig.savefig(Path(save_dir) / 'results.png', dpi=200)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HOJUWzzSBrHU"
      },
      "source": [
        "# TorchUtils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yVCSK2XTB7AF",
        "outputId": "9c5650ca-a942-498f-e79a-405b89aeba1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting thop\n",
            "  Downloading thop-0.1.1.post2209072238-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: torch in c:\\users\\teo shi han\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from thop) (2.0.0)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\teo shi han\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->thop) (3.1.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\teo shi han\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->thop) (3.0)\n",
            "Requirement already satisfied: typing-extensions in c:\\users\\teo shi han\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->thop) (4.0.0)\n",
            "Requirement already satisfied: filelock in c:\\users\\teo shi han\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->thop) (3.10.0)\n",
            "Requirement already satisfied: sympy in c:\\users\\teo shi han\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch->thop) (1.11.1)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\teo shi han\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from jinja2->torch->thop) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in c:\\users\\teo shi han\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from sympy->torch->thop) (1.3.0)\n",
            "Installing collected packages: thop\n",
            "Successfully installed thop-0.1.1.post2209072238\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the 'c:\\users\\teo shi han\\appdata\\local\\programs\\python\\python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "!pip install thop\n",
        "# A tool to count the FLOPs of PyTorch model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1wSlr8pCBni6"
      },
      "outputs": [],
      "source": [
        "# YOLOv5 PyTorch utils\n",
        "\n",
        "import datetime\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import platform\n",
        "import subprocess\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "try:\n",
        "    import thop  # for FLOPS computation\n",
        "except ImportError:\n",
        "    thop = None\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def torch_distributed_zero_first(local_rank: int):\n",
        "    \"\"\"\n",
        "    Decorator to make all processes in distributed training wait for each local_master to do something.\n",
        "    \"\"\"\n",
        "    if local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()\n",
        "    yield\n",
        "    if local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "\n",
        "def init_torch_seeds(seed=0):\n",
        "    # Speed-reproducibility tradeoff https://pytorch.org/docs/stable/notes/randomness.html\n",
        "    torch.manual_seed(seed)\n",
        "    if seed == 0:  # slower, more reproducible\n",
        "        cudnn.benchmark, cudnn.deterministic = False, True\n",
        "    else:  # faster, less reproducible\n",
        "        cudnn.benchmark, cudnn.deterministic = True, False\n",
        "\n",
        "\n",
        "# def date_modified(path=__file__):\n",
        "#     # return human-readable file modification date, i.e. '2021-3-26'\n",
        "#     t = datetime.datetime.fromtimestamp(Path(path).stat().st_mtime)\n",
        "#     return f'{t.year}-{t.month}-{t.day}'\n",
        "\n",
        "\n",
        "# def git_describe(path=Path(__file__).parent):  # path must be a directory\n",
        "#     # return human-readable git description, i.e. v5.0-5-g3e25f1e https://git-scm.com/docs/git-describe\n",
        "#     s = f'git -C {path} describe --tags --long --always'\n",
        "#     try:\n",
        "#         return subprocess.check_output(s, shell=True, stderr=subprocess.STDOUT).decode()[:-1]\n",
        "#     except subprocess.CalledProcessError as e:\n",
        "#         return ''  # not a git repository\n",
        "\n",
        "\n",
        "def select_device(device='', batch_size=None):\n",
        "    # device = 'cpu' or '0' or '0,1,2,3'\n",
        "    # s = f'YOLOv5 ðŸš€ {git_describe() or date_modified()} torch {torch.__version__} '  # string\n",
        "    cpu = device.lower() == 'cpu'\n",
        "    if cpu:\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = '-1'  # force torch.cuda.is_available() = False\n",
        "    elif device:  # non-cpu device requested\n",
        "        os.environ['CUDA_VISIBLE_DEVICES'] = device  # set environment variable\n",
        "        assert torch.cuda.is_available(), f'CUDA unavailable, invalid device {device} requested'  # check availability\n",
        "\n",
        "    cuda = not cpu and torch.cuda.is_available()\n",
        "    if cuda:\n",
        "        n = torch.cuda.device_count()\n",
        "        if n > 1 and batch_size:  # check that batch_size is compatible with device_count\n",
        "            assert batch_size % n == 0, f'batch-size {batch_size} not multiple of GPU count {n}'\n",
        "        space = ' ' * len(s)\n",
        "        for i, d in enumerate(device.split(',') if device else range(n)):\n",
        "            p = torch.cuda.get_device_properties(i)\n",
        "            s += f\"{'' if i == 0 else space}CUDA:{d} ({p.name}, {p.total_memory / 1024 ** 2}MB)\\n\"  # bytes to MB\n",
        "    else:\n",
        "        # s += 'CPU\\n'\n",
        "        pass\n",
        "\n",
        "    # logger.info(s.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else s)  # emoji-safe\n",
        "    return torch.device('cuda:0' if cuda else 'cpu')\n",
        "\n",
        "\n",
        "def time_synchronized():\n",
        "    # pytorch-accurate time\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.synchronize()\n",
        "    return time.time()\n",
        "\n",
        "\n",
        "def profile(x, ops, n=100, device=None):\n",
        "    # profile a pytorch module or list of modules. Example usage:\n",
        "    #     x = torch.randn(16, 3, 640, 640)  # input\n",
        "    #     m1 = lambda x: x * torch.sigmoid(x)\n",
        "    #     m2 = nn.SiLU()\n",
        "    #     profile(x, [m1, m2], n=100)  # profile speed over 100 iterations\n",
        "\n",
        "    device = device or torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "    x = x.to(device)\n",
        "    x.requires_grad = True\n",
        "    print(torch.__version__, device.type, torch.cuda.get_device_properties(0) if device.type == 'cuda' else '')\n",
        "    print(f\"\\n{'Params':>12s}{'GFLOPS':>12s}{'forward (ms)':>16s}{'backward (ms)':>16s}{'input':>24s}{'output':>24s}\")\n",
        "    for m in ops if isinstance(ops, list) else [ops]:\n",
        "        m = m.to(device) if hasattr(m, 'to') else m  # device\n",
        "        m = m.half() if hasattr(m, 'half') and isinstance(x, torch.Tensor) and x.dtype is torch.float16 else m  # type\n",
        "        dtf, dtb, t = 0., 0., [0., 0., 0.]  # dt forward, backward\n",
        "        try:\n",
        "            flops = thop.profile(m, inputs=(x,), verbose=False)[0] / 1E9 * 2  # GFLOPS\n",
        "        except:\n",
        "            flops = 0\n",
        "\n",
        "        for _ in range(n):\n",
        "            t[0] = time_synchronized()\n",
        "            y = m(x)\n",
        "            t[1] = time_synchronized()\n",
        "            try:\n",
        "                _ = y.sum().backward()\n",
        "                t[2] = time_synchronized()\n",
        "            except:  # no backward method\n",
        "                t[2] = float('nan')\n",
        "            dtf += (t[1] - t[0]) * 1000 / n  # ms per op forward\n",
        "            dtb += (t[2] - t[1]) * 1000 / n  # ms per op backward\n",
        "\n",
        "        s_in = tuple(x.shape) if isinstance(x, torch.Tensor) else 'list'\n",
        "        s_out = tuple(y.shape) if isinstance(y, torch.Tensor) else 'list'\n",
        "        p = sum(list(x.numel() for x in m.parameters())) if isinstance(m, nn.Module) else 0  # parameters\n",
        "        print(f'{p:12}{flops:12.4g}{dtf:16.4g}{dtb:16.4g}{str(s_in):>24s}{str(s_out):>24s}')\n",
        "\n",
        "\n",
        "def is_parallel(model):\n",
        "    return type(model) in (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel)\n",
        "\n",
        "\n",
        "def intersect_dicts(da, db, exclude=()):\n",
        "    # Dictionary intersection of matching keys and shapes, omitting 'exclude' keys, using da values\n",
        "    return {k: v for k, v in da.items() if k in db and not any(x in k for x in exclude) and v.shape == db[k].shape}\n",
        "\n",
        "\n",
        "def initialize_weights(model):\n",
        "    for m in model.modules():\n",
        "        t = type(m)\n",
        "        if t is nn.Conv2d:\n",
        "            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "        elif t is nn.BatchNorm2d:\n",
        "            m.eps = 1e-3\n",
        "            m.momentum = 0.03\n",
        "        elif t in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6]:\n",
        "            m.inplace = True\n",
        "\n",
        "\n",
        "def find_modules(model, mclass=nn.Conv2d):\n",
        "    # Finds layer indices matching module class 'mclass'\n",
        "    return [i for i, m in enumerate(model.module_list) if isinstance(m, mclass)]\n",
        "\n",
        "\n",
        "def sparsity(model):\n",
        "    # Return global model sparsity\n",
        "    a, b = 0., 0.\n",
        "    for p in model.parameters():\n",
        "        a += p.numel()\n",
        "        b += (p == 0).sum()\n",
        "    return b / a\n",
        "\n",
        "\n",
        "def prune(model, amount=0.3):\n",
        "    # Prune model to requested global sparsity\n",
        "    import torch.nn.utils.prune as prune\n",
        "    print('Pruning model... ', end='')\n",
        "    for name, m in model.named_modules():\n",
        "        if isinstance(m, nn.Conv2d):\n",
        "            prune.l1_unstructured(m, name='weight', amount=amount)  # prune\n",
        "            prune.remove(m, 'weight')  # make permanent\n",
        "    print(' %.3g global sparsity' % sparsity(model))\n",
        "\n",
        "\n",
        "def fuse_conv_and_bn(conv, bn):\n",
        "    # Fuse convolution and batchnorm layers https://tehnokv.com/posts/fusing-batchnorm-and-conv/\n",
        "    fusedconv = nn.Conv2d(conv.in_channels,\n",
        "                          conv.out_channels,\n",
        "                          kernel_size=conv.kernel_size,\n",
        "                          stride=conv.stride,\n",
        "                          padding=conv.padding,\n",
        "                          groups=conv.groups,\n",
        "                          bias=True).requires_grad_(False).to(conv.weight.device)\n",
        "\n",
        "    # prepare filters\n",
        "    w_conv = conv.weight.clone().view(conv.out_channels, -1)\n",
        "    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))\n",
        "    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))\n",
        "\n",
        "    # prepare spatial bias\n",
        "    b_conv = torch.zeros(conv.weight.size(0), device=conv.weight.device) if conv.bias is None else conv.bias\n",
        "    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))\n",
        "    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)\n",
        "\n",
        "    return fusedconv\n",
        "\n",
        "\n",
        "def model_info(model, verbose=False, img_size=640):\n",
        "    # Model information. img_size may be int or list, i.e. img_size=640 or img_size=[640, 320]\n",
        "    n_p = sum(x.numel() for x in model.parameters())  # number parameters\n",
        "    n_g = sum(x.numel() for x in model.parameters() if x.requires_grad)  # number gradients\n",
        "    if verbose:\n",
        "        print('%5s %40s %9s %12s %20s %10s %10s' % ('layer', 'name', 'gradient', 'parameters', 'shape', 'mu', 'sigma'))\n",
        "        for i, (name, p) in enumerate(model.named_parameters()):\n",
        "            name = name.replace('module_list.', '')\n",
        "            print('%5g %40s %9s %12g %20s %10.3g %10.3g' %\n",
        "                  (i, name, p.requires_grad, p.numel(), list(p.shape), p.mean(), p.std()))\n",
        "\n",
        "    try:  # FLOPS\n",
        "        from thop import profile\n",
        "        stride = max(int(model.stride.max()), 32) if hasattr(model, 'stride') else 32\n",
        "        img = torch.zeros((1, model.yaml.get('ch', 3), stride, stride), device=next(model.parameters()).device)  # input\n",
        "        flops = profile(deepcopy(model), inputs=(img,), verbose=False)[0] / 1E9 * 2  # stride GFLOPS\n",
        "        img_size = img_size if isinstance(img_size, list) else [img_size, img_size]  # expand if int/float\n",
        "        fs = ', %.1f GFLOPS' % (flops * img_size[0] / stride * img_size[1] / stride)  # 640x640 GFLOPS\n",
        "    except (ImportError, Exception):\n",
        "        fs = ''\n",
        "\n",
        "    logger.info(f\"Model Summary: {len(list(model.modules()))} layers, {n_p} parameters, {n_g} gradients{fs}\")\n",
        "\n",
        "\n",
        "def load_classifier(name='resnet101', n=2):\n",
        "    # Loads a pretrained model reshaped to n-class output\n",
        "    model = torchvision.models.__dict__[name](pretrained=True)\n",
        "\n",
        "    # ResNet model properties\n",
        "    # input_size = [3, 224, 224]\n",
        "    # input_space = 'RGB'\n",
        "    # input_range = [0, 1]\n",
        "    # mean = [0.485, 0.456, 0.406]\n",
        "    # std = [0.229, 0.224, 0.225]\n",
        "\n",
        "    # Reshape output to n classes\n",
        "    filters = model.fc.weight.shape[1]\n",
        "    model.fc.bias = nn.Parameter(torch.zeros(n), requires_grad=True)\n",
        "    model.fc.weight = nn.Parameter(torch.zeros(n, filters), requires_grad=True)\n",
        "    model.fc.out_features = n\n",
        "    return model\n",
        "\n",
        "\n",
        "def scale_img(img, ratio=1.0, same_shape=False, gs=32):  # img(16,3,256,416)\n",
        "    # scales img(bs,3,y,x) by ratio constrained to gs-multiple\n",
        "    if ratio == 1.0:\n",
        "        return img\n",
        "    else:\n",
        "        h, w = img.shape[2:]\n",
        "        s = (int(h * ratio), int(w * ratio))  # new size\n",
        "        img = F.interpolate(img, size=s, mode='bilinear', align_corners=False)  # resize\n",
        "        if not same_shape:  # pad/crop img\n",
        "            h, w = [math.ceil(x * ratio / gs) * gs for x in (h, w)]\n",
        "        return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean\n",
        "\n",
        "\n",
        "def copy_attr(a, b, include=(), exclude=()):\n",
        "    # Copy attributes from b to a, options to only include [...] and to exclude [...]\n",
        "    for k, v in b.__dict__.items():\n",
        "        if (len(include) and k not in include) or k.startswith('_') or k in exclude:\n",
        "            continue\n",
        "        else:\n",
        "            setattr(a, k, v)\n",
        "\n",
        "\n",
        "class ModelEMA:\n",
        "    \"\"\" Model Exponential Moving Average from https://github.com/rwightman/pytorch-image-models\n",
        "    Keep a moving average of everything in the model state_dict (parameters and buffers).\n",
        "    This is intended to allow functionality like\n",
        "    https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage\n",
        "    A smoothed version of the weights is necessary for some training schemes to perform well.\n",
        "    This class is sensitive where it is initialized in the sequence of model init,\n",
        "    GPU assignment and distributed training wrappers.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model, decay=0.9999, updates=0):\n",
        "        # Create EMA\n",
        "        self.ema = deepcopy(model.module if is_parallel(model) else model).eval()  # FP32 EMA\n",
        "        # if next(model.parameters()).device.type != 'cpu':\n",
        "        #     self.ema.half()  # FP16 EMA\n",
        "        self.updates = updates  # number of EMA updates\n",
        "        self.decay = lambda x: decay * (1 - math.exp(-x / 2000))  # decay exponential ramp (to help early epochs)\n",
        "        for p in self.ema.parameters():\n",
        "            p.requires_grad_(False)\n",
        "\n",
        "    def update(self, model):\n",
        "        # Update EMA parameters\n",
        "        with torch.no_grad():\n",
        "            self.updates += 1\n",
        "            d = self.decay(self.updates)\n",
        "\n",
        "            msd = model.module.state_dict() if is_parallel(model) else model.state_dict()  # model state_dict\n",
        "            for k, v in self.ema.state_dict().items():\n",
        "                if v.dtype.is_floating_point:\n",
        "                    v *= d\n",
        "                    v += (1. - d) * msd[k].detach()\n",
        "\n",
        "    def update_attr(self, model, include=(), exclude=('process_group', 'reducer')):\n",
        "        # Update EMA attributes\n",
        "        copy_attr(self.ema, model, include, exclude)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGu64KHTCjZz"
      },
      "source": [
        "# Metrices"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IqrAWUbaCowi"
      },
      "outputs": [],
      "source": [
        "# Model validation metrics\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# from . import general\n",
        "\n",
        "\n",
        "def fitness(x):\n",
        "    # Model fitness as a weighted combination of metrics\n",
        "    w = [0.0, 0.0, 0.1, 0.9]  # weights for [P, R, mAP@0.5, mAP@0.5:0.95]\n",
        "    return (x[:, :4] * w).sum(1)\n",
        "\n",
        "\n",
        "def ap_per_class(tp, conf, pred_cls, target_cls, plot=False, save_dir='.', names=()):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves.\n",
        "    Source: https://github.com/rafaelpadilla/Object-Detection-Metrics.\n",
        "    # Arguments\n",
        "        tp:  True positives (nparray, nx1 or nx10).\n",
        "        conf:  Objectness value from 0-1 (nparray).\n",
        "        pred_cls:  Predicted object classes (nparray).\n",
        "        target_cls:  True object classes (nparray).\n",
        "        plot:  Plot precision-recall curve at mAP@0.5\n",
        "        save_dir:  Plot save directory\n",
        "    # Returns\n",
        "        The average precision as computed in py-faster-rcnn.\n",
        "    \"\"\"\n",
        "\n",
        "    # Sort by objectness\n",
        "\n",
        "    # get_sort_index\n",
        "    i = np.argsort(-conf)\n",
        "    \n",
        "    tp, conf, pred_cls = tp[i], conf[i], pred_cls[i]\n",
        "\n",
        "    # Find unique classes\n",
        "    unique_classes = np.unique(target_cls)\n",
        "    nc = unique_classes.shape[0]  # number of classes, number of detections\n",
        "\n",
        "    # Create Precision-Recall curve and compute AP for each class\n",
        "    px, py = np.linspace(0, 1, 1000), []  # for plotting\n",
        "    ap, p, r = np.zeros((nc, tp.shape[1])), np.zeros((nc, 1000)), np.zeros((nc, 1000))\n",
        "    for ci, c in enumerate(unique_classes):\n",
        "        i = pred_cls == c\n",
        "        n_l = (target_cls == c).sum()  # number of labels\n",
        "        n_p = i.sum()  # number of predictions\n",
        "\n",
        "        if n_p == 0 or n_l == 0:\n",
        "            continue\n",
        "        else:\n",
        "            # Accumulate FPs and TPs\n",
        "            fpc = (1 - tp[i]).cumsum(0)\n",
        "            tpc = tp[i].cumsum(0)\n",
        "\n",
        "            # Recall\n",
        "            recall = tpc / (n_l + 1e-16)  # recall curve\n",
        "            r[ci] = np.interp(-px, -conf[i], recall[:, 0], left=0)  # negative x, xp because xp decreases\n",
        "\n",
        "            # Precision\n",
        "            precision = tpc / (tpc + fpc)  # precision curve\n",
        "            p[ci] = np.interp(-px, -conf[i], precision[:, 0], left=1)  # p at pr_score\n",
        "\n",
        "            # AP from recall-precision curve\n",
        "            for j in range(tp.shape[1]):\n",
        "                ap[ci, j], mpre, mrec = compute_ap(recall[:, j], precision[:, j])\n",
        "                if plot and j == 0:\n",
        "                    py.append(np.interp(px, mrec, mpre))  # precision at mAP@0.5\n",
        "\n",
        "    # Compute F1 (harmonic mean of precision and recall)\n",
        "    f1 = 2 * p * r / (p + r + 1e-16)\n",
        "    if plot:\n",
        "        plot_pr_curve(px, py, ap, Path(save_dir) / 'PR_curve.png', names)\n",
        "        plot_mc_curve(px, f1, Path(save_dir) / 'F1_curve.png', names, ylabel='F1')\n",
        "        plot_mc_curve(px, p, Path(save_dir) / 'P_curve.png', names, ylabel='Precision')\n",
        "        plot_mc_curve(px, r, Path(save_dir) / 'R_curve.png', names, ylabel='Recall')\n",
        "\n",
        "    i = f1.mean(0).argmax()  # max F1 index\n",
        "    return p[:, i], r[:, i], ap, f1[:, i], unique_classes.astype('int32')\n",
        "\n",
        "\n",
        "def compute_ap(recall, precision):\n",
        "    \"\"\" Compute the average precision, given the recall and precision curves\n",
        "    # Arguments\n",
        "        recall:    The recall curve (list)\n",
        "        precision: The precision curve (list)\n",
        "    # Returns\n",
        "        Average precision, precision curve, recall curve\n",
        "    \"\"\"\n",
        "\n",
        "    # Append sentinel values to beginning and end\n",
        "    mrec = np.concatenate(([0.], recall, [recall[-1] + 0.01]))\n",
        "    mpre = np.concatenate(([1.], precision, [0.]))\n",
        "\n",
        "    # Compute the precision envelope\n",
        "    mpre = np.flip(np.maximum.accumulate(np.flip(mpre)))\n",
        "\n",
        "    # Integrate area under curve\n",
        "    method = 'interp'  # methods: 'continuous', 'interp'\n",
        "    if method == 'interp':\n",
        "        x = np.linspace(0, 1, 101)  # 101-point interp (COCO)\n",
        "        ap = np.trapz(np.interp(x, mrec, mpre), x)  # integrate\n",
        "    else:  # 'continuous'\n",
        "        i = np.where(mrec[1:] != mrec[:-1])[0]  # points where x axis (recall) changes\n",
        "        ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])  # area under curve\n",
        "\n",
        "    return ap, mpre, mrec\n",
        "\n",
        "\n",
        "class ConfusionMatrix:\n",
        "    # Updated version of https://github.com/kaanakan/object_detection_confusion_matrix\n",
        "    def __init__(self, nc, conf=0.25, iou_thres=0.45):\n",
        "        self.matrix = np.zeros((nc + 1, nc + 1))\n",
        "        self.nc = nc  # number of classes\n",
        "        self.conf = conf\n",
        "        self.iou_thres = iou_thres\n",
        "\n",
        "    def process_batch(self, detections, labels):\n",
        "        \"\"\"\n",
        "        Return intersection-over-union (Jaccard index) of boxes.\n",
        "        Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
        "        Arguments:\n",
        "            detections (Array[N, 6]), x1, y1, x2, y2, conf, class\n",
        "            labels (Array[M, 5]), class, x1, y1, x2, y2\n",
        "        Returns:\n",
        "            None, updates confusion matrix accordingly\n",
        "        \"\"\"\n",
        "        detections = detections[detections[:, 4] > self.conf]\n",
        "        gt_classes = labels[:, 0].int()\n",
        "        detection_classes = detections[:, 5].int()\n",
        "        iou = box_iou(labels[:, 1:], detections[:, :4])\n",
        "\n",
        "        x = torch.where(iou > self.iou_thres)\n",
        "        if x[0].shape[0]:\n",
        "            matches = torch.cat((torch.stack(x, 1), iou[x[0], x[1]][:, None]), 1).cpu().numpy()\n",
        "            if x[0].shape[0] > 1:\n",
        "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
        "                matches = matches[np.unique(matches[:, 1], return_index=True)[1]]\n",
        "                matches = matches[matches[:, 2].argsort()[::-1]]\n",
        "                matches = matches[np.unique(matches[:, 0], return_index=True)[1]]\n",
        "        else:\n",
        "            matches = np.zeros((0, 3))\n",
        "\n",
        "        n = matches.shape[0] > 0\n",
        "        m0, m1, _ = matches.transpose().astype(np.int16)\n",
        "        for i, gc in enumerate(gt_classes):\n",
        "            j = m0 == i\n",
        "            if n and sum(j) == 1:\n",
        "                self.matrix[detection_classes[m1[j]], gc] += 1  # correct\n",
        "            else:\n",
        "                self.matrix[self.nc, gc] += 1  # background FP\n",
        "\n",
        "        if n:\n",
        "            for i, dc in enumerate(detection_classes):\n",
        "                if not any(m1 == i):\n",
        "                    self.matrix[dc, self.nc] += 1  # background FN\n",
        "\n",
        "    def matrix(self):\n",
        "        return self.matrix\n",
        "\n",
        "    def plot(self, save_dir='', names=()):\n",
        "        try:\n",
        "            import seaborn as sn\n",
        "\n",
        "            array = self.matrix / (self.matrix.sum(0).reshape(1, self.nc + 1) + 1E-6)  # normalize\n",
        "            array[array < 0.005] = np.nan  # don't annotate (would appear as 0.00)\n",
        "\n",
        "            fig = plt.figure(figsize=(12, 9), tight_layout=True)\n",
        "            sn.set(font_scale=1.0 if self.nc < 50 else 0.8)  # for label size\n",
        "            labels = (0 < len(names) < 99) and len(names) == self.nc  # apply names to ticklabels\n",
        "            sn.heatmap(array, annot=self.nc < 30, annot_kws={\"size\": 8}, cmap='Blues', fmt='.2f', square=True,\n",
        "                       xticklabels=names + ['background FP'] if labels else \"auto\",\n",
        "                       yticklabels=names + ['background FN'] if labels else \"auto\").set_facecolor((1, 1, 1))\n",
        "            fig.axes[0].set_xlabel('True')\n",
        "            fig.axes[0].set_ylabel('Predicted')\n",
        "            fig.savefig(Path(save_dir) / 'confusion_matrix.png', dpi=250)\n",
        "        except Exception as e:\n",
        "            pass\n",
        "\n",
        "    def print(self):\n",
        "        for i in range(self.nc + 1):\n",
        "            print(' '.join(map(str, self.matrix[i])))\n",
        "\n",
        "\n",
        "# Plots ----------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "def plot_pr_curve(px, py, ap, save_dir='pr_curve.png', names=()):\n",
        "    # Precision-recall curve\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n",
        "    py = np.stack(py, axis=1)\n",
        "\n",
        "    if 0 < len(names) < 21:  # display per-class legend if < 21 classes\n",
        "        for i, y in enumerate(py.T):\n",
        "            ax.plot(px, y, linewidth=1, label=f'{names[i]} {ap[i, 0]:.3f}')  # plot(recall, precision)\n",
        "    else:\n",
        "        ax.plot(px, py, linewidth=1, color='grey')  # plot(recall, precision)\n",
        "\n",
        "    ax.plot(px, py.mean(1), linewidth=3, color='blue', label='all classes %.3f mAP@0.5' % ap[:, 0].mean())\n",
        "    ax.set_xlabel('Recall')\n",
        "    ax.set_ylabel('Precision')\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
        "    fig.savefig(Path(save_dir), dpi=250)\n",
        "\n",
        "\n",
        "def plot_mc_curve(px, py, save_dir='mc_curve.png', names=(), xlabel='Confidence', ylabel='Metric'):\n",
        "    # Metric-confidence curve\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(9, 6), tight_layout=True)\n",
        "\n",
        "    if 0 < len(names) < 21:  # display per-class legend if < 21 classes\n",
        "        for i, y in enumerate(py):\n",
        "            ax.plot(px, y, linewidth=1, label=f'{names[i]}')  # plot(confidence, metric)\n",
        "    else:\n",
        "        ax.plot(px, py.T, linewidth=1, color='grey')  # plot(confidence, metric)\n",
        "\n",
        "    y = py.mean(0)\n",
        "    ax.plot(px, y, linewidth=3, color='blue', label=f'all classes {y.max():.2f} at {px[y.argmax()]:.3f}')\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel(ylabel)\n",
        "    ax.set_xlim(0, 1)\n",
        "    ax.set_ylim(0, 1)\n",
        "    plt.legend(bbox_to_anchor=(1.04, 1), loc=\"upper left\")\n",
        "    fig.savefig(Path(save_dir), dpi=250)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E6e2KP4jDG7T"
      },
      "source": [
        "# Google Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "pVYSHb0VDMEZ"
      },
      "outputs": [],
      "source": [
        "# Google utils: https://cloud.google.com/storage/docs/reference/libraries\n",
        "\n",
        "import os\n",
        "import platform\n",
        "import subprocess\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "import requests\n",
        "import torch\n",
        "\n",
        "\n",
        "def gsutil_getsize(url=''):\n",
        "    # gs://bucket/file size https://cloud.google.com/storage/docs/gsutil/commands/du\n",
        "    s = subprocess.check_output(f'gsutil du {url}', shell=True).decode('utf-8')\n",
        "    return eval(s.split(' ')[0]) if len(s) else 0  # bytes\n",
        "\n",
        "\n",
        "def attempt_download(file, repo='ultralytics/yolov5'):\n",
        "    # Attempt file download if does not exist\n",
        "    file = Path(str(file).strip().replace(\"'\", ''))\n",
        "\n",
        "    if not file.exists():\n",
        "        file.parent.mkdir(parents=True, exist_ok=True)  # make parent dir (if required)\n",
        "        try:\n",
        "            response = requests.get(f'https://api.github.com/repos/{repo}/releases/latest').json()  # github api\n",
        "            assets = [x['name'] for x in response['assets']]  # release assets, i.e. ['yolov5s.pt', 'yolov5m.pt', ...]\n",
        "            tag = response['tag_name']  # i.e. 'v1.0'\n",
        "        except:  # fallback plan\n",
        "            assets = ['yolov5s.pt', 'yolov5m.pt', 'yolov5l.pt', 'yolov5x.pt',\n",
        "                      'yolov5s6.pt', 'yolov5m6.pt', 'yolov5l6.pt', 'yolov5x6.pt']\n",
        "            try:\n",
        "                tag = subprocess.check_output('git tag', shell=True, stderr=subprocess.STDOUT).decode().split()[-1]\n",
        "            except:\n",
        "                tag = 'v5.0'  # current release\n",
        "\n",
        "        name = file.name\n",
        "        if name in assets:\n",
        "            msg = f'{file} missing, try downloading from https://github.com/{repo}/releases/'\n",
        "            redundant = False  # second download option\n",
        "            try:  # GitHub\n",
        "                url = f'https://github.com/{repo}/releases/download/{tag}/{name}'\n",
        "                print(f'Downloading {url} to {file}...')\n",
        "                torch.hub.download_url_to_file(url, file)\n",
        "                assert file.exists() and file.stat().st_size > 1E6  # check\n",
        "            except Exception as e:  # GCP\n",
        "                print(f'Download error: {e}')\n",
        "                assert redundant, 'No secondary mirror'\n",
        "                url = f'https://storage.googleapis.com/{repo}/ckpt/{name}'\n",
        "                print(f'Downloading {url} to {file}...')\n",
        "                os.system(f\"curl -L '{url}' -o '{file}' --retry 3 -C -\")  # curl download, retry and resume on fail\n",
        "            finally:\n",
        "                if not file.exists() or file.stat().st_size < 1E6:  # check\n",
        "                    file.unlink(missing_ok=True)  # remove partial downloads\n",
        "                    print(f'ERROR: Download failure: {msg}')\n",
        "                print('')\n",
        "                return\n",
        "\n",
        "\n",
        "def gdrive_download(id='16TiPfZj7htmTyhntwcZyEEAejOUxuT6m', file='tmp.zip'):\n",
        "    # Downloads a file from Google Drive. from yolov5.utils.google_utils import *; gdrive_download()\n",
        "    t = time.time()\n",
        "    file = Path(file)\n",
        "    cookie = Path('cookie')  # gdrive cookie\n",
        "    print(f'Downloading https://drive.google.com/uc?export=download&id={id} as {file}... ', end='')\n",
        "    file.unlink(missing_ok=True)  # remove existing file\n",
        "    cookie.unlink(missing_ok=True)  # remove existing cookie\n",
        "\n",
        "    # Attempt file download\n",
        "    out = \"NUL\" if platform.system() == \"Windows\" else \"/dev/null\"\n",
        "    os.system(f'curl -c ./cookie -s -L \"drive.google.com/uc?export=download&id={id}\" > {out}')\n",
        "    if os.path.exists('cookie'):  # large file\n",
        "        s = f'curl -Lb ./cookie \"drive.google.com/uc?export=download&confirm={get_token()}&id={id}\" -o {file}'\n",
        "    else:  # small file\n",
        "        s = f'curl -s -L -o {file} \"drive.google.com/uc?export=download&id={id}\"'\n",
        "    r = os.system(s)  # execute, capture return\n",
        "    cookie.unlink(missing_ok=True)  # remove existing cookie\n",
        "\n",
        "    # Error check\n",
        "    if r != 0:\n",
        "        file.unlink(missing_ok=True)  # remove partial\n",
        "        print('Download error ')  # raise Exception('Download error')\n",
        "        return r\n",
        "\n",
        "    # Unzip if archive\n",
        "    if file.suffix == '.zip':\n",
        "        print('unzipping... ', end='')\n",
        "        os.system(f'unzip -q {file}')  # unzip\n",
        "        file.unlink()  # remove zip to free space\n",
        "\n",
        "    print(f'Done ({time.time() - t:.1f}s)')\n",
        "    return r\n",
        "\n",
        "\n",
        "def get_token(cookie=\"./cookie\"):\n",
        "    with open(cookie) as f:\n",
        "        for line in f:\n",
        "            if \"download\" in line:\n",
        "                return line.split()[-1]\n",
        "    return \"\"\n",
        "\n",
        "# def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
        "#     # Uploads a file to a bucket\n",
        "#     # https://cloud.google.com/storage/docs/uploading-objects#storage-upload-object-python\n",
        "#\n",
        "#     storage_client = storage.Client()\n",
        "#     bucket = storage_client.get_bucket(bucket_name)\n",
        "#     blob = bucket.blob(destination_blob_name)\n",
        "#\n",
        "#     blob.upload_from_filename(source_file_name)\n",
        "#\n",
        "#     print('File {} uploaded to {}.'.format(\n",
        "#         source_file_name,\n",
        "#         destination_blob_name))\n",
        "#\n",
        "#\n",
        "# def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
        "#     # Uploads a blob from a bucket\n",
        "#     storage_client = storage.Client()\n",
        "#     bucket = storage_client.get_bucket(bucket_name)\n",
        "#     blob = bucket.blob(source_blob_name)\n",
        "#\n",
        "#     blob.download_to_filename(destination_file_name)\n",
        "#\n",
        "#     print('Blob {} downloaded to {}.'.format(\n",
        "#         source_blob_name,\n",
        "#         destination_file_name))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JMa18JJCDuPy"
      },
      "source": [
        "# General"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "EjgszMUnDxCB"
      },
      "outputs": [],
      "source": [
        "# YOLOv5 general utils\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import platform\n",
        "import random\n",
        "import re\n",
        "import subprocess\n",
        "import time\n",
        "from itertools import repeat\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from pathlib import Path\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pkg_resources as pkg\n",
        "import torch\n",
        "import torchvision\n",
        "import yaml\n",
        "\n",
        "# from utils.google_utils import gsutil_getsize\n",
        "# from utils.metrics import fitness\n",
        "\n",
        "# from utils.torch_utils import init_torch_seeds\n",
        "\n",
        "# Settings\n",
        "torch.set_printoptions(linewidth=320, precision=5, profile='long')\n",
        "np.set_printoptions(linewidth=320, formatter={'float_kind': '{:11.5g}'.format})  # format short g, %precision=5\n",
        "pd.options.display.max_columns = 10\n",
        "cv2.setNumThreads(0)  # prevent OpenCV from multithreading (incompatible with PyTorch DataLoader)\n",
        "os.environ['NUMEXPR_MAX_THREADS'] = str(min(os.cpu_count(), 8))  # NumExpr max threads\n",
        "\n",
        "\n",
        "def set_logging(rank=-1, verbose=True):\n",
        "    logging.basicConfig(\n",
        "        format=\"%(message)s\",\n",
        "        level=logging.INFO if (verbose and rank in [-1, 0]) else logging.WARN)\n",
        "\n",
        "\n",
        "def init_seeds(seed=0):\n",
        "    # Initialize random number generator (RNG) seeds\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    init_torch_seeds(seed)\n",
        "\n",
        "\n",
        "def get_latest_run(search_dir='.'):\n",
        "    # Return path to most recent 'last.pt' in /runs (i.e. to --resume from)\n",
        "    last_list = glob.glob(f'{search_dir}/**/last*.pt', recursive=True)\n",
        "    return max(last_list, key=os.path.getctime) if last_list else ''\n",
        "\n",
        "\n",
        "def is_docker():\n",
        "    # Is environment a Docker container\n",
        "    return Path('/workspace').exists()  # or Path('/.dockerenv').exists()\n",
        "\n",
        "\n",
        "def is_colab():\n",
        "    # Is environment a Google Colab instance\n",
        "    try:\n",
        "        import google.colab\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        return False\n",
        "\n",
        "\n",
        "def emojis(str=''):\n",
        "    # Return platform-dependent emoji-safe version of string\n",
        "    return str.encode().decode('ascii', 'ignore') if platform.system() == 'Windows' else str\n",
        "\n",
        "\n",
        "def file_size(file):\n",
        "    # Return file size in MB\n",
        "    return Path(file).stat().st_size / 1e6\n",
        "\n",
        "\n",
        "def check_online():\n",
        "    # Check internet connectivity\n",
        "    import socket\n",
        "    try:\n",
        "        socket.create_connection((\"1.1.1.1\", 443), 5)  # check host accesability\n",
        "        return True\n",
        "    except OSError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def check_git_status():\n",
        "    # Recommend 'git pull' if code is out of date\n",
        "    print(colorstr('github: '), end='')\n",
        "    try:\n",
        "        assert Path('.git').exists(), 'skipping check (not a git repository)'\n",
        "        assert not is_docker(), 'skipping check (Docker image)'\n",
        "        assert check_online(), 'skipping check (offline)'\n",
        "\n",
        "        cmd = 'git fetch && git config --get remote.origin.url'\n",
        "        url = subprocess.check_output(cmd, shell=True).decode().strip().rstrip('.git')  # github repo url\n",
        "        branch = subprocess.check_output('git rev-parse --abbrev-ref HEAD', shell=True).decode().strip()  # checked out\n",
        "        n = int(subprocess.check_output(f'git rev-list {branch}..origin/master --count', shell=True))  # commits behind\n",
        "        if n > 0:\n",
        "            s = f\"âš ï¸ WARNING: code is out of date by {n} commit{'s' * (n > 1)}. \" \\\n",
        "                f\"Use 'git pull' to update or 'git clone {url}' to download latest.\"\n",
        "        else:\n",
        "            s = f'up to date with {url} âœ…'\n",
        "        print(emojis(s))  # emoji-safe\n",
        "    except Exception as e:\n",
        "        print(e)\n",
        "\n",
        "\n",
        "def check_python(minimum='3.7.0', required=True):\n",
        "    # Check current python version vs. required python version\n",
        "    current = platform.python_version()\n",
        "    result = pkg.parse_version(current) >= pkg.parse_version(minimum)\n",
        "    if required:\n",
        "        assert result, f'Python {minimum} required by YOLOv5, but Python {current} is currently installed'\n",
        "    return result\n",
        "\n",
        "\n",
        "def check_requirements(requirements='requirements.txt', exclude=()):\n",
        "    # Check installed dependencies meet requirements (pass *.txt file or list of packages)\n",
        "    prefix = colorstr('red', 'bold', 'requirements:')\n",
        "    check_python()  # check python version\n",
        "    if isinstance(requirements, (str, Path)):  # requirements.txt file\n",
        "        file = Path(requirements)\n",
        "        if not file.exists():\n",
        "            print(f\"{prefix} {file.resolve()} not found, check failed.\")\n",
        "            return\n",
        "        requirements = [f'{x.name}{x.specifier}' for x in pkg.parse_requirements(file.open()) if x.name not in exclude]\n",
        "    else:  # list or tuple of packages\n",
        "        requirements = [x for x in requirements if x not in exclude]\n",
        "\n",
        "    n = 0  # number of packages updates\n",
        "    for r in requirements:\n",
        "        try:\n",
        "            pkg.require(r)\n",
        "        except Exception as e:  # DistributionNotFound or VersionConflict if requirements not met\n",
        "            n += 1\n",
        "            print(f\"{prefix} {r} not found and is required by YOLOv5, attempting auto-update...\")\n",
        "            print(subprocess.check_output(f\"pip install '{r}'\", shell=True).decode())\n",
        "\n",
        "    if n:  # if packages updated\n",
        "        source = file.resolve() if 'file' in locals() else requirements\n",
        "        s = f\"{prefix} {n} package{'s' * (n > 1)} updated per {source}\\n\" \\\n",
        "            f\"{prefix} âš ï¸ {colorstr('bold', 'Restart runtime or rerun command for updates to take effect')}\\n\"\n",
        "        print(emojis(s))  # emoji-safe\n",
        "\n",
        "\n",
        "def check_img_size(img_size, s=32):\n",
        "    # Verify img_size is a multiple of stride s\n",
        "    new_size = make_divisible(img_size, int(s))  # ceil gs-multiple\n",
        "    if new_size != img_size:\n",
        "        print('WARNING: --img-size %g must be multiple of max stride %g, updating to %g' % (img_size, s, new_size))\n",
        "    return new_size\n",
        "\n",
        "\n",
        "def check_imshow():\n",
        "    # Check if environment supports image displays\n",
        "    try:\n",
        "        assert not is_docker(), 'cv2.imshow() is disabled in Docker environments'\n",
        "        assert not is_colab(), 'cv2.imshow() is disabled in Google Colab environments'\n",
        "        cv2.imshow('test', np.zeros((1, 1, 3)))\n",
        "        cv2.waitKey(1)\n",
        "        cv2.destroyAllWindows()\n",
        "        cv2.waitKey(1)\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f'WARNING: Environment does not support cv2.imshow() or PIL Image.show() image displays\\n{e}')\n",
        "        return False\n",
        "\n",
        "\n",
        "def check_file(file):\n",
        "    # Search for file if not found\n",
        "    if Path(file).is_file() or file == '':\n",
        "        return file\n",
        "    else:\n",
        "        files = glob.glob('./**/' + file, recursive=True)  # find file\n",
        "        assert len(files), f'File Not Found: {file}'  # assert file was found\n",
        "        assert len(files) == 1, f\"Multiple files match '{file}', specify exact path: {files}\"  # assert unique\n",
        "        return files[0]  # return file\n",
        "\n",
        "\n",
        "def check_dataset(dict):\n",
        "    # Download dataset if not found locally\n",
        "    val, s = dict.get('val'), dict.get('download')\n",
        "    if val and len(val):\n",
        "        val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val path\n",
        "        if not all(x.exists() for x in val):\n",
        "            print('\\nWARNING: Dataset not found, nonexistent paths: %s' % [str(x) for x in val if not x.exists()])\n",
        "            if s and len(s):  # download script\n",
        "                if s.startswith('http') and s.endswith('.zip'):  # URL\n",
        "                    f = Path(s).name  # filename\n",
        "                    print(f'Downloading {s} ...')\n",
        "                    torch.hub.download_url_to_file(s, f)\n",
        "                    r = os.system(f'unzip -q {f} -d ../ && rm {f}')  # unzip\n",
        "                elif s.startswith('bash '):  # bash script\n",
        "                    print(f'Running {s} ...')\n",
        "                    r = os.system(s)\n",
        "                else:  # python script\n",
        "                    r = exec(s)  # return None\n",
        "                print('Dataset autodownload %s\\n' % ('success' if r in (0, None) else 'failure'))  # print result\n",
        "            else:\n",
        "                raise Exception('Dataset not found.')\n",
        "\n",
        "\n",
        "def download(url, dir='.', unzip=True, delete=True, curl=False, threads=1):\n",
        "    # Multi-threaded file download and unzip function\n",
        "    def download_one(url, dir):\n",
        "        # Download 1 file\n",
        "        f = dir / Path(url).name  # filename\n",
        "        if not f.exists():\n",
        "            print(f'Downloading {url} to {f}...')\n",
        "            if curl:\n",
        "                os.system(f\"curl -L '{url}' -o '{f}' --retry 9 -C -\")  # curl download, retry and resume on fail\n",
        "            else:\n",
        "                torch.hub.download_url_to_file(url, f, progress=True)  # torch download\n",
        "        if unzip and f.suffix in ('.zip', '.gz'):\n",
        "            print(f'Unzipping {f}...')\n",
        "            if f.suffix == '.zip':\n",
        "                s = f'unzip -qo {f} -d {dir} && rm {f}'  # unzip -quiet -overwrite\n",
        "            elif f.suffix == '.gz':\n",
        "                s = f'tar xfz {f} --directory {f.parent}'  # unzip\n",
        "            if delete:  # delete zip file after unzip\n",
        "                s += f' && rm {f}'\n",
        "            os.system(s)\n",
        "\n",
        "    dir = Path(dir)\n",
        "    dir.mkdir(parents=True, exist_ok=True)  # make directory\n",
        "    if threads > 1:\n",
        "        pool = ThreadPool(threads)\n",
        "        pool.imap(lambda x: download_one(*x), zip(url, repeat(dir)))  # multi-threaded\n",
        "        pool.close()\n",
        "        pool.join()\n",
        "    else:\n",
        "        for u in tuple(url) if isinstance(url, str) else url:\n",
        "            download_one(u, dir)\n",
        "\n",
        "\n",
        "def make_divisible(x, divisor):\n",
        "    # Returns x evenly divisible by divisor\n",
        "    return math.ceil(x / divisor) * divisor\n",
        "\n",
        "\n",
        "def clean_str(s):\n",
        "    # Cleans a string by replacing special characters with underscore _\n",
        "    return re.sub(pattern=\"[|@#!Â¡Â·$â‚¬%&()=?Â¿^*;:,Â¨Â´><+]\", repl=\"_\", string=s)\n",
        "\n",
        "\n",
        "def one_cycle(y1=0.0, y2=1.0, steps=100):\n",
        "    # lambda function for sinusoidal ramp from y1 to y2\n",
        "    return lambda x: ((1 - math.cos(x * math.pi / steps)) / 2) * (y2 - y1) + y1\n",
        "\n",
        "\n",
        "def colorstr(*input):\n",
        "    # Colors a string https://en.wikipedia.org/wiki/ANSI_escape_code, i.e.  colorstr('blue', 'hello world')\n",
        "    *args, string = input if len(input) > 1 else ('blue', 'bold', input[0])  # color arguments, string\n",
        "    colors = {'black': '\\033[30m',  # basic colors\n",
        "              'red': '\\033[31m',\n",
        "              'green': '\\033[32m',\n",
        "              'yellow': '\\033[33m',\n",
        "              'blue': '\\033[34m',\n",
        "              'magenta': '\\033[35m',\n",
        "              'cyan': '\\033[36m',\n",
        "              'white': '\\033[37m',\n",
        "              'bright_black': '\\033[90m',  # bright colors\n",
        "              'bright_red': '\\033[91m',\n",
        "              'bright_green': '\\033[92m',\n",
        "              'bright_yellow': '\\033[93m',\n",
        "              'bright_blue': '\\033[94m',\n",
        "              'bright_magenta': '\\033[95m',\n",
        "              'bright_cyan': '\\033[96m',\n",
        "              'bright_white': '\\033[97m',\n",
        "              'end': '\\033[0m',  # misc\n",
        "              'bold': '\\033[1m',\n",
        "              'underline': '\\033[4m'}\n",
        "    return ''.join(colors[x] for x in args) + f'{string}' + colors['end']\n",
        "\n",
        "\n",
        "def labels_to_class_weights(labels, nc=80):\n",
        "    # Get class weights (inverse frequency) from training labels\n",
        "    if labels[0] is None:  # no labels loaded\n",
        "        return torch.Tensor()\n",
        "\n",
        "    labels = np.concatenate(labels, 0)  # labels.shape = (866643, 5) for COCO\n",
        "    classes = labels[:, 0].astype(np.int)  # labels = [class xywh]\n",
        "    weights = np.bincount(classes, minlength=nc)  # occurrences per class\n",
        "\n",
        "    # Prepend gridpoint count (for uCE training)\n",
        "    # gpi = ((320 / 32 * np.array([1, 2, 4])) ** 2 * 3).sum()  # gridpoints per image\n",
        "    # weights = np.hstack([gpi * len(labels)  - weights.sum() * 9, weights * 9]) ** 0.5  # prepend gridpoints to start\n",
        "\n",
        "    weights[weights == 0] = 1  # replace empty bins with 1\n",
        "    weights = 1 / weights  # number of targets per class\n",
        "    weights /= weights.sum()  # normalize\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def labels_to_image_weights(labels, nc=80, class_weights=np.ones(80)):\n",
        "    # Produces image weights based on class_weights and image contents\n",
        "    class_counts = np.array([np.bincount(x[:, 0].astype(np.int), minlength=nc) for x in labels])\n",
        "    image_weights = (class_weights.reshape(1, nc) * class_counts).sum(1)\n",
        "    # index = random.choices(range(n), weights=image_weights, k=1)  # weight image sample\n",
        "    return image_weights\n",
        "\n",
        "\n",
        "def coco80_to_coco91_class():  # converts 80-index (val2014) to 91-index (paper)\n",
        "    # https://tech.amikelive.com/node-718/what-object-categories-labels-are-in-coco-dataset/\n",
        "    # a = np.loadtxt('data/coco.names', dtype='str', delimiter='\\n')\n",
        "    # b = np.loadtxt('data/coco_paper.names', dtype='str', delimiter='\\n')\n",
        "    # x1 = [list(a[i] == b).index(True) + 1 for i in range(80)]  # darknet to coco\n",
        "    # x2 = [list(b[i] == a).index(True) if any(b[i] == a) else None for i in range(91)]  # coco to darknet\n",
        "    x = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 27, 28, 31, 32, 33, 34,\n",
        "         35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63,\n",
        "         64, 65, 67, 70, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 84, 85, 86, 87, 88, 89, 90]\n",
        "    return x\n",
        "\n",
        "\n",
        "def xyxy2xywh(x):\n",
        "    # Convert nx4 boxes from [x1, y1, x2, y2] to [x, y, w, h] where xy1=top-left, xy2=bottom-right\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[:, 0] = (x[:, 0] + x[:, 2]) / 2  # x center\n",
        "    y[:, 1] = (x[:, 1] + x[:, 3]) / 2  # y center\n",
        "    y[:, 2] = x[:, 2] - x[:, 0]  # width\n",
        "    y[:, 3] = x[:, 3] - x[:, 1]  # height\n",
        "    return y\n",
        "\n",
        "\n",
        "def xywh2xyxy(x):\n",
        "    # Convert nx4 boxes from [x, y, w, h] to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[:, 0] = x[:, 0] - x[:, 2] / 2  # top left x\n",
        "    y[:, 1] = x[:, 1] - x[:, 3] / 2  # top left y\n",
        "    y[:, 2] = x[:, 0] + x[:, 2] / 2  # bottom right x\n",
        "    y[:, 3] = x[:, 1] + x[:, 3] / 2  # bottom right y\n",
        "    return y\n",
        "\n",
        "\n",
        "def xywhn2xyxy(x, w=640, h=640, padw=0, padh=0):\n",
        "    # Convert nx4 boxes from [x, y, w, h] normalized to [x1, y1, x2, y2] where xy1=top-left, xy2=bottom-right\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[:, 0] = w * (x[:, 0] - x[:, 2] / 2) + padw  # top left x\n",
        "    y[:, 1] = h * (x[:, 1] - x[:, 3] / 2) + padh  # top left y\n",
        "    y[:, 2] = w * (x[:, 0] + x[:, 2] / 2) + padw  # bottom right x\n",
        "    y[:, 3] = h * (x[:, 1] + x[:, 3] / 2) + padh  # bottom right y\n",
        "    return y\n",
        "\n",
        "\n",
        "def xyn2xy(x, w=640, h=640, padw=0, padh=0):\n",
        "    # Convert normalized segments into pixel segments, shape (n,2)\n",
        "    y = x.clone() if isinstance(x, torch.Tensor) else np.copy(x)\n",
        "    y[:, 0] = w * x[:, 0] + padw  # top left x\n",
        "    y[:, 1] = h * x[:, 1] + padh  # top left y\n",
        "    return y\n",
        "\n",
        "\n",
        "def segment2box(segment, width=640, height=640):\n",
        "    # Convert 1 segment label to 1 box label, applying inside-image constraint, i.e. (xy1, xy2, ...) to (xyxy)\n",
        "    x, y = segment.T  # segment xy\n",
        "    inside = (x >= 0) & (y >= 0) & (x <= width) & (y <= height)\n",
        "    x, y, = x[inside], y[inside]\n",
        "    return np.array([x.min(), y.min(), x.max(), y.max()]) if any(x) else np.zeros((1, 4))  # xyxy\n",
        "\n",
        "\n",
        "def segments2boxes(segments):\n",
        "    # Convert segment labels to box labels, i.e. (cls, xy1, xy2, ...) to (cls, xywh)\n",
        "    boxes = []\n",
        "    for s in segments:\n",
        "        x, y = s.T  # segment xy\n",
        "        boxes.append([x.min(), y.min(), x.max(), y.max()])  # cls, xyxy\n",
        "    return xyxy2xywh(np.array(boxes))  # cls, xywh\n",
        "\n",
        "\n",
        "def resample_segments(segments, n=1000):\n",
        "    # Up-sample an (n,2) segment\n",
        "    for i, s in enumerate(segments):\n",
        "        x = np.linspace(0, len(s) - 1, n)\n",
        "        xp = np.arange(len(s))\n",
        "        segments[i] = np.concatenate([np.interp(x, xp, s[:, i]) for i in range(2)]).reshape(2, -1).T  # segment xy\n",
        "    return segments\n",
        "\n",
        "\n",
        "def scale_coords(img1_shape, coords, img0_shape, ratio_pad=None):\n",
        "    # Rescale coords (xyxy) from img1_shape to img0_shape\n",
        "    if ratio_pad is None:  # calculate from img0_shape\n",
        "        gain = min(img1_shape[0] / img0_shape[0], img1_shape[1] / img0_shape[1])  # gain  = old / new\n",
        "        pad = (img1_shape[1] - img0_shape[1] * gain) / 2, (img1_shape[0] - img0_shape[0] * gain) / 2  # wh padding\n",
        "    else:\n",
        "        gain = ratio_pad[0][0]\n",
        "        pad = ratio_pad[1]\n",
        "\n",
        "    coords[:, [0, 2]] -= pad[0]  # x padding\n",
        "    coords[:, [1, 3]] -= pad[1]  # y padding\n",
        "    coords[:, :4] /= gain\n",
        "    clip_coords(coords, img0_shape)\n",
        "    return coords\n",
        "\n",
        "\n",
        "def clip_coords(boxes, img_shape):\n",
        "    # Clip bounding xyxy bounding boxes to image shape (height, width)\n",
        "    boxes[:, 0].clamp_(0, img_shape[1])  # x1\n",
        "    boxes[:, 1].clamp_(0, img_shape[0])  # y1\n",
        "    boxes[:, 2].clamp_(0, img_shape[1])  # x2\n",
        "    boxes[:, 3].clamp_(0, img_shape[0])  # y2\n",
        "\n",
        "\n",
        "def bbox_iou(box1, box2, x1y1x2y2=True, GIoU=False, DIoU=False, CIoU=False, eps=1e-7):\n",
        "    # Returns the IoU of box1 to box2. box1 is 4, box2 is nx4\n",
        "    box2 = box2.T\n",
        "\n",
        "    # Get the coordinates of bounding boxes\n",
        "    if x1y1x2y2:  # x1, y1, x2, y2 = box1\n",
        "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
        "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
        "    else:  # transform from xywh to xyxy\n",
        "        b1_x1, b1_x2 = box1[0] - box1[2] / 2, box1[0] + box1[2] / 2\n",
        "        b1_y1, b1_y2 = box1[1] - box1[3] / 2, box1[1] + box1[3] / 2\n",
        "        b2_x1, b2_x2 = box2[0] - box2[2] / 2, box2[0] + box2[2] / 2\n",
        "        b2_y1, b2_y2 = box2[1] - box2[3] / 2, box2[1] + box2[3] / 2\n",
        "\n",
        "    # Intersection area\n",
        "    inter = (torch.min(b1_x2, b2_x2) - torch.max(b1_x1, b2_x1)).clamp(0) * \\\n",
        "            (torch.min(b1_y2, b2_y2) - torch.max(b1_y1, b2_y1)).clamp(0)\n",
        "\n",
        "    # Union Area\n",
        "    w1, h1 = b1_x2 - b1_x1, b1_y2 - b1_y1 + eps\n",
        "    w2, h2 = b2_x2 - b2_x1, b2_y2 - b2_y1 + eps\n",
        "    union = w1 * h1 + w2 * h2 - inter + eps\n",
        "\n",
        "    iou = inter / union\n",
        "    if GIoU or DIoU or CIoU:\n",
        "        cw = torch.max(b1_x2, b2_x2) - torch.min(b1_x1, b2_x1)  # convex (smallest enclosing box) width\n",
        "        ch = torch.max(b1_y2, b2_y2) - torch.min(b1_y1, b2_y1)  # convex height\n",
        "        if CIoU or DIoU:  # Distance or Complete IoU https://arxiv.org/abs/1911.08287v1\n",
        "            c2 = cw ** 2 + ch ** 2 + eps  # convex diagonal squared\n",
        "            rho2 = ((b2_x1 + b2_x2 - b1_x1 - b1_x2) ** 2 +\n",
        "                    (b2_y1 + b2_y2 - b1_y1 - b1_y2) ** 2) / 4  # center distance squared\n",
        "            if DIoU:\n",
        "                return iou - rho2 / c2  # DIoU\n",
        "            elif CIoU:  # https://github.com/Zzh-tju/DIoU-SSD-pytorch/blob/master/utils/box/box_utils.py#L47\n",
        "                v = (4 / math.pi ** 2) * torch.pow(torch.atan(w2 / h2) - torch.atan(w1 / h1), 2)\n",
        "                with torch.no_grad():\n",
        "                    alpha = v / (v - iou + (1 + eps))\n",
        "                return iou - (rho2 / c2 + v * alpha)  # CIoU\n",
        "        else:  # GIoU https://arxiv.org/pdf/1902.09630.pdf\n",
        "            c_area = cw * ch + eps  # convex area\n",
        "            return iou - (c_area - union) / c_area  # GIoU\n",
        "    else:\n",
        "        return iou  # IoU\n",
        "\n",
        "\n",
        "def box_iou(box1, box2):\n",
        "    # https://github.com/pytorch/vision/blob/master/torchvision/ops/boxes.py\n",
        "    \"\"\"\n",
        "    Return intersection-over-union (Jaccard index) of boxes.\n",
        "    Both sets of boxes are expected to be in (x1, y1, x2, y2) format.\n",
        "    Arguments:\n",
        "        box1 (Tensor[N, 4])\n",
        "        box2 (Tensor[M, 4])\n",
        "    Returns:\n",
        "        iou (Tensor[N, M]): the NxM matrix containing the pairwise\n",
        "            IoU values for every element in boxes1 and boxes2\n",
        "    \"\"\"\n",
        "\n",
        "    def box_area(box):\n",
        "        # box = 4xn\n",
        "        return (box[2] - box[0]) * (box[3] - box[1])\n",
        "\n",
        "    area1 = box_area(box1.T)\n",
        "    area2 = box_area(box2.T)\n",
        "\n",
        "    # inter(N,M) = (rb(N,M,2) - lt(N,M,2)).clamp(0).prod(2)\n",
        "    inter = (torch.min(box1[:, None, 2:], box2[:, 2:]) - torch.max(box1[:, None, :2], box2[:, :2])).clamp(0).prod(2)\n",
        "    return inter / (area1[:, None] + area2 - inter)  # iou = inter / (area1 + area2 - inter)\n",
        "\n",
        "\n",
        "def wh_iou(wh1, wh2):\n",
        "    # Returns the nxm IoU matrix. wh1 is nx2, wh2 is mx2\n",
        "    wh1 = wh1[:, None]  # [N,1,2]\n",
        "    wh2 = wh2[None]  # [1,M,2]\n",
        "    inter = torch.min(wh1, wh2).prod(2)  # [N,M]\n",
        "    return inter / (wh1.prod(2) + wh2.prod(2) - inter)  # iou = inter / (area1 + area2 - inter)\n",
        "\n",
        "\n",
        "def non_max_suppression(prediction, conf_thres=0.25, iou_thres=0.45, classes=None, agnostic=False, multi_label=False,\n",
        "                        labels=()):\n",
        "    \"\"\"Runs Non-Maximum Suppression (NMS) on inference results\n",
        "\n",
        "    Returns:\n",
        "         list of detections, on (n,6) tensor per image [xyxy, conf, cls]\n",
        "    \"\"\"\n",
        "\n",
        "    nc = prediction.shape[2] - 5  # number of classes\n",
        "    xc = prediction[..., 4] > conf_thres  # candidates\n",
        "\n",
        "    # Checks\n",
        "    assert 0 <= conf_thres <= 1, f'Invalid Confidence threshold {conf_thres}, valid values are between 0.0 and 1.0'\n",
        "    assert 0 <= iou_thres <= 1, f'Invalid IoU {iou_thres}, valid values are between 0.0 and 1.0'\n",
        "\n",
        "    # Settings\n",
        "    min_wh, max_wh = 2, 4096  # (pixels) minimum and maximum box width and height\n",
        "    max_det = 300  # maximum number of detections per image\n",
        "    max_nms = 30000  # maximum number of boxes into torchvision.ops.nms()\n",
        "    time_limit = 10.0  # seconds to quit after\n",
        "    redundant = True  # require redundant detections\n",
        "    multi_label &= nc > 1  # multiple labels per box (adds 0.5ms/img)\n",
        "    merge = False  # use merge-NMS\n",
        "\n",
        "    t = time.time()\n",
        "    output = [torch.zeros((0, 6), device=prediction.device)] * prediction.shape[0]\n",
        "    for xi, x in enumerate(prediction):  # image index, image inference\n",
        "        # Apply constraints\n",
        "        # x[((x[..., 2:4] < min_wh) | (x[..., 2:4] > max_wh)).any(1), 4] = 0  # width-height\n",
        "        x = x[xc[xi]]  # confidence\n",
        "\n",
        "        # Cat apriori labels if autolabelling\n",
        "        if labels and len(labels[xi]):\n",
        "            l = labels[xi]\n",
        "            v = torch.zeros((len(l), nc + 5), device=x.device)\n",
        "            v[:, :4] = l[:, 1:5]  # box\n",
        "            v[:, 4] = 1.0  # conf\n",
        "            v[range(len(l)), l[:, 0].long() + 5] = 1.0  # cls\n",
        "            x = torch.cat((x, v), 0)\n",
        "\n",
        "        # If none remain process next image\n",
        "        if not x.shape[0]:\n",
        "            continue\n",
        "\n",
        "        # Compute conf\n",
        "        x[:, 5:] *= x[:, 4:5]  # conf = obj_conf * cls_conf\n",
        "\n",
        "        # Box (center x, center y, width, height) to (x1, y1, x2, y2)\n",
        "        box = xywh2xyxy(x[:, :4])\n",
        "\n",
        "        # Detections matrix nx6 (xyxy, conf, cls)\n",
        "        if multi_label:\n",
        "            i, j = (x[:, 5:] > conf_thres).nonzero(as_tuple=False).T\n",
        "            x = torch.cat((box[i], x[i, j + 5, None], j[:, None].float()), 1)\n",
        "        else:  # best class only\n",
        "            conf, j = x[:, 5:].max(1, keepdim=True)\n",
        "            x = torch.cat((box, conf, j.float()), 1)[conf.view(-1) > conf_thres]\n",
        "\n",
        "        # Filter by class\n",
        "        if classes is not None:\n",
        "            x = x[(x[:, 5:6] == torch.tensor(classes, device=x.device)).any(1)]\n",
        "\n",
        "        # Apply finite constraint\n",
        "        # if not torch.isfinite(x).all():\n",
        "        #     x = x[torch.isfinite(x).all(1)]\n",
        "\n",
        "        # Check shape\n",
        "        n = x.shape[0]  # number of boxes\n",
        "        if not n:  # no boxes\n",
        "            continue\n",
        "        elif n > max_nms:  # excess boxes\n",
        "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]  # sort by confidence\n",
        "\n",
        "        # Batched NMS\n",
        "        c = x[:, 5:6] * (0 if agnostic else max_wh)  # classes\n",
        "        boxes, scores = x[:, :4] + c, x[:, 4]  # boxes (offset by class), scores\n",
        "        i = torchvision.ops.nms(boxes, scores, iou_thres)  # NMS\n",
        "        if i.shape[0] > max_det:  # limit detections\n",
        "            i = i[:max_det]\n",
        "        if merge and (1 < n < 3E3):  # Merge NMS (boxes merged using weighted mean)\n",
        "            # update boxes as boxes(i,4) = weights(i,n) * boxes(n,4)\n",
        "            iou = box_iou(boxes[i], boxes) > iou_thres  # iou matrix\n",
        "            weights = iou * scores[None]  # box weights\n",
        "            x[i, :4] = torch.mm(weights, x[:, :4]).float() / weights.sum(1, keepdim=True)  # merged boxes\n",
        "            if redundant:\n",
        "                i = i[iou.sum(1) > 1]  # require redundancy\n",
        "\n",
        "        output[xi] = x[i]\n",
        "        if (time.time() - t) > time_limit:\n",
        "            print(f'WARNING: NMS time limit {time_limit}s exceeded')\n",
        "            break  # time limit exceeded\n",
        "\n",
        "    return output\n",
        "\n",
        "\n",
        "def strip_optimizer(f='best.pt', s=''):  # from utils.general import *; strip_optimizer()\n",
        "    # Strip optimizer from 'f' to finalize training, optionally save as 's'\n",
        "    x = torch.load(f, map_location=torch.device('cpu'))\n",
        "    if x.get('ema'):\n",
        "        x['model'] = x['ema']  # replace model with ema\n",
        "    for k in 'optimizer', 'training_results', 'wandb_id', 'ema', 'updates':  # keys\n",
        "        x[k] = None\n",
        "    x['epoch'] = -1\n",
        "    x['model'].half()  # to FP16\n",
        "    for p in x['model'].parameters():\n",
        "        p.requires_grad = False\n",
        "    torch.save(x, s or f)\n",
        "    mb = os.path.getsize(s or f) / 1E6  # filesize\n",
        "    print(f\"Optimizer stripped from {f},{(' saved as %s,' % s) if s else ''} {mb:.1f}MB\")\n",
        "\n",
        "\n",
        "def print_mutation(hyp, results, yaml_file='hyp_evolved.yaml', bucket=''):\n",
        "    # Print mutation results to evolve.txt (for use with train.py --evolve)\n",
        "    a = '%10s' * len(hyp) % tuple(hyp.keys())  # hyperparam keys\n",
        "    b = '%10.3g' * len(hyp) % tuple(hyp.values())  # hyperparam values\n",
        "    c = '%10.4g' * len(results) % results  # results (P, R, mAP@0.5, mAP@0.5:0.95, val_losses x 3)\n",
        "    print('\\n%s\\n%s\\nEvolved fitness: %s\\n' % (a, b, c))\n",
        "\n",
        "    if bucket:\n",
        "        url = 'gs://%s/evolve.txt' % bucket\n",
        "        if gsutil_getsize(url) > (os.path.getsize('evolve.txt') if os.path.exists('evolve.txt') else 0):\n",
        "            os.system('gsutil cp %s .' % url)  # download evolve.txt if larger than local\n",
        "\n",
        "    with open('evolve.txt', 'a') as f:  # append result\n",
        "        f.write(c + b + '\\n')\n",
        "    x = np.unique(np.loadtxt('evolve.txt', ndmin=2), axis=0)  # load unique rows\n",
        "    x = x[np.argsort(-fitness(x))]  # sort\n",
        "    np.savetxt('evolve.txt', x, '%10.3g')  # save sort by fitness\n",
        "\n",
        "    # Save yaml\n",
        "    for i, k in enumerate(hyp.keys()):\n",
        "        hyp[k] = float(x[0, i + 7])\n",
        "    with open(yaml_file, 'w') as f:\n",
        "        results = tuple(x[0, :7])\n",
        "        c = '%10.4g' * len(results) % results  # results (P, R, mAP@0.5, mAP@0.5:0.95, val_losses x 3)\n",
        "        f.write('# Hyperparameter Evolution Results\\n# Generations: %g\\n# Metrics: ' % len(x) + c + '\\n\\n')\n",
        "        yaml.safe_dump(hyp, f, sort_keys=False)\n",
        "\n",
        "    if bucket:\n",
        "        os.system('gsutil cp evolve.txt %s gs://%s' % (yaml_file, bucket))  # upload\n",
        "\n",
        "\n",
        "def apply_classifier(x, model, img, im0):\n",
        "    # Apply a second stage classifier to yolo outputs\n",
        "    im0 = [im0] if isinstance(im0, np.ndarray) else im0\n",
        "    for i, d in enumerate(x):  # per image\n",
        "        if d is not None and len(d):\n",
        "            d = d.clone()\n",
        "\n",
        "            # Reshape and pad cutouts\n",
        "            b = xyxy2xywh(d[:, :4])  # boxes\n",
        "            b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # rectangle to square\n",
        "            b[:, 2:] = b[:, 2:] * 1.3 + 30  # pad\n",
        "            d[:, :4] = xywh2xyxy(b).long()\n",
        "\n",
        "            # Rescale boxes from img_size to im0 size\n",
        "            scale_coords(img.shape[2:], d[:, :4], im0[i].shape)\n",
        "\n",
        "            # Classes\n",
        "            pred_cls1 = d[:, 5].long()\n",
        "            ims = []\n",
        "            for j, a in enumerate(d):  # per item\n",
        "                cutout = im0[i][int(a[1]):int(a[3]), int(a[0]):int(a[2])]\n",
        "                im = cv2.resize(cutout, (224, 224))  # BGR\n",
        "                # cv2.imwrite('test%i.jpg' % j, cutout)\n",
        "\n",
        "                im = im[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "                im = np.ascontiguousarray(im, dtype=np.float32)  # uint8 to float32\n",
        "                im /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "                ims.append(im)\n",
        "\n",
        "            pred_cls2 = model(torch.Tensor(ims).to(d.device)).argmax(1)  # classifier prediction\n",
        "            x[i] = x[i][pred_cls1 == pred_cls2]  # retain matching class detections\n",
        "\n",
        "    return x\n",
        "\n",
        "\n",
        "def save_one_box(xyxy, im, file='image.jpg', gain=1.02, pad=10, square=False, BGR=False):\n",
        "    # Save an image crop as {file} with crop size multiplied by {gain} and padded by {pad} pixels\n",
        "    xyxy = torch.tensor(xyxy).view(-1, 4)\n",
        "    b = xyxy2xywh(xyxy)  # boxes\n",
        "    if square:\n",
        "        b[:, 2:] = b[:, 2:].max(1)[0].unsqueeze(1)  # attempt rectangle to square\n",
        "    b[:, 2:] = b[:, 2:] * gain + pad  # box wh * gain + pad\n",
        "    xyxy = xywh2xyxy(b).long()\n",
        "    clip_coords(xyxy, im.shape)\n",
        "    crop = im[int(xyxy[0, 1]):int(xyxy[0, 3]), int(xyxy[0, 0]):int(xyxy[0, 2])]\n",
        "    cv2.imwrite(str(increment_path(file, mkdir=True).with_suffix('.jpg')), crop if BGR else crop[..., ::-1])\n",
        "\n",
        "\n",
        "def increment_path(path, exist_ok=False, sep='', mkdir=False):\n",
        "    # Increment file or directory path, i.e. runs/exp --> runs/exp{sep}2, runs/exp{sep}3, ... etc.\n",
        "    path = Path(path)  # os-agnostic\n",
        "    if path.exists() and not exist_ok:\n",
        "        suffix = path.suffix\n",
        "        path = path.with_suffix('')\n",
        "        dirs = glob.glob(f\"{path}{sep}*\")  # similar paths\n",
        "        matches = [re.search(rf\"%s{sep}(\\d+)\" % path.stem, d) for d in dirs]\n",
        "        i = [int(m.groups()[0]) for m in matches if m]  # indices\n",
        "        n = max(i) + 1 if i else 2  # increment number\n",
        "        path = Path(f\"{path}{sep}{n}{suffix}\")  # update path\n",
        "    dir = path if path.suffix == '' else path.parent  # directory\n",
        "    if not dir.exists() and mkdir:\n",
        "        dir.mkdir(parents=True, exist_ok=True)  # make directory\n",
        "    return path\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lbzmYJzsD7Dr"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting tqdm\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "Requirement already satisfied: colorama; platform_system == \"Windows\" in c:\\users\\teo shi han\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from tqdm) (0.4.4)\n",
            "Installing collected packages: tqdm\n",
            "Successfully installed tqdm-4.65.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: You are using pip version 20.2.3; however, version 23.0.1 is available.\n",
            "You should consider upgrading via the 'c:\\Users\\Teo Shi Han\\AppData\\Local\\Programs\\Python\\Python39\\python.exe -m pip install --upgrade pip' command.\n"
          ]
        }
      ],
      "source": [
        "pip install tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TVfWrMztD81h"
      },
      "outputs": [],
      "source": [
        "# Dataset utils and dataloaders\n",
        "\n",
        "import glob\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "import time\n",
        "from itertools import repeat\n",
        "from multiprocessing.pool import ThreadPool\n",
        "from pathlib import Path\n",
        "from threading import Thread\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from PIL import Image, ExifTags\n",
        "from torch.utils.data import Dataset\n",
        "from tqdm import tqdm\n",
        "\n",
        "# from utils.general import check_requirements, xyxy2xywh, xywh2xyxy, xywhn2xyxy, xyn2xy, segment2box, segments2boxes, \\\n",
        "#     resample_segments, clean_str\n",
        "# from utils.torch_utils import torch_distributed_zero_first\n",
        "\n",
        "# Parameters\n",
        "help_url = 'https://github.com/ultralytics/yolov5/wiki/Train-Custom-Data'\n",
        "img_formats = ['bmp', 'jpg', 'jpeg', 'png', 'tif', 'tiff', 'dng', 'webp', 'mpo']  # acceptable image suffixes\n",
        "vid_formats = ['mov', 'avi', 'mp4', 'mpg', 'mpeg', 'm4v', 'wmv', 'mkv']  # acceptable video suffixes\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Get orientation exif tag\n",
        "for orientation in ExifTags.TAGS.keys():\n",
        "    if ExifTags.TAGS[orientation] == 'Orientation':\n",
        "        break\n",
        "\n",
        "\n",
        "def get_hash(files):\n",
        "    # Returns a single hash value of a list of files\n",
        "    return sum(os.path.getsize(f) for f in files if os.path.isfile(f))\n",
        "\n",
        "\n",
        "def exif_size(img):\n",
        "    # Returns exif-corrected PIL size\n",
        "    s = img.size  # (width, height)\n",
        "    try:\n",
        "        rotation = dict(img._getexif().items())[orientation]\n",
        "        if rotation == 6:  # rotation 270\n",
        "            s = (s[1], s[0])\n",
        "        elif rotation == 8:  # rotation 90\n",
        "            s = (s[1], s[0])\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return s\n",
        "\n",
        "\n",
        "def create_dataloader(path, imgsz, batch_size, stride, opt, hyp=None, augment=False, cache=False, pad=0.0, rect=False,\n",
        "                      rank=-1, world_size=1, workers=8, image_weights=False, quad=False, prefix=''):\n",
        "    # Make sure only the first process in DDP process the dataset first, and the following others can use the cache\n",
        "    with torch_distributed_zero_first(rank):\n",
        "        dataset = LoadImagesAndLabels(path, imgsz, batch_size,\n",
        "                                      augment=augment,  # augment images\n",
        "                                      hyp=hyp,  # augmentation hyperparameters\n",
        "                                      rect=rect,  # rectangular training\n",
        "                                      cache_images=cache,\n",
        "                                      single_cls=opt.single_cls,\n",
        "                                      stride=int(stride),\n",
        "                                      pad=pad,\n",
        "                                      image_weights=image_weights,\n",
        "                                      prefix=prefix)\n",
        "\n",
        "    batch_size = min(batch_size, len(dataset))\n",
        "    nw = min([os.cpu_count() // world_size, batch_size if batch_size > 1 else 0, workers])  # number of workers\n",
        "    sampler = torch.utils.data.distributed.DistributedSampler(dataset) if rank != -1 else None\n",
        "    loader = torch.utils.data.DataLoader if image_weights else InfiniteDataLoader\n",
        "    # Use torch.utils.data.DataLoader() if dataset.properties will update during training else InfiniteDataLoader()\n",
        "    dataloader = loader(dataset,\n",
        "                        batch_size=batch_size,\n",
        "                        num_workers=nw,\n",
        "                        sampler=sampler,\n",
        "                        pin_memory=True,\n",
        "                        collate_fn=LoadImagesAndLabels.collate_fn4 if quad else LoadImagesAndLabels.collate_fn)\n",
        "    return dataloader, dataset\n",
        "\n",
        "\n",
        "class InfiniteDataLoader(torch.utils.data.dataloader.DataLoader):\n",
        "    \"\"\" Dataloader that reuses workers\n",
        "\n",
        "    Uses same syntax as vanilla DataLoader\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super().__init__(*args, **kwargs)\n",
        "        object.__setattr__(self, 'batch_sampler', _RepeatSampler(self.batch_sampler))\n",
        "        self.iterator = super().__iter__()\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.batch_sampler.sampler)\n",
        "\n",
        "    def __iter__(self):\n",
        "        for i in range(len(self)):\n",
        "            yield next(self.iterator)\n",
        "\n",
        "\n",
        "class _RepeatSampler(object):\n",
        "    \"\"\" Sampler that repeats forever\n",
        "\n",
        "    Args:\n",
        "        sampler (Sampler)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sampler):\n",
        "        self.sampler = sampler\n",
        "\n",
        "    def __iter__(self):\n",
        "        while True:\n",
        "            yield from iter(self.sampler)\n",
        "\n",
        "\n",
        "class LoadImages:  # for inference\n",
        "    def __init__(self, path, img_size=640, stride=32):\n",
        "        p = str(Path(path).absolute())  # os-agnostic absolute path\n",
        "        if '*' in p:\n",
        "            files = sorted(glob.glob(p, recursive=True))  # glob\n",
        "        elif os.path.isdir(p):\n",
        "            files = sorted(glob.glob(os.path.join(p, '*.*')))  # dir\n",
        "        elif os.path.isfile(p):\n",
        "            files = [p]  # files\n",
        "        else:\n",
        "            raise Exception(f'ERROR: {p} does not exist')\n",
        "\n",
        "        images = [x for x in files if x.split('.')[-1].lower() in img_formats]\n",
        "        videos = [x for x in files if x.split('.')[-1].lower() in vid_formats]\n",
        "        ni, nv = len(images), len(videos)\n",
        "\n",
        "        self.img_size = img_size\n",
        "        self.stride = stride\n",
        "        self.files = images + videos\n",
        "        self.nf = ni + nv  # number of files\n",
        "        self.video_flag = [False] * ni + [True] * nv\n",
        "        self.mode = 'image'\n",
        "        if any(videos):\n",
        "            self.new_video(videos[0])  # new video\n",
        "        else:\n",
        "            self.cap = None\n",
        "        assert self.nf > 0, f'No images or videos found in {p}. ' \\\n",
        "                            f'Supported formats are:\\nimages: {img_formats}\\nvideos: {vid_formats}'\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = 0\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        if self.count == self.nf:\n",
        "            raise StopIteration\n",
        "        path = self.files[self.count]\n",
        "\n",
        "        if self.video_flag[self.count]:\n",
        "            # Read video\n",
        "            self.mode = 'video'\n",
        "            ret_val, img0 = self.cap.read()\n",
        "            if not ret_val:\n",
        "                self.count += 1\n",
        "                self.cap.release()\n",
        "                if self.count == self.nf:  # last video\n",
        "                    raise StopIteration\n",
        "                else:\n",
        "                    path = self.files[self.count]\n",
        "                    self.new_video(path)\n",
        "                    ret_val, img0 = self.cap.read()\n",
        "\n",
        "            self.frame += 1\n",
        "            print(f'video {self.count + 1}/{self.nf} ({self.frame}/{self.frames}) {path}: ', end='')\n",
        "\n",
        "        else:\n",
        "            # Read image\n",
        "            self.count += 1\n",
        "            img0 = cv2.imread(path)  # BGR\n",
        "            assert img0 is not None, 'Image Not Found ' + path\n",
        "            print(f'image {self.count}/{self.nf} {path}: ', end='')\n",
        "\n",
        "        # Padded resize\n",
        "        img = letterbox(img0, self.img_size, stride=self.stride)[0]\n",
        "\n",
        "        # Convert\n",
        "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        return path, img, img0, self.cap\n",
        "\n",
        "    def new_video(self, path):\n",
        "        self.frame = 0\n",
        "        self.cap = cv2.VideoCapture(path)\n",
        "        self.frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.nf  # number of files\n",
        "\n",
        "\n",
        "class LoadWebcam:  # for inference\n",
        "    def __init__(self, pipe='0', img_size=640, stride=32):\n",
        "        self.img_size = img_size\n",
        "        self.stride = stride\n",
        "\n",
        "        if pipe.isnumeric():\n",
        "            pipe = eval(pipe)  # local camera\n",
        "        # pipe = 'rtsp://192.168.1.64/1'  # IP camera\n",
        "        # pipe = 'rtsp://username:password@192.168.1.64/1'  # IP camera with login\n",
        "        # pipe = 'http://wmccpinetop.axiscam.net/mjpg/video.mjpg'  # IP golf camera\n",
        "\n",
        "        self.pipe = pipe\n",
        "        self.cap = cv2.VideoCapture(pipe)  # video capture object\n",
        "        self.cap.set(cv2.CAP_PROP_BUFFERSIZE, 3)  # set buffer size\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = -1\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        self.count += 1\n",
        "        if cv2.waitKey(1) == ord('q'):  # q to quit\n",
        "            self.cap.release()\n",
        "            cv2.destroyAllWindows()\n",
        "            raise StopIteration\n",
        "\n",
        "        # Read frame\n",
        "        if self.pipe == 0:  # local camera\n",
        "            ret_val, img0 = self.cap.read()\n",
        "            img0 = cv2.flip(img0, 1)  # flip left-right\n",
        "        else:  # IP camera\n",
        "            n = 0\n",
        "            while True:\n",
        "                n += 1\n",
        "                self.cap.grab()\n",
        "                if n % 30 == 0:  # skip frames\n",
        "                    ret_val, img0 = self.cap.retrieve()\n",
        "                    if ret_val:\n",
        "                        break\n",
        "\n",
        "        # Print\n",
        "        assert ret_val, f'Camera Error {self.pipe}'\n",
        "        img_path = 'webcam.jpg'\n",
        "        print(f'webcam {self.count}: ', end='')\n",
        "\n",
        "        # Padded resize\n",
        "        img = letterbox(img0, self.img_size, stride=self.stride)[0]\n",
        "\n",
        "        # Convert\n",
        "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        return img_path, img, img0, None\n",
        "\n",
        "    def __len__(self):\n",
        "        return 0\n",
        "\n",
        "\n",
        "class LoadStreams:  # multiple IP or RTSP cameras\n",
        "    def __init__(self, sources='streams.txt', img_size=640, stride=32):\n",
        "        self.mode = 'stream'\n",
        "        self.img_size = img_size\n",
        "        self.stride = stride\n",
        "\n",
        "        if os.path.isfile(sources):\n",
        "            with open(sources, 'r') as f:\n",
        "                sources = [x.strip() for x in f.read().strip().splitlines() if len(x.strip())]\n",
        "        else:\n",
        "            sources = [sources]\n",
        "\n",
        "        n = len(sources)\n",
        "        self.imgs = [None] * n\n",
        "        self.sources = [clean_str(x) for x in sources]  # clean source names for later\n",
        "        for i, s in enumerate(sources):  # index, source\n",
        "            # Start thread to read frames from video stream\n",
        "            print(f'{i + 1}/{n}: {s}... ', end='')\n",
        "            if 'youtube.com/' in s or 'youtu.be/' in s:  # if source is YouTube video\n",
        "                check_requirements(('pafy', 'youtube_dl'))\n",
        "                import pafy\n",
        "                s = pafy.new(s).getbest(preftype=\"mp4\").url  # YouTube URL\n",
        "            s = eval(s) if s.isnumeric() else s  # i.e. s = '0' local webcam\n",
        "            cap = cv2.VideoCapture(s)\n",
        "            assert cap.isOpened(), f'Failed to open {s}'\n",
        "            w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "            h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "            self.fps = cap.get(cv2.CAP_PROP_FPS) % 100\n",
        "            self.frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "            _, self.imgs[i] = cap.read()  # guarantee first frame\n",
        "            thread = Thread(target=self.update, args=([i, cap]), daemon=True)\n",
        "            print(f\" success ({f'{self.frames} frames ' if self.frames else ''}{w}x{h} at {self.fps:.2f} FPS).\")\n",
        "            thread.start()\n",
        "        print('')  # newline\n",
        "\n",
        "        # check for common shapes\n",
        "        s = np.stack([letterbox(x, self.img_size, stride=self.stride)[0].shape for x in self.imgs], 0)  # shapes\n",
        "        self.rect = np.unique(s, axis=0).shape[0] == 1  # rect inference if all shapes equal\n",
        "        if not self.rect:\n",
        "            print('WARNING: Different stream shapes detected. For optimal performance supply similarly-shaped streams.')\n",
        "\n",
        "    def update(self, index, cap):\n",
        "        # Read next stream frame in a daemon thread\n",
        "        n = 0\n",
        "        while cap.isOpened():\n",
        "            n += 1\n",
        "            # _, self.imgs[index] = cap.read()\n",
        "            cap.grab()\n",
        "            if n == 4:  # read every 4th frame\n",
        "                success, im = cap.retrieve()\n",
        "                self.imgs[index] = im if success else self.imgs[index] * 0\n",
        "                n = 0\n",
        "            time.sleep(1 / self.fps)  # wait time\n",
        "\n",
        "    def __iter__(self):\n",
        "        self.count = -1\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        self.count += 1\n",
        "        img0 = self.imgs.copy()\n",
        "        if cv2.waitKey(1) == ord('q'):  # q to quit\n",
        "            cv2.destroyAllWindows()\n",
        "            raise StopIteration\n",
        "\n",
        "        # Letterbox\n",
        "        img = [letterbox(x, self.img_size, auto=self.rect, stride=self.stride)[0] for x in img0]\n",
        "\n",
        "        # Stack\n",
        "        img = np.stack(img, 0)\n",
        "\n",
        "        # Convert\n",
        "        img = img[:, :, :, ::-1].transpose(0, 3, 1, 2)  # BGR to RGB, to bsx3x416x416\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        return self.sources, img, img0, None\n",
        "\n",
        "    def __len__(self):\n",
        "        return 0  # 1E12 frames = 32 streams at 30 FPS for 30 years\n",
        "\n",
        "\n",
        "def img2label_paths(img_paths):\n",
        "    # Define label paths as a function of image paths\n",
        "    sa, sb = os.sep + 'images' + os.sep, os.sep + 'labels' + os.sep  # /images/, /labels/ substrings\n",
        "    return ['txt'.join(x.replace(sa, sb, 1).rsplit(x.split('.')[-1], 1)) for x in img_paths]\n",
        "\n",
        "\n",
        "class LoadImagesAndLabels(Dataset):  # for training/testing\n",
        "    def __init__(self, path, img_size=640, batch_size=16, augment=False, hyp=None, rect=False, image_weights=False,\n",
        "                 cache_images=False, single_cls=False, stride=32, pad=0.0, prefix=''):\n",
        "        self.img_size = img_size\n",
        "        self.augment = augment\n",
        "        self.hyp = hyp\n",
        "        self.image_weights = image_weights\n",
        "        self.rect = False if image_weights else rect\n",
        "        self.mosaic = self.augment and not self.rect  # load 4 images at a time into a mosaic (only during training)\n",
        "        self.mosaic_border = [-img_size // 2, -img_size // 2]\n",
        "        self.stride = stride\n",
        "        self.path = path\n",
        "\n",
        "        try:\n",
        "            f = []  # image files\n",
        "            for p in path if isinstance(path, list) else [path]:\n",
        "                p = Path(p)  # os-agnostic\n",
        "                if p.is_dir():  # dir\n",
        "                    f += glob.glob(str(p / '**' / '*.*'), recursive=True)\n",
        "                    # f = list(p.rglob('**/*.*'))  # pathlib\n",
        "                elif p.is_file():  # file\n",
        "                    with open(p, 'r') as t:\n",
        "                        t = t.read().strip().splitlines()\n",
        "                        parent = str(p.parent) + os.sep\n",
        "                        f += [x.replace('./', parent) if x.startswith('./') else x for x in t]  # local to global path\n",
        "                        # f += [p.parent / x.lstrip(os.sep) for x in t]  # local to global path (pathlib)\n",
        "                else:\n",
        "                    raise Exception(f'{prefix}{p} does not exist')\n",
        "            self.img_files = sorted([x.replace('/', os.sep) for x in f if x.split('.')[-1].lower() in img_formats])\n",
        "            # self.img_files = sorted([x for x in f if x.suffix[1:].lower() in img_formats])  # pathlib\n",
        "            assert self.img_files, f'{prefix}No images found'\n",
        "        except Exception as e:\n",
        "            raise Exception(f'{prefix}Error loading data from {path}: {e}\\nSee {help_url}')\n",
        "\n",
        "        # Check cache\n",
        "        self.label_files = img2label_paths(self.img_files)  # labels\n",
        "        cache_path = (p if p.is_file() else Path(self.label_files[0]).parent).with_suffix('.cache')  # cached labels\n",
        "        if cache_path.is_file():\n",
        "            cache, exists = torch.load(cache_path), True  # load\n",
        "            if cache['hash'] != get_hash(self.label_files + self.img_files) or 'version' not in cache:  # changed\n",
        "                cache, exists = self.cache_labels(cache_path, prefix), False  # re-cache\n",
        "        else:\n",
        "            cache, exists = self.cache_labels(cache_path, prefix), False  # cache\n",
        "\n",
        "        # Display cache\n",
        "        nf, nm, ne, nc, n = cache.pop('results')  # found, missing, empty, corrupted, total\n",
        "        if exists:\n",
        "            d = f\"Scanning '{cache_path}' images and labels... {nf} found, {nm} missing, {ne} empty, {nc} corrupted\"\n",
        "            tqdm(None, desc=prefix + d, total=n, initial=n)  # display cache results\n",
        "        assert nf > 0 or not augment, f'{prefix}No labels in {cache_path}. Can not train without labels. See {help_url}'\n",
        "\n",
        "        # Read cache\n",
        "        cache.pop('hash')  # remove hash\n",
        "        cache.pop('version')  # remove version\n",
        "        labels, shapes, self.segments = zip(*cache.values())\n",
        "        self.labels = list(labels)\n",
        "        self.shapes = np.array(shapes, dtype=np.float64)\n",
        "        self.img_files = list(cache.keys())  # update\n",
        "        self.label_files = img2label_paths(cache.keys())  # update\n",
        "        if single_cls:\n",
        "            for x in self.labels:\n",
        "                x[:, 0] = 0\n",
        "\n",
        "        n = len(shapes)  # number of images\n",
        "        bi = np.floor(np.arange(n) / batch_size).astype(np.int)  # batch index\n",
        "        nb = bi[-1] + 1  # number of batches\n",
        "        self.batch = bi  # batch index of image\n",
        "        self.n = n\n",
        "        self.indices = range(n)\n",
        "\n",
        "        # Rectangular Training\n",
        "        if self.rect:\n",
        "            # Sort by aspect ratio\n",
        "            s = self.shapes  # wh\n",
        "            ar = s[:, 1] / s[:, 0]  # aspect ratio\n",
        "            irect = ar.argsort()\n",
        "            self.img_files = [self.img_files[i] for i in irect]\n",
        "            self.label_files = [self.label_files[i] for i in irect]\n",
        "            self.labels = [self.labels[i] for i in irect]\n",
        "            self.shapes = s[irect]  # wh\n",
        "            ar = ar[irect]\n",
        "\n",
        "            # Set training image shapes\n",
        "            shapes = [[1, 1]] * nb\n",
        "            for i in range(nb):\n",
        "                ari = ar[bi == i]\n",
        "                mini, maxi = ari.min(), ari.max()\n",
        "                if maxi < 1:\n",
        "                    shapes[i] = [maxi, 1]\n",
        "                elif mini > 1:\n",
        "                    shapes[i] = [1, 1 / mini]\n",
        "\n",
        "            self.batch_shapes = np.ceil(np.array(shapes) * img_size / stride + pad).astype(np.int) * stride\n",
        "\n",
        "        # Cache images into memory for faster training (WARNING: large datasets may exceed system RAM)\n",
        "        self.imgs = [None] * n\n",
        "        if cache_images:\n",
        "            gb = 0  # Gigabytes of cached images\n",
        "            self.img_hw0, self.img_hw = [None] * n, [None] * n\n",
        "            results = ThreadPool(8).imap(lambda x: load_image(*x), zip(repeat(self), range(n)))  # 8 threads\n",
        "            pbar = tqdm(enumerate(results), total=n)\n",
        "            for i, x in pbar:\n",
        "                self.imgs[i], self.img_hw0[i], self.img_hw[i] = x  # img, hw_original, hw_resized = load_image(self, i)\n",
        "                gb += self.imgs[i].nbytes\n",
        "                pbar.desc = f'{prefix}Caching images ({gb / 1E9:.1f}GB)'\n",
        "            pbar.close()\n",
        "\n",
        "    def cache_labels(self, path=Path('./labels.cache'), prefix=''):\n",
        "        # Cache dataset labels, check images and read shapes\n",
        "        x = {}  # dict\n",
        "        nm, nf, ne, nc = 0, 0, 0, 0  # number missing, found, empty, duplicate\n",
        "        pbar = tqdm(zip(self.img_files, self.label_files), desc='Scanning images', total=len(self.img_files))\n",
        "        for i, (im_file, lb_file) in enumerate(pbar):\n",
        "            try:\n",
        "                # verify images\n",
        "                im = Image.open(im_file)\n",
        "                im.verify()  # PIL verify\n",
        "                shape = exif_size(im)  # image size\n",
        "                segments = []  # instance segments\n",
        "                assert (shape[0] > 9) & (shape[1] > 9), f'image size {shape} <10 pixels'\n",
        "                assert im.format.lower() in img_formats, f'invalid image format {im.format}'\n",
        "\n",
        "                # verify labels\n",
        "                if os.path.isfile(lb_file):\n",
        "                    nf += 1  # label found\n",
        "                    with open(lb_file, 'r') as f:\n",
        "                        l = [x.split() for x in f.read().strip().splitlines()]\n",
        "                        if any([len(x) > 8 for x in l]):  # is segment\n",
        "                            classes = np.array([x[0] for x in l], dtype=np.float32)\n",
        "                            segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in l]  # (cls, xy1...)\n",
        "                            l = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (cls, xywh)\n",
        "                        l = np.array(l, dtype=np.float32)\n",
        "                    if len(l):\n",
        "                        assert l.shape[1] == 5, 'labels require 5 columns each'\n",
        "                        assert (l >= 0).all(), 'negative labels'\n",
        "                        assert (l[:, 1:] <= 1).all(), 'non-normalized or out of bounds coordinate labels'\n",
        "                        assert np.unique(l, axis=0).shape[0] == l.shape[0], 'duplicate labels'\n",
        "                    else:\n",
        "                        ne += 1  # label empty\n",
        "                        l = np.zeros((0, 5), dtype=np.float32)\n",
        "                else:\n",
        "                    nm += 1  # label missing\n",
        "                    l = np.zeros((0, 5), dtype=np.float32)\n",
        "                x[im_file] = [l, shape, segments]\n",
        "            except Exception as e:\n",
        "                nc += 1\n",
        "                logging.info(f'{prefix}WARNING: Ignoring corrupted image and/or label {im_file}: {e}')\n",
        "\n",
        "            pbar.desc = f\"{prefix}Scanning '{path.parent / path.stem}' images and labels... \" \\\n",
        "                        f\"{nf} found, {nm} missing, {ne} empty, {nc} corrupted\"\n",
        "        pbar.close()\n",
        "\n",
        "        if nf == 0:\n",
        "            logging.info(f'{prefix}WARNING: No labels found in {path}. See {help_url}')\n",
        "\n",
        "        x['hash'] = get_hash(self.label_files + self.img_files)\n",
        "        x['results'] = nf, nm, ne, nc, i + 1\n",
        "        x['version'] = 0.1  # cache version\n",
        "        try:\n",
        "            torch.save(x, path)  # save for next time\n",
        "            logging.info(f'{prefix}New cache created: {path}')\n",
        "        except Exception as e:\n",
        "            logging.info(f'{prefix}WARNING: Cache directory {path.parent} is not writeable: {e}')  # path not writeable\n",
        "        return x\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_files)\n",
        "\n",
        "    # def __iter__(self):\n",
        "    #     self.count = -1\n",
        "    #     print('ran dataset iter')\n",
        "    #     #self.shuffled_vector = np.random.permutation(self.nF) if self.augment else np.arange(self.nF)\n",
        "    #     return self\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        index = self.indices[index]  # linear, shuffled, or image_weights\n",
        "\n",
        "        hyp = self.hyp\n",
        "        mosaic = self.mosaic and random.random() < hyp['mosaic']\n",
        "        if mosaic:\n",
        "            # Load mosaic\n",
        "            img, labels = load_mosaic(self, index)\n",
        "            shapes = None\n",
        "\n",
        "            # MixUp https://arxiv.org/pdf/1710.09412.pdf\n",
        "            if random.random() < hyp['mixup']:\n",
        "                img2, labels2 = load_mosaic(self, random.randint(0, self.n - 1))\n",
        "                r = np.random.beta(8.0, 8.0)  # mixup ratio, alpha=beta=8.0\n",
        "                img = (img * r + img2 * (1 - r)).astype(np.uint8)\n",
        "                labels = np.concatenate((labels, labels2), 0)\n",
        "\n",
        "        else:\n",
        "            # Load image\n",
        "            img, (h0, w0), (h, w) = load_image(self, index)\n",
        "\n",
        "            # Letterbox\n",
        "            shape = self.batch_shapes[self.batch[index]] if self.rect else self.img_size  # final letterboxed shape\n",
        "            img, ratio, pad = letterbox(img, shape, auto=False, scaleup=self.augment)\n",
        "            shapes = (h0, w0), ((h / h0, w / w0), pad)  # for COCO mAP rescaling\n",
        "\n",
        "            labels = self.labels[index].copy()\n",
        "            if labels.size:  # normalized xywh to pixel xyxy format\n",
        "                labels[:, 1:] = xywhn2xyxy(labels[:, 1:], ratio[0] * w, ratio[1] * h, padw=pad[0], padh=pad[1])\n",
        "\n",
        "        if self.augment:\n",
        "            # Augment imagespace\n",
        "            if not mosaic:\n",
        "                img, labels = random_perspective(img, labels,\n",
        "                                                 degrees=hyp['degrees'],\n",
        "                                                 translate=hyp['translate'],\n",
        "                                                 scale=hyp['scale'],\n",
        "                                                 shear=hyp['shear'],\n",
        "                                                 perspective=hyp['perspective'])\n",
        "\n",
        "            # Augment colorspace\n",
        "            augment_hsv(img, hgain=hyp['hsv_h'], sgain=hyp['hsv_s'], vgain=hyp['hsv_v'])\n",
        "\n",
        "            # Apply cutouts\n",
        "            # if random.random() < 0.9:\n",
        "            #     labels = cutout(img, labels)\n",
        "\n",
        "        nL = len(labels)  # number of labels\n",
        "        if nL:\n",
        "            labels[:, 1:5] = xyxy2xywh(labels[:, 1:5])  # convert xyxy to xywh\n",
        "            labels[:, [2, 4]] /= img.shape[0]  # normalized height 0-1\n",
        "            labels[:, [1, 3]] /= img.shape[1]  # normalized width 0-1\n",
        "\n",
        "        if self.augment:\n",
        "            # flip up-down\n",
        "            if random.random() < hyp['flipud']:\n",
        "                img = np.flipud(img)\n",
        "                if nL:\n",
        "                    labels[:, 2] = 1 - labels[:, 2]\n",
        "\n",
        "            # flip left-right\n",
        "            if random.random() < hyp['fliplr']:\n",
        "                img = np.fliplr(img)\n",
        "                if nL:\n",
        "                    labels[:, 1] = 1 - labels[:, 1]\n",
        "\n",
        "        labels_out = torch.zeros((nL, 6))\n",
        "        if nL:\n",
        "            labels_out[:, 1:] = torch.from_numpy(labels)\n",
        "\n",
        "        # Convert\n",
        "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "        img = np.ascontiguousarray(img)\n",
        "\n",
        "        return torch.from_numpy(img), labels_out, self.img_files[index], shapes\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn(batch):\n",
        "        img, label, path, shapes = zip(*batch)  # transposed\n",
        "        for i, l in enumerate(label):\n",
        "            l[:, 0] = i  # add target image index for build_targets()\n",
        "        return torch.stack(img, 0), torch.cat(label, 0), path, shapes\n",
        "\n",
        "    @staticmethod\n",
        "    def collate_fn4(batch):\n",
        "        img, label, path, shapes = zip(*batch)  # transposed\n",
        "        n = len(shapes) // 4\n",
        "        img4, label4, path4, shapes4 = [], [], path[:n], shapes[:n]\n",
        "\n",
        "        ho = torch.tensor([[0., 0, 0, 1, 0, 0]])\n",
        "        wo = torch.tensor([[0., 0, 1, 0, 0, 0]])\n",
        "        s = torch.tensor([[1, 1, .5, .5, .5, .5]])  # scale\n",
        "        for i in range(n):  # zidane torch.zeros(16,3,720,1280)  # BCHW\n",
        "            i *= 4\n",
        "            if random.random() < 0.5:\n",
        "                im = F.interpolate(img[i].unsqueeze(0).float(), scale_factor=2., mode='bilinear', align_corners=False)[\n",
        "                    0].type(img[i].type())\n",
        "                l = label[i]\n",
        "            else:\n",
        "                im = torch.cat((torch.cat((img[i], img[i + 1]), 1), torch.cat((img[i + 2], img[i + 3]), 1)), 2)\n",
        "                l = torch.cat((label[i], label[i + 1] + ho, label[i + 2] + wo, label[i + 3] + ho + wo), 0) * s\n",
        "            img4.append(im)\n",
        "            label4.append(l)\n",
        "\n",
        "        for i, l in enumerate(label4):\n",
        "            l[:, 0] = i  # add target image index for build_targets()\n",
        "\n",
        "        return torch.stack(img4, 0), torch.cat(label4, 0), path4, shapes4\n",
        "\n",
        "\n",
        "# Ancillary functions --------------------------------------------------------------------------------------------------\n",
        "def load_image(self, index):\n",
        "    # loads 1 image from dataset, returns img, original hw, resized hw\n",
        "    img = self.imgs[index]\n",
        "    if img is None:  # not cached\n",
        "        path = self.img_files[index]\n",
        "        img = cv2.imread(path)  # BGR\n",
        "        assert img is not None, 'Image Not Found ' + path\n",
        "        h0, w0 = img.shape[:2]  # orig hw\n",
        "        r = self.img_size / max(h0, w0)  # ratio\n",
        "        if r != 1:  # if sizes are not equal\n",
        "            img = cv2.resize(img, (int(w0 * r), int(h0 * r)),\n",
        "                             interpolation=cv2.INTER_AREA if r < 1 and not self.augment else cv2.INTER_LINEAR)\n",
        "        return img, (h0, w0), img.shape[:2]  # img, hw_original, hw_resized\n",
        "    else:\n",
        "        return self.imgs[index], self.img_hw0[index], self.img_hw[index]  # img, hw_original, hw_resized\n",
        "\n",
        "\n",
        "def augment_hsv(img, hgain=0.5, sgain=0.5, vgain=0.5):\n",
        "    r = np.random.uniform(-1, 1, 3) * [hgain, sgain, vgain] + 1  # random gains\n",
        "    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))\n",
        "    dtype = img.dtype  # uint8\n",
        "\n",
        "    x = np.arange(0, 256, dtype=np.int16)\n",
        "    lut_hue = ((x * r[0]) % 180).astype(dtype)\n",
        "    lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)\n",
        "    lut_val = np.clip(x * r[2], 0, 255).astype(dtype)\n",
        "\n",
        "    img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(dtype)\n",
        "    cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR, dst=img)  # no return needed\n",
        "\n",
        "\n",
        "def hist_equalize(img, clahe=True, bgr=False):\n",
        "    # Equalize histogram on BGR image 'img' with img.shape(n,m,3) and range 0-255\n",
        "    yuv = cv2.cvtColor(img, cv2.COLOR_BGR2YUV if bgr else cv2.COLOR_RGB2YUV)\n",
        "    if clahe:\n",
        "        c = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "        yuv[:, :, 0] = c.apply(yuv[:, :, 0])\n",
        "    else:\n",
        "        yuv[:, :, 0] = cv2.equalizeHist(yuv[:, :, 0])  # equalize Y channel histogram\n",
        "    return cv2.cvtColor(yuv, cv2.COLOR_YUV2BGR if bgr else cv2.COLOR_YUV2RGB)  # convert YUV image to RGB\n",
        "\n",
        "\n",
        "def load_mosaic(self, index):\n",
        "    # loads images in a 4-mosaic\n",
        "\n",
        "    labels4, segments4 = [], []\n",
        "    s = self.img_size\n",
        "    yc, xc = [int(random.uniform(-x, 2 * s + x)) for x in self.mosaic_border]  # mosaic center x, y\n",
        "    indices = [index] + random.choices(self.indices, k=3)  # 3 additional image indices\n",
        "    for i, index in enumerate(indices):\n",
        "        # Load image\n",
        "        img, _, (h, w) = load_image(self, index)\n",
        "\n",
        "        # place img in img4\n",
        "        if i == 0:  # top left\n",
        "            img4 = np.full((s * 2, s * 2, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n",
        "            x1a, y1a, x2a, y2a = max(xc - w, 0), max(yc - h, 0), xc, yc  # xmin, ymin, xmax, ymax (large image)\n",
        "            x1b, y1b, x2b, y2b = w - (x2a - x1a), h - (y2a - y1a), w, h  # xmin, ymin, xmax, ymax (small image)\n",
        "        elif i == 1:  # top right\n",
        "            x1a, y1a, x2a, y2a = xc, max(yc - h, 0), min(xc + w, s * 2), yc\n",
        "            x1b, y1b, x2b, y2b = 0, h - (y2a - y1a), min(w, x2a - x1a), h\n",
        "        elif i == 2:  # bottom left\n",
        "            x1a, y1a, x2a, y2a = max(xc - w, 0), yc, xc, min(s * 2, yc + h)\n",
        "            x1b, y1b, x2b, y2b = w - (x2a - x1a), 0, w, min(y2a - y1a, h)\n",
        "        elif i == 3:  # bottom right\n",
        "            x1a, y1a, x2a, y2a = xc, yc, min(xc + w, s * 2), min(s * 2, yc + h)\n",
        "            x1b, y1b, x2b, y2b = 0, 0, min(w, x2a - x1a), min(y2a - y1a, h)\n",
        "\n",
        "        img4[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n",
        "        padw = x1a - x1b\n",
        "        padh = y1a - y1b\n",
        "\n",
        "        # Labels\n",
        "        labels, segments = self.labels[index].copy(), self.segments[index].copy()\n",
        "        if labels.size:\n",
        "            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padw, padh)  # normalized xywh to pixel xyxy format\n",
        "            segments = [xyn2xy(x, w, h, padw, padh) for x in segments]\n",
        "        labels4.append(labels)\n",
        "        segments4.extend(segments)\n",
        "\n",
        "    # Concat/clip labels\n",
        "    labels4 = np.concatenate(labels4, 0)\n",
        "    for x in (labels4[:, 1:], *segments4):\n",
        "        np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()\n",
        "    # img4, labels4 = replicate(img4, labels4)  # replicate\n",
        "\n",
        "    # Augment\n",
        "    img4, labels4 = random_perspective(img4, labels4, segments4,\n",
        "                                       degrees=self.hyp['degrees'],\n",
        "                                       translate=self.hyp['translate'],\n",
        "                                       scale=self.hyp['scale'],\n",
        "                                       shear=self.hyp['shear'],\n",
        "                                       perspective=self.hyp['perspective'],\n",
        "                                       border=self.mosaic_border)  # border to remove\n",
        "\n",
        "    return img4, labels4\n",
        "\n",
        "\n",
        "def load_mosaic9(self, index):\n",
        "    # loads images in a 9-mosaic\n",
        "\n",
        "    labels9, segments9 = [], []\n",
        "    s = self.img_size\n",
        "    indices = [index] + random.choices(self.indices, k=8)  # 8 additional image indices\n",
        "    for i, index in enumerate(indices):\n",
        "        # Load image\n",
        "        img, _, (h, w) = load_image(self, index)\n",
        "\n",
        "        # place img in img9\n",
        "        if i == 0:  # center\n",
        "            img9 = np.full((s * 3, s * 3, img.shape[2]), 114, dtype=np.uint8)  # base image with 4 tiles\n",
        "            h0, w0 = h, w\n",
        "            c = s, s, s + w, s + h  # xmin, ymin, xmax, ymax (base) coordinates\n",
        "        elif i == 1:  # top\n",
        "            c = s, s - h, s + w, s\n",
        "        elif i == 2:  # top right\n",
        "            c = s + wp, s - h, s + wp + w, s\n",
        "        elif i == 3:  # right\n",
        "            c = s + w0, s, s + w0 + w, s + h\n",
        "        elif i == 4:  # bottom right\n",
        "            c = s + w0, s + hp, s + w0 + w, s + hp + h\n",
        "        elif i == 5:  # bottom\n",
        "            c = s + w0 - w, s + h0, s + w0, s + h0 + h\n",
        "        elif i == 6:  # bottom left\n",
        "            c = s + w0 - wp - w, s + h0, s + w0 - wp, s + h0 + h\n",
        "        elif i == 7:  # left\n",
        "            c = s - w, s + h0 - h, s, s + h0\n",
        "        elif i == 8:  # top left\n",
        "            c = s - w, s + h0 - hp - h, s, s + h0 - hp\n",
        "\n",
        "        padx, pady = c[:2]\n",
        "        x1, y1, x2, y2 = [max(x, 0) for x in c]  # allocate coords\n",
        "\n",
        "        # Labels\n",
        "        labels, segments = self.labels[index].copy(), self.segments[index].copy()\n",
        "        if labels.size:\n",
        "            labels[:, 1:] = xywhn2xyxy(labels[:, 1:], w, h, padx, pady)  # normalized xywh to pixel xyxy format\n",
        "            segments = [xyn2xy(x, w, h, padx, pady) for x in segments]\n",
        "        labels9.append(labels)\n",
        "        segments9.extend(segments)\n",
        "\n",
        "        # Image\n",
        "        img9[y1:y2, x1:x2] = img[y1 - pady:, x1 - padx:]  # img9[ymin:ymax, xmin:xmax]\n",
        "        hp, wp = h, w  # height, width previous\n",
        "\n",
        "    # Offset\n",
        "    yc, xc = [int(random.uniform(0, s)) for _ in self.mosaic_border]  # mosaic center x, y\n",
        "    img9 = img9[yc:yc + 2 * s, xc:xc + 2 * s]\n",
        "\n",
        "    # Concat/clip labels\n",
        "    labels9 = np.concatenate(labels9, 0)\n",
        "    labels9[:, [1, 3]] -= xc\n",
        "    labels9[:, [2, 4]] -= yc\n",
        "    c = np.array([xc, yc])  # centers\n",
        "    segments9 = [x - c for x in segments9]\n",
        "\n",
        "    for x in (labels9[:, 1:], *segments9):\n",
        "        np.clip(x, 0, 2 * s, out=x)  # clip when using random_perspective()\n",
        "    # img9, labels9 = replicate(img9, labels9)  # replicate\n",
        "\n",
        "    # Augment\n",
        "    img9, labels9 = random_perspective(img9, labels9, segments9,\n",
        "                                       degrees=self.hyp['degrees'],\n",
        "                                       translate=self.hyp['translate'],\n",
        "                                       scale=self.hyp['scale'],\n",
        "                                       shear=self.hyp['shear'],\n",
        "                                       perspective=self.hyp['perspective'],\n",
        "                                       border=self.mosaic_border)  # border to remove\n",
        "\n",
        "    return img9, labels9\n",
        "\n",
        "\n",
        "def replicate(img, labels):\n",
        "    # Replicate labels\n",
        "    h, w = img.shape[:2]\n",
        "    boxes = labels[:, 1:].astype(int)\n",
        "    x1, y1, x2, y2 = boxes.T\n",
        "    s = ((x2 - x1) + (y2 - y1)) / 2  # side length (pixels)\n",
        "    for i in s.argsort()[:round(s.size * 0.5)]:  # smallest indices\n",
        "        x1b, y1b, x2b, y2b = boxes[i]\n",
        "        bh, bw = y2b - y1b, x2b - x1b\n",
        "        yc, xc = int(random.uniform(0, h - bh)), int(random.uniform(0, w - bw))  # offset x, y\n",
        "        x1a, y1a, x2a, y2a = [xc, yc, xc + bw, yc + bh]\n",
        "        img[y1a:y2a, x1a:x2a] = img[y1b:y2b, x1b:x2b]  # img4[ymin:ymax, xmin:xmax]\n",
        "        labels = np.append(labels, [[labels[i, 0], x1a, y1a, x2a, y2a]], axis=0)\n",
        "\n",
        "    return img, labels\n",
        "\n",
        "\n",
        "def letterbox(img, new_shape=(640, 640), color=(114, 114, 114), auto=True, scaleFill=False, scaleup=True, stride=32):\n",
        "    # Resize and pad image while meeting stride-multiple constraints\n",
        "    shape = img.shape[:2]  # current shape [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "\n",
        "    # Scale ratio (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    if not scaleup:  # only scale down, do not scale up (for better test mAP)\n",
        "        r = min(r, 1.0)\n",
        "\n",
        "    # Compute padding\n",
        "    ratio = r, r  # width, height ratios\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]  # wh padding\n",
        "    if auto:  # minimum rectangle\n",
        "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)  # wh padding\n",
        "    elif scaleFill:  # stretch\n",
        "        dw, dh = 0.0, 0.0\n",
        "        new_unpad = (new_shape[1], new_shape[0])\n",
        "        ratio = new_shape[1] / shape[1], new_shape[0] / shape[0]  # width, height ratios\n",
        "\n",
        "    dw /= 2  # divide padding into 2 sides\n",
        "    dh /= 2\n",
        "\n",
        "    if shape[::-1] != new_unpad:  # resize\n",
        "        img = cv2.resize(img, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    img = cv2.copyMakeBorder(img, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color)  # add border\n",
        "    return img, ratio, (dw, dh)\n",
        "\n",
        "\n",
        "def random_perspective(img, targets=(), segments=(), degrees=10, translate=.1, scale=.1, shear=10, perspective=0.0,\n",
        "                       border=(0, 0)):\n",
        "    # torchvision.transforms.RandomAffine(degrees=(-10, 10), translate=(.1, .1), scale=(.9, 1.1), shear=(-10, 10))\n",
        "    # targets = [cls, xyxy]\n",
        "\n",
        "    height = img.shape[0] + border[0] * 2  # shape(h,w,c)\n",
        "    width = img.shape[1] + border[1] * 2\n",
        "\n",
        "    # Center\n",
        "    C = np.eye(3)\n",
        "    C[0, 2] = -img.shape[1] / 2  # x translation (pixels)\n",
        "    C[1, 2] = -img.shape[0] / 2  # y translation (pixels)\n",
        "\n",
        "    # Perspective\n",
        "    P = np.eye(3)\n",
        "    P[2, 0] = random.uniform(-perspective, perspective)  # x perspective (about y)\n",
        "    P[2, 1] = random.uniform(-perspective, perspective)  # y perspective (about x)\n",
        "\n",
        "    # Rotation and Scale\n",
        "    R = np.eye(3)\n",
        "    a = random.uniform(-degrees, degrees)\n",
        "    # a += random.choice([-180, -90, 0, 90])  # add 90deg rotations to small rotations\n",
        "    s = random.uniform(1 - scale, 1 + scale)\n",
        "    # s = 2 ** random.uniform(-scale, scale)\n",
        "    R[:2] = cv2.getRotationMatrix2D(angle=a, center=(0, 0), scale=s)\n",
        "\n",
        "    # Shear\n",
        "    S = np.eye(3)\n",
        "    S[0, 1] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # x shear (deg)\n",
        "    S[1, 0] = math.tan(random.uniform(-shear, shear) * math.pi / 180)  # y shear (deg)\n",
        "\n",
        "    # Translation\n",
        "    T = np.eye(3)\n",
        "    T[0, 2] = random.uniform(0.5 - translate, 0.5 + translate) * width  # x translation (pixels)\n",
        "    T[1, 2] = random.uniform(0.5 - translate, 0.5 + translate) * height  # y translation (pixels)\n",
        "\n",
        "    # Combined rotation matrix\n",
        "    M = T @ S @ R @ P @ C  # order of operations (right to left) is IMPORTANT\n",
        "    if (border[0] != 0) or (border[1] != 0) or (M != np.eye(3)).any():  # image changed\n",
        "        if perspective:\n",
        "            img = cv2.warpPerspective(img, M, dsize=(width, height), borderValue=(114, 114, 114))\n",
        "        else:  # affine\n",
        "            img = cv2.warpAffine(img, M[:2], dsize=(width, height), borderValue=(114, 114, 114))\n",
        "\n",
        "    # Visualize\n",
        "    # import matplotlib.pyplot as plt\n",
        "    # ax = plt.subplots(1, 2, figsize=(12, 6))[1].ravel()\n",
        "    # ax[0].imshow(img[:, :, ::-1])  # base\n",
        "    # ax[1].imshow(img2[:, :, ::-1])  # warped\n",
        "\n",
        "    # Transform label coordinates\n",
        "    n = len(targets)\n",
        "    if n:\n",
        "        use_segments = any(x.any() for x in segments)\n",
        "        new = np.zeros((n, 4))\n",
        "        if use_segments:  # warp segments\n",
        "            segments = resample_segments(segments)  # upsample\n",
        "            for i, segment in enumerate(segments):\n",
        "                xy = np.ones((len(segment), 3))\n",
        "                xy[:, :2] = segment\n",
        "                xy = xy @ M.T  # transform\n",
        "                xy = xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]  # perspective rescale or affine\n",
        "\n",
        "                # clip\n",
        "                new[i] = segment2box(xy, width, height)\n",
        "\n",
        "        else:  # warp boxes\n",
        "            xy = np.ones((n * 4, 3))\n",
        "            xy[:, :2] = targets[:, [1, 2, 3, 4, 1, 4, 3, 2]].reshape(n * 4, 2)  # x1y1, x2y2, x1y2, x2y1\n",
        "            xy = xy @ M.T  # transform\n",
        "            xy = (xy[:, :2] / xy[:, 2:3] if perspective else xy[:, :2]).reshape(n, 8)  # perspective rescale or affine\n",
        "\n",
        "            # create new boxes\n",
        "            x = xy[:, [0, 2, 4, 6]]\n",
        "            y = xy[:, [1, 3, 5, 7]]\n",
        "            new = np.concatenate((x.min(1), y.min(1), x.max(1), y.max(1))).reshape(4, n).T\n",
        "\n",
        "            # clip\n",
        "            new[:, [0, 2]] = new[:, [0, 2]].clip(0, width)\n",
        "            new[:, [1, 3]] = new[:, [1, 3]].clip(0, height)\n",
        "\n",
        "        # filter candidates\n",
        "        i = box_candidates(box1=targets[:, 1:5].T * s, box2=new.T, area_thr=0.01 if use_segments else 0.10)\n",
        "        targets = targets[i]\n",
        "        targets[:, 1:5] = new[i]\n",
        "\n",
        "    return img, targets\n",
        "\n",
        "\n",
        "def box_candidates(box1, box2, wh_thr=2, ar_thr=20, area_thr=0.1, eps=1e-16):  # box1(4,n), box2(4,n)\n",
        "    # Compute candidate boxes: box1 before augment, box2 after augment, wh_thr (pixels), aspect_ratio_thr, area_ratio\n",
        "    w1, h1 = box1[2] - box1[0], box1[3] - box1[1]\n",
        "    w2, h2 = box2[2] - box2[0], box2[3] - box2[1]\n",
        "    ar = np.maximum(w2 / (h2 + eps), h2 / (w2 + eps))  # aspect ratio\n",
        "    return (w2 > wh_thr) & (h2 > wh_thr) & (w2 * h2 / (w1 * h1 + eps) > area_thr) & (ar < ar_thr)  # candidates\n",
        "\n",
        "\n",
        "def cutout(image, labels):\n",
        "    # Applies image cutout augmentation https://arxiv.org/abs/1708.04552\n",
        "    h, w = image.shape[:2]\n",
        "\n",
        "    def bbox_ioa(box1, box2):\n",
        "        # Returns the intersection over box2 area given box1, box2. box1 is 4, box2 is nx4. boxes are x1y1x2y2\n",
        "        box2 = box2.transpose()\n",
        "\n",
        "        # Get the coordinates of bounding boxes\n",
        "        b1_x1, b1_y1, b1_x2, b1_y2 = box1[0], box1[1], box1[2], box1[3]\n",
        "        b2_x1, b2_y1, b2_x2, b2_y2 = box2[0], box2[1], box2[2], box2[3]\n",
        "\n",
        "        # Intersection area\n",
        "        inter_area = (np.minimum(b1_x2, b2_x2) - np.maximum(b1_x1, b2_x1)).clip(0) * \\\n",
        "                     (np.minimum(b1_y2, b2_y2) - np.maximum(b1_y1, b2_y1)).clip(0)\n",
        "\n",
        "        # box2 area\n",
        "        box2_area = (b2_x2 - b2_x1) * (b2_y2 - b2_y1) + 1e-16\n",
        "\n",
        "        # Intersection over box2 area\n",
        "        return inter_area / box2_area\n",
        "\n",
        "    # create random masks\n",
        "    scales = [0.5] * 1 + [0.25] * 2 + [0.125] * 4 + [0.0625] * 8 + [0.03125] * 16  # image size fraction\n",
        "    for s in scales:\n",
        "        mask_h = random.randint(1, int(h * s))\n",
        "        mask_w = random.randint(1, int(w * s))\n",
        "\n",
        "        # box\n",
        "        xmin = max(0, random.randint(0, w) - mask_w // 2)\n",
        "        ymin = max(0, random.randint(0, h) - mask_h // 2)\n",
        "        xmax = min(w, xmin + mask_w)\n",
        "        ymax = min(h, ymin + mask_h)\n",
        "\n",
        "        # apply random color mask\n",
        "        image[ymin:ymax, xmin:xmax] = [random.randint(64, 191) for _ in range(3)]\n",
        "\n",
        "        # return unobscured labels\n",
        "        if len(labels) and s > 0.03:\n",
        "            box = np.array([xmin, ymin, xmax, ymax], dtype=np.float32)\n",
        "            ioa = bbox_ioa(box, labels[:, 1:5])  # intersection over area\n",
        "            labels = labels[ioa < 0.60]  # remove >60% obscured labels\n",
        "\n",
        "    return labels\n",
        "\n",
        "\n",
        "def create_folder(path='./new'):\n",
        "    # Create folder\n",
        "    if os.path.exists(path):\n",
        "        shutil.rmtree(path)  # delete output folder\n",
        "    os.makedirs(path)  # make new output folder\n",
        "\n",
        "\n",
        "def flatten_recursive(path='../coco128'):\n",
        "    # Flatten a recursive directory by bringing all files to top level\n",
        "    new_path = Path(path + '_flat')\n",
        "    create_folder(new_path)\n",
        "    for file in tqdm(glob.glob(str(Path(path)) + '/**/*.*', recursive=True)):\n",
        "        shutil.copyfile(file, new_path / Path(file).name)\n",
        "\n",
        "\n",
        "def extract_boxes(path='../coco128/'):  # from utils.datasets import *; extract_boxes('../coco128')\n",
        "    # Convert detection dataset into classification dataset, with one directory per class\n",
        "\n",
        "    path = Path(path)  # images dir\n",
        "    shutil.rmtree(path / 'classifier') if (path / 'classifier').is_dir() else None  # remove existing\n",
        "    files = list(path.rglob('*.*'))\n",
        "    n = len(files)  # number of files\n",
        "    for im_file in tqdm(files, total=n):\n",
        "        if im_file.suffix[1:] in img_formats:\n",
        "            # image\n",
        "            im = cv2.imread(str(im_file))[..., ::-1]  # BGR to RGB\n",
        "            h, w = im.shape[:2]\n",
        "\n",
        "            # labels\n",
        "            lb_file = Path(img2label_paths([str(im_file)])[0])\n",
        "            if Path(lb_file).exists():\n",
        "                with open(lb_file, 'r') as f:\n",
        "                    lb = np.array([x.split() for x in f.read().strip().splitlines()], dtype=np.float32)  # labels\n",
        "\n",
        "                for j, x in enumerate(lb):\n",
        "                    c = int(x[0])  # class\n",
        "                    f = (path / 'classifier') / f'{c}' / f'{path.stem}_{im_file.stem}_{j}.jpg'  # new filename\n",
        "                    if not f.parent.is_dir():\n",
        "                        f.parent.mkdir(parents=True)\n",
        "\n",
        "                    b = x[1:] * [w, h, w, h]  # box\n",
        "                    # b[2:] = b[2:].max()  # rectangle to square\n",
        "                    b[2:] = b[2:] * 1.2 + 3  # pad\n",
        "                    b = xywh2xyxy(b.reshape(-1, 4)).ravel().astype(np.int)\n",
        "\n",
        "                    b[[0, 2]] = np.clip(b[[0, 2]], 0, w)  # clip boxes outside of image\n",
        "                    b[[1, 3]] = np.clip(b[[1, 3]], 0, h)\n",
        "                    assert cv2.imwrite(str(f), im[b[1]:b[3], b[0]:b[2]]), f'box failure in {f}'\n",
        "\n",
        "\n",
        "def autosplit(path='../coco128', weights=(0.9, 0.1, 0.0), annotated_only=False):\n",
        "    \"\"\" Autosplit a dataset into train/val/test splits and save path/autosplit_*.txt files\n",
        "    Usage: from utils.datasets import *; autosplit('../coco128')\n",
        "    Arguments\n",
        "        path:           Path to images directory\n",
        "        weights:        Train, val, test weights (list)\n",
        "        annotated_only: Only use images with an annotated txt file\n",
        "    \"\"\"\n",
        "    path = Path(path)  # images dir\n",
        "    files = sum([list(path.rglob(f\"*.{img_ext}\")) for img_ext in img_formats], [])  # image files only\n",
        "    n = len(files)  # number of files\n",
        "    indices = random.choices([0, 1, 2], weights=weights, k=n)  # assign each image to a split\n",
        "\n",
        "    txt = ['autosplit_train.txt', 'autosplit_val.txt', 'autosplit_test.txt']  # 3 txt files\n",
        "    [(path / x).unlink() for x in txt if (path / x).exists()]  # remove existing\n",
        "\n",
        "    print(f'Autosplitting images from {path}' + ', using *.txt labeled images only' * annotated_only)\n",
        "    for i, img in tqdm(zip(indices, files), total=n):\n",
        "        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # check label\n",
        "            with open(path / txt[i], 'a') as f:\n",
        "                f.write(str(img) + '\\n')  # add image to txt file\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dwk9cs_dEBfp"
      },
      "source": [
        "# Common"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "krh_I4XBEHCp"
      },
      "outputs": [],
      "source": [
        "# YOLOv5 common modules\n",
        "\n",
        "import math\n",
        "from copy import copy\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from PIL import Image\n",
        "from torch.cuda import amp\n",
        "\n",
        "# from utils.datasets import letterbox\n",
        "# from utils.general import non_max_suppression, make_divisible, scale_coords, increment_path, xyxy2xywh, save_one_box\n",
        "# from utils.plots import colors, plot_one_box\n",
        "# from utils.torch_utils import time_synchronized\n",
        "\n",
        "\n",
        "def autopad(k, p=None):  # kernel, padding\n",
        "    # Pad to 'same'\n",
        "    if p is None:\n",
        "        p = k // 2 if isinstance(k, int) else [x // 2 for x in k]  # auto-pad\n",
        "    return p\n",
        "\n",
        "\n",
        "def DWConv(c1, c2, k=1, s=1, act=True):\n",
        "    # Depthwise convolution\n",
        "    return Conv(c1, c2, k, s, g=math.gcd(c1, c2), act=act)\n",
        "\n",
        "\n",
        "class Conv(nn.Module):\n",
        "    # Standard convolution\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n",
        "        super(Conv, self).__init__()\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g, bias=False)\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = nn.SiLU() if act is True else (act if isinstance(act, nn.Module) else nn.Identity())\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.act(self.bn(self.conv(x)))\n",
        "\n",
        "    def fuseforward(self, x):\n",
        "        return self.act(self.conv(x))\n",
        "\n",
        "\n",
        "class TransformerLayer(nn.Module):\n",
        "    # Transformer layer https://arxiv.org/abs/2010.11929 (LayerNorm layers removed for better performance)\n",
        "    def __init__(self, c, num_heads):\n",
        "        super().__init__()\n",
        "        self.q = nn.Linear(c, c, bias=False)\n",
        "        self.k = nn.Linear(c, c, bias=False)\n",
        "        self.v = nn.Linear(c, c, bias=False)\n",
        "        self.ma = nn.MultiheadAttention(embed_dim=c, num_heads=num_heads)\n",
        "        self.fc1 = nn.Linear(c, c, bias=False)\n",
        "        self.fc2 = nn.Linear(c, c, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.ma(self.q(x), self.k(x), self.v(x))[0] + x\n",
        "        x = self.fc2(self.fc1(x)) + x\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    # Vision Transformer https://arxiv.org/abs/2010.11929\n",
        "    def __init__(self, c1, c2, num_heads, num_layers):\n",
        "        super().__init__()\n",
        "        self.conv = None\n",
        "        if c1 != c2:\n",
        "            self.conv = Conv(c1, c2)\n",
        "        self.linear = nn.Linear(c2, c2)  # learnable position embedding\n",
        "        self.tr = nn.Sequential(*[TransformerLayer(c2, num_heads) for _ in range(num_layers)])\n",
        "        self.c2 = c2\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.conv is not None:\n",
        "            x = self.conv(x)\n",
        "        b, _, w, h = x.shape\n",
        "        p = x.flatten(2)\n",
        "        p = p.unsqueeze(0)\n",
        "        p = p.transpose(0, 3)\n",
        "        p = p.squeeze(3)\n",
        "        e = self.linear(p)\n",
        "        x = p + e\n",
        "\n",
        "        x = self.tr(x)\n",
        "        x = x.unsqueeze(3)\n",
        "        x = x.transpose(0, 3)\n",
        "        x = x.reshape(b, self.c2, w, h)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Bottleneck(nn.Module):\n",
        "    # Standard bottleneck\n",
        "    def __init__(self, c1, c2, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, shortcut, groups, expansion\n",
        "        super(Bottleneck, self).__init__()\n",
        "        c_ = int(c2 * e)  # hidden channels\n",
        "        self.cv1 = Conv(c1, c_, 1, 1)\n",
        "        self.cv2 = Conv(c_, c2, 3, 1, g=g)\n",
        "        self.add = shortcut and c1 == c2\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
        "\n",
        "\n",
        "class BottleneckCSP(nn.Module):\n",
        "    # CSP Bottleneck https://github.com/WongKinYiu/CrossStagePartialNetworks\n",
        "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n",
        "        super(BottleneckCSP, self).__init__()\n",
        "        c_ = int(c2 * e)  # hidden channels\n",
        "        self.cv1 = Conv(c1, c_, 1, 1)\n",
        "        self.cv2 = nn.Conv2d(c1, c_, 1, 1, bias=False)\n",
        "        self.cv3 = nn.Conv2d(c_, c_, 1, 1, bias=False)\n",
        "        self.cv4 = Conv(2 * c_, c2, 1, 1)\n",
        "        self.bn = nn.BatchNorm2d(2 * c_)  # applied to cat(cv2, cv3)\n",
        "        self.act = nn.LeakyReLU(0.1, inplace=True)\n",
        "        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        y1 = self.cv3(self.m(self.cv1(x)))\n",
        "        y2 = self.cv2(x)\n",
        "        return self.cv4(self.act(self.bn(torch.cat((y1, y2), dim=1))))\n",
        "\n",
        "\n",
        "class C3(nn.Module):\n",
        "    # CSP Bottleneck with 3 convolutions\n",
        "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):  # ch_in, ch_out, number, shortcut, groups, expansion\n",
        "        super(C3, self).__init__()\n",
        "        c_ = int(c2 * e)  # hidden channels\n",
        "        self.cv1 = Conv(c1, c_, 1, 1)\n",
        "        self.cv2 = Conv(c1, c_, 1, 1)\n",
        "        self.cv3 = Conv(2 * c_, c2, 1)  # act=FReLU(c2)\n",
        "        self.m = nn.Sequential(*[Bottleneck(c_, c_, shortcut, g, e=1.0) for _ in range(n)])\n",
        "        # self.m = nn.Sequential(*[CrossConv(c_, c_, 3, 1, g, 1.0, shortcut) for _ in range(n)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.cv3(torch.cat((self.m(self.cv1(x)), self.cv2(x)), dim=1))\n",
        "\n",
        "\n",
        "class C3TR(C3):\n",
        "    # C3 module with TransformerBlock()\n",
        "    def __init__(self, c1, c2, n=1, shortcut=True, g=1, e=0.5):\n",
        "        super().__init__(c1, c2, n, shortcut, g, e)\n",
        "        c_ = int(c2 * e)\n",
        "        self.m = TransformerBlock(c_, c_, 4, n)\n",
        "\n",
        "\n",
        "class SPP(nn.Module):\n",
        "    # Spatial pyramid pooling layer used in YOLOv3-SPP\n",
        "    def __init__(self, c1, c2, k=(5, 9, 13)):\n",
        "        super(SPP, self).__init__()\n",
        "        c_ = c1 // 2  # hidden channels\n",
        "        self.cv1 = Conv(c1, c_, 1, 1)\n",
        "        self.cv2 = Conv(c_ * (len(k) + 1), c2, 1, 1)\n",
        "        self.m = nn.ModuleList([nn.MaxPool2d(kernel_size=x, stride=1, padding=x // 2) for x in k])\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.cv1(x)\n",
        "        return self.cv2(torch.cat([x] + [m(x) for m in self.m], 1))\n",
        "\n",
        "\n",
        "class Focus(nn.Module):\n",
        "    # Focus wh information into c-space\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1, act=True):  # ch_in, ch_out, kernel, stride, padding, groups\n",
        "        super(Focus, self).__init__()\n",
        "        self.conv = Conv(c1 * 4, c2, k, s, p, g, act)\n",
        "        # self.contract = Contract(gain=2)\n",
        "\n",
        "    def forward(self, x):  # x(b,c,w,h) -> y(b,4c,w/2,h/2)\n",
        "        return self.conv(torch.cat([x[..., ::2, ::2], x[..., 1::2, ::2], x[..., ::2, 1::2], x[..., 1::2, 1::2]], 1))\n",
        "        # return self.conv(self.contract(x))\n",
        "\n",
        "\n",
        "class Contract(nn.Module):\n",
        "    # Contract width-height into channels, i.e. x(1,64,80,80) to x(1,256,40,40)\n",
        "    def __init__(self, gain=2):\n",
        "        super().__init__()\n",
        "        self.gain = gain\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()  # assert (H / s == 0) and (W / s == 0), 'Indivisible gain'\n",
        "        s = self.gain\n",
        "        x = x.view(N, C, H // s, s, W // s, s)  # x(1,64,40,2,40,2)\n",
        "        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # x(1,2,2,64,40,40)\n",
        "        return x.view(N, C * s * s, H // s, W // s)  # x(1,256,40,40)\n",
        "\n",
        "\n",
        "class Expand(nn.Module):\n",
        "    # Expand channels into width-height, i.e. x(1,64,80,80) to x(1,16,160,160)\n",
        "    def __init__(self, gain=2):\n",
        "        super().__init__()\n",
        "        self.gain = gain\n",
        "\n",
        "    def forward(self, x):\n",
        "        N, C, H, W = x.size()  # assert C / s ** 2 == 0, 'Indivisible gain'\n",
        "        s = self.gain\n",
        "        x = x.view(N, s, s, C // s ** 2, H, W)  # x(1,2,2,16,80,80)\n",
        "        x = x.permute(0, 3, 4, 1, 5, 2).contiguous()  # x(1,16,80,2,80,2)\n",
        "        return x.view(N, C // s ** 2, H * s, W * s)  # x(1,16,160,160)\n",
        "\n",
        "\n",
        "class Concat(nn.Module):\n",
        "    # Concatenate a list of tensors along dimension\n",
        "    def __init__(self, dimension=1):\n",
        "        super(Concat, self).__init__()\n",
        "        self.d = dimension\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.cat(x, self.d)\n",
        "\n",
        "\n",
        "class NMS(nn.Module):\n",
        "    # Non-Maximum Suppression (NMS) module\n",
        "    conf = 0.25  # confidence threshold\n",
        "    iou = 0.45  # IoU threshold\n",
        "    classes = None  # (optional list) filter by class\n",
        "\n",
        "    def __init__(self):\n",
        "        super(NMS, self).__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return non_max_suppression(x[0], conf_thres=self.conf, iou_thres=self.iou, classes=self.classes)\n",
        "\n",
        "\n",
        "class AutoShape(nn.Module):\n",
        "    # input-robust model wrapper for passing cv2/np/PIL/torch inputs. Includes preprocessing, inference and NMS\n",
        "    conf = 0.25  # NMS confidence threshold\n",
        "    iou = 0.45  # NMS IoU threshold\n",
        "    classes = None  # (optional list) filter by class\n",
        "\n",
        "    def __init__(self, model):\n",
        "        super(AutoShape, self).__init__()\n",
        "        self.model = model.eval()\n",
        "\n",
        "    def autoshape(self):\n",
        "        print('AutoShape already enabled, skipping... ')  # model already converted to model.autoshape()\n",
        "        return self\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, imgs, size=640, augment=False, profile=False):\n",
        "        # Inference from various sources. For height=640, width=1280, RGB images example inputs are:\n",
        "        #   filename:   imgs = 'data/images/zidane.jpg'\n",
        "        #   URI:             = 'https://github.com/ultralytics/yolov5/releases/download/v1.0/zidane.jpg'\n",
        "        #   OpenCV:          = cv2.imread('image.jpg')[:,:,::-1]  # HWC BGR to RGB x(640,1280,3)\n",
        "        #   PIL:             = Image.open('image.jpg')  # HWC x(640,1280,3)\n",
        "        #   numpy:           = np.zeros((640,1280,3))  # HWC\n",
        "        #   torch:           = torch.zeros(16,3,320,640)  # BCHW (scaled to size=640, 0-1 values)\n",
        "        #   multiple:        = [Image.open('image1.jpg'), Image.open('image2.jpg'), ...]  # list of images\n",
        "\n",
        "        t = [time_synchronized()]\n",
        "        p = next(self.model.parameters())  # for device and type\n",
        "        if isinstance(imgs, torch.Tensor):  # torch\n",
        "            with amp.autocast(enabled=p.device.type != 'cpu'):\n",
        "                return self.model(imgs.to(p.device).type_as(p), augment, profile)  # inference\n",
        "\n",
        "        # Pre-process\n",
        "        n, imgs = (len(imgs), imgs) if isinstance(imgs, list) else (1, [imgs])  # number of images, list of images\n",
        "        shape0, shape1, files = [], [], []  # image and inference shapes, filenames\n",
        "        for i, im in enumerate(imgs):\n",
        "            f = f'image{i}'  # filename\n",
        "            if isinstance(im, str):  # filename or uri\n",
        "                im, f = np.asarray(Image.open(requests.get(im, stream=True).raw if im.startswith('http') else im)), im\n",
        "            elif isinstance(im, Image.Image):  # PIL Image\n",
        "                im, f = np.asarray(im), getattr(im, 'filename', f) or f\n",
        "            files.append(Path(f).with_suffix('.jpg').name)\n",
        "            if im.shape[0] < 5:  # image in CHW\n",
        "                im = im.transpose((1, 2, 0))  # reverse dataloader .transpose(2, 0, 1)\n",
        "            im = im[:, :, :3] if im.ndim == 3 else np.tile(im[:, :, None], 3)  # enforce 3ch input\n",
        "            s = im.shape[:2]  # HWC\n",
        "            shape0.append(s)  # image shape\n",
        "            g = (size / max(s))  # gain\n",
        "            shape1.append([y * g for y in s])\n",
        "            imgs[i] = im if im.data.contiguous else np.ascontiguousarray(im)  # update\n",
        "        shape1 = [make_divisible(x, int(self.stride.max())) for x in np.stack(shape1, 0).max(0)]  # inference shape\n",
        "        x = [letterbox(im, new_shape=shape1, auto=False)[0] for im in imgs]  # pad\n",
        "        x = np.stack(x, 0) if n > 1 else x[0][None]  # stack\n",
        "        x = np.ascontiguousarray(x.transpose((0, 3, 1, 2)))  # BHWC to BCHW\n",
        "        x = torch.from_numpy(x).to(p.device).type_as(p) / 255.  # uint8 to fp16/32\n",
        "        t.append(time_synchronized())\n",
        "\n",
        "        with amp.autocast(enabled=p.device.type != 'cpu'):\n",
        "            # Inference\n",
        "            y = self.model(x, augment, profile)[0]  # forward\n",
        "            t.append(time_synchronized())\n",
        "\n",
        "            # Post-process\n",
        "            y = non_max_suppression(y, conf_thres=self.conf, iou_thres=self.iou, classes=self.classes)  # NMS\n",
        "            for i in range(n):\n",
        "                scale_coords(shape1, y[i][:, :4], shape0[i])\n",
        "\n",
        "            t.append(time_synchronized())\n",
        "            return Detections(imgs, y, files, t, self.names, x.shape)\n",
        "\n",
        "\n",
        "class Detections:\n",
        "    # detections class for YOLOv5 inference results\n",
        "    def __init__(self, imgs, pred, files, times=None, names=None, shape=None):\n",
        "        super(Detections, self).__init__()\n",
        "        d = pred[0].device  # device\n",
        "        gn = [torch.tensor([*[im.shape[i] for i in [1, 0, 1, 0]], 1., 1.], device=d) for im in imgs]  # normalizations\n",
        "        self.imgs = imgs  # list of images as numpy arrays\n",
        "        self.pred = pred  # list of tensors pred[0] = (xyxy, conf, cls)\n",
        "        self.names = names  # class names\n",
        "        self.files = files  # image filenames\n",
        "        self.xyxy = pred  # xyxy pixels\n",
        "        self.xywh = [xyxy2xywh(x) for x in pred]  # xywh pixels\n",
        "        self.xyxyn = [x / g for x, g in zip(self.xyxy, gn)]  # xyxy normalized\n",
        "        self.xywhn = [x / g for x, g in zip(self.xywh, gn)]  # xywh normalized\n",
        "        self.n = len(self.pred)  # number of images (batch size)\n",
        "        self.t = tuple((times[i + 1] - times[i]) * 1000 / self.n for i in range(3))  # timestamps (ms)\n",
        "        self.s = shape  # inference BCHW shape\n",
        "\n",
        "    def display(self, pprint=False, show=False, save=False, crop=False, render=False, save_dir=Path('')):\n",
        "        for i, (im, pred) in enumerate(zip(self.imgs, self.pred)):\n",
        "            str = f'image {i + 1}/{len(self.pred)}: {im.shape[0]}x{im.shape[1]} '\n",
        "            if pred is not None:\n",
        "                for c in pred[:, -1].unique():\n",
        "                    n = (pred[:, -1] == c).sum()  # detections per class\n",
        "                    str += f\"{n} {self.names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
        "                if show or save or render or crop:\n",
        "                    for *box, conf, cls in pred:  # xyxy, confidence, class\n",
        "                        label = f'{self.names[int(cls)]} {conf:.2f}'\n",
        "                        if crop:\n",
        "                            save_one_box(box, im, file=save_dir / 'crops' / self.names[int(cls)] / self.files[i])\n",
        "                        else:  # all others\n",
        "                            plot_one_box(box, im, label=label, color=colors(cls))\n",
        "\n",
        "            im = Image.fromarray(im.astype(np.uint8)) if isinstance(im, np.ndarray) else im  # from np\n",
        "            if pprint:\n",
        "                print(str.rstrip(', '))\n",
        "            if show:\n",
        "                im.show(self.files[i])  # show\n",
        "            if save:\n",
        "                f = self.files[i]\n",
        "                im.save(save_dir / f)  # save\n",
        "                print(f\"{'Saved' * (i == 0)} {f}\", end=',' if i < self.n - 1 else f' to {save_dir}\\n')\n",
        "            if render:\n",
        "                self.imgs[i] = np.asarray(im)\n",
        "\n",
        "    def print(self):\n",
        "        self.display(pprint=True)  # print results\n",
        "        print(f'Speed: %.1fms pre-process, %.1fms inference, %.1fms NMS per image at shape {tuple(self.s)}' % self.t)\n",
        "\n",
        "    def show(self):\n",
        "        self.display(show=True)  # show results\n",
        "\n",
        "    def save(self, save_dir='runs/hub/exp'):\n",
        "        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/hub/exp', mkdir=True)  # increment save_dir\n",
        "        self.display(save=True, save_dir=save_dir)  # save results\n",
        "\n",
        "    def crop(self, save_dir='runs/hub/exp'):\n",
        "        save_dir = increment_path(save_dir, exist_ok=save_dir != 'runs/hub/exp', mkdir=True)  # increment save_dir\n",
        "        self.display(crop=True, save_dir=save_dir)  # crop results\n",
        "        print(f'Saved results to {save_dir}\\n')\n",
        "\n",
        "    def render(self):\n",
        "        self.display(render=True)  # render results\n",
        "        return self.imgs\n",
        "\n",
        "    def pandas(self):\n",
        "        # return detections as pandas DataFrames, i.e. print(results.pandas().xyxy[0])\n",
        "        new = copy(self)  # return copy\n",
        "        ca = 'xmin', 'ymin', 'xmax', 'ymax', 'confidence', 'class', 'name'  # xyxy columns\n",
        "        cb = 'xcenter', 'ycenter', 'width', 'height', 'confidence', 'class', 'name'  # xywh columns\n",
        "        for k, c in zip(['xyxy', 'xyxyn', 'xywh', 'xywhn'], [ca, ca, cb, cb]):\n",
        "            a = [[x[:5] + [int(x[5]), self.names[int(x[5])]] for x in x.tolist()] for x in getattr(self, k)]  # update\n",
        "            setattr(new, k, [pd.DataFrame(x, columns=c) for x in a])\n",
        "        return new\n",
        "\n",
        "    def tolist(self):\n",
        "        # return a list of Detections objects, i.e. 'for result in results.tolist():'\n",
        "        x = [Detections([self.imgs[i]], [self.pred[i]], self.names, self.s) for i in range(self.n)]\n",
        "        for d in x:\n",
        "            for k in ['imgs', 'pred', 'xyxy', 'xyxyn', 'xywh', 'xywhn']:\n",
        "                setattr(d, k, getattr(d, k)[0])  # pop out of list\n",
        "        return x\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n\n",
        "\n",
        "\n",
        "class Classify(nn.Module):\n",
        "    # Classification head, i.e. x(b,c1,20,20) to x(b,c2)\n",
        "    def __init__(self, c1, c2, k=1, s=1, p=None, g=1):  # ch_in, ch_out, kernel, stride, padding, groups\n",
        "        super(Classify, self).__init__()\n",
        "        self.aap = nn.AdaptiveAvgPool2d(1)  # to x(b,c1,1,1)\n",
        "        self.conv = nn.Conv2d(c1, c2, k, s, autopad(k, p), groups=g)  # to x(b,c2,1,1)\n",
        "        self.flat = nn.Flatten()\n",
        "\n",
        "    def forward(self, x):\n",
        "        z = torch.cat([self.aap(y) for y in (x if isinstance(x, list) else [x])], 1)  # cat if list\n",
        "        return self.flat(self.conv(z))  # flatten to x(b,c2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DQG-9jwVEJXa"
      },
      "source": [
        "# Experimental"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "y1IQJD2vELs5"
      },
      "outputs": [],
      "source": [
        "# YOLOv5 experimental modules\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# from models.common import Conv, DWConv\n",
        "# from utils.google_utils import attempt_download\n",
        "\n",
        "\n",
        "class CrossConv(nn.Module):\n",
        "    # Cross Convolution Downsample\n",
        "    def __init__(self, c1, c2, k=3, s=1, g=1, e=1.0, shortcut=False):\n",
        "        # ch_in, ch_out, kernel, stride, groups, expansion, shortcut\n",
        "        super(CrossConv, self).__init__()\n",
        "        c_ = int(c2 * e)  # hidden channels\n",
        "        self.cv1 = Conv(c1, c_, (1, k), (1, s))\n",
        "        self.cv2 = Conv(c_, c2, (k, 1), (s, 1), g=g)\n",
        "        self.add = shortcut and c1 == c2\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.cv2(self.cv1(x)) if self.add else self.cv2(self.cv1(x))\n",
        "\n",
        "\n",
        "class Sum(nn.Module):\n",
        "    # Weighted sum of 2 or more layers https://arxiv.org/abs/1911.09070\n",
        "    def __init__(self, n, weight=False):  # n: number of inputs\n",
        "        super(Sum, self).__init__()\n",
        "        self.weight = weight  # apply weights boolean\n",
        "        self.iter = range(n - 1)  # iter object\n",
        "        if weight:\n",
        "            self.w = nn.Parameter(-torch.arange(1., n) / 2, requires_grad=True)  # layer weights\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = x[0]  # no weight\n",
        "        if self.weight:\n",
        "            w = torch.sigmoid(self.w) * 2\n",
        "            for i in self.iter:\n",
        "                y = y + x[i + 1] * w[i]\n",
        "        else:\n",
        "            for i in self.iter:\n",
        "                y = y + x[i + 1]\n",
        "        return y\n",
        "\n",
        "\n",
        "class GhostConv(nn.Module):\n",
        "    # Ghost Convolution https://github.com/huawei-noah/ghostnet\n",
        "    def __init__(self, c1, c2, k=1, s=1, g=1, act=True):  # ch_in, ch_out, kernel, stride, groups\n",
        "        super(GhostConv, self).__init__()\n",
        "        c_ = c2 // 2  # hidden channels\n",
        "        self.cv1 = Conv(c1, c_, k, s, None, g, act)\n",
        "        self.cv2 = Conv(c_, c_, 5, 1, None, c_, act)\n",
        "\n",
        "    def forward(self, x):\n",
        "        y = self.cv1(x)\n",
        "        return torch.cat([y, self.cv2(y)], 1)\n",
        "\n",
        "\n",
        "class GhostBottleneck(nn.Module):\n",
        "    # Ghost Bottleneck https://github.com/huawei-noah/ghostnet\n",
        "    def __init__(self, c1, c2, k=3, s=1):  # ch_in, ch_out, kernel, stride\n",
        "        super(GhostBottleneck, self).__init__()\n",
        "        c_ = c2 // 2\n",
        "        self.conv = nn.Sequential(GhostConv(c1, c_, 1, 1),  # pw\n",
        "                                  DWConv(c_, c_, k, s, act=False) if s == 2 else nn.Identity(),  # dw\n",
        "                                  GhostConv(c_, c2, 1, 1, act=False))  # pw-linear\n",
        "        self.shortcut = nn.Sequential(DWConv(c1, c1, k, s, act=False),\n",
        "                                      Conv(c1, c2, 1, 1, act=False)) if s == 2 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.conv(x) + self.shortcut(x)\n",
        "\n",
        "\n",
        "class MixConv2d(nn.Module):\n",
        "    # Mixed Depthwise Conv https://arxiv.org/abs/1907.09595\n",
        "    def __init__(self, c1, c2, k=(1, 3), s=1, equal_ch=True):\n",
        "        super(MixConv2d, self).__init__()\n",
        "        groups = len(k)\n",
        "        if equal_ch:  # equal c_ per group\n",
        "            i = torch.linspace(0, groups - 1E-6, c2).floor()  # c2 indices\n",
        "            c_ = [(i == g).sum() for g in range(groups)]  # intermediate channels\n",
        "        else:  # equal weight.numel() per group\n",
        "            b = [c2] + [0] * groups\n",
        "            a = np.eye(groups + 1, groups, k=-1)\n",
        "            a -= np.roll(a, 1, axis=1)\n",
        "            a *= np.array(k) ** 2\n",
        "            a[0] = 1\n",
        "            c_ = np.linalg.lstsq(a, b, rcond=None)[0].round()  # solve for equal weight indices, ax = b\n",
        "\n",
        "        self.m = nn.ModuleList([nn.Conv2d(c1, int(c_[g]), k[g], s, k[g] // 2, bias=False) for g in range(groups)])\n",
        "        self.bn = nn.BatchNorm2d(c2)\n",
        "        self.act = nn.LeakyReLU(0.1, inplace=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.act(self.bn(torch.cat([m(x) for m in self.m], 1)))\n",
        "\n",
        "\n",
        "class Ensemble(nn.ModuleList):\n",
        "    # Ensemble of models\n",
        "    def __init__(self):\n",
        "        super(Ensemble, self).__init__()\n",
        "\n",
        "    def forward(self, x, augment=False):\n",
        "        y = []\n",
        "        for module in self:\n",
        "            y.append(module(x, augment)[0])\n",
        "        # y = torch.stack(y).max(0)[0]  # max ensemble\n",
        "        # y = torch.stack(y).mean(0)  # mean ensemble\n",
        "        y = torch.cat(y, 1)  # nms ensemble\n",
        "        return y, None  # inference, train output\n",
        "\n",
        "\n",
        "def attempt_load(weights, map_location=None, inplace=True):\n",
        "    # Loads an ensemble of models weights=[a,b,c] or a single model weights=[a] or weights=a\n",
        "    model = Ensemble()\n",
        "    for w in weights if isinstance(weights, list) else [weights]:\n",
        "        attempt_download(w)\n",
        "        print(\"downloaded\")\n",
        "        print(w)\n",
        "        ckpt = torch.load(w, map_location=map_location)  # load\n",
        "        print(\"loaded\")\n",
        "        model.append(ckpt['ema' if ckpt.get('ema') else 'model'].float().fuse().eval())  # FP32 model\n",
        "\n",
        "    # Compatibility updates\n",
        "    for m in model.modules():\n",
        "        if type(m) in [nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU, Detect, Model]:\n",
        "            m.inplace = inplace  # pytorch 1.7.0 compatibility\n",
        "        elif type(m) is Conv:\n",
        "            m._non_persistent_buffers_set = set()  # pytorch 1.6.0 compatibility\n",
        "\n",
        "    if len(model) == 1:\n",
        "        return model[-1]  # return model\n",
        "    else:\n",
        "        print(f'Ensemble created with {weights}\\n')\n",
        "        for k in ['names']:\n",
        "            setattr(model, k, getattr(model[-1], k))\n",
        "        model.stride = model[torch.argmax(torch.tensor([m.stride.max() for m in model])).int()].stride  # max stride\n",
        "        return model  # return ensemble\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nuJguHgavW04"
      },
      "source": [
        "# YOLO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "XgNtx1x9vU4N"
      },
      "outputs": [],
      "source": [
        "# YOLOv5 YOLO-specific modules\n",
        "\n",
        "import argparse\n",
        "import logging\n",
        "import sys\n",
        "from copy import deepcopy\n",
        "from pathlib import Path\n",
        "\n",
        "# sys.path.append(Path(__file__).parent.parent.absolute().__str__())  # to run '$ python *.py' files in subdirectories\n",
        "# logger = logging.getLogger(__name__)\n",
        "\n",
        "# from models.common import *\n",
        "# from models.experimental import *\n",
        "# from utils.autoanchor import check_anchor_order\n",
        "# from utils.general import make_divisible, check_file, set_logging\n",
        "# from utils.torch_utils import time_synchronized, fuse_conv_and_bn, model_info, scale_img, initialize_weights, \\\n",
        "#     select_device, copy_attr\n",
        "\n",
        "try:\n",
        "    import thop  # for FLOPS computation\n",
        "except ImportError:\n",
        "    thop = None\n",
        "\n",
        "\n",
        "class Detect(nn.Module):\n",
        "    stride = None  # strides computed during build\n",
        "    onnx_dynamic = False  # ONNX export parameter\n",
        "\n",
        "    def __init__(self, nc=80, anchors=(), ch=(), inplace=True):  # detection layer\n",
        "        super(Detect, self).__init__()\n",
        "        self.nc = nc  # number of classes\n",
        "        self.no = nc + 5  # number of outputs per anchor\n",
        "        self.nl = len(anchors)  # number of detection layers\n",
        "        self.na = len(anchors[0]) // 2  # number of anchors\n",
        "        self.grid = [torch.zeros(1)] * self.nl  # init grid\n",
        "        a = torch.tensor(anchors).float().view(self.nl, -1, 2)\n",
        "        self.register_buffer('anchors', a)  # shape(nl,na,2)\n",
        "        self.register_buffer('anchor_grid', a.clone().view(self.nl, 1, -1, 1, 1, 2))  # shape(nl,1,na,1,1,2)\n",
        "        self.m = nn.ModuleList(nn.Conv2d(x, self.no * self.na, 1) for x in ch)  # output conv\n",
        "        self.inplace = inplace  # use in-place ops (e.g. slice assignment)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x = x.copy()  # for profiling\n",
        "        z = []  # inference output\n",
        "        for i in range(self.nl):\n",
        "            x[i] = self.m[i](x[i])  # conv\n",
        "            bs, _, ny, nx = x[i].shape  # x(bs,255,20,20) to x(bs,3,20,20,85)\n",
        "            x[i] = x[i].view(bs, self.na, self.no, ny, nx).permute(0, 1, 3, 4, 2).contiguous()\n",
        "\n",
        "            if not self.training:  # inference\n",
        "                if self.grid[i].shape[2:4] != x[i].shape[2:4] or self.onnx_dynamic:\n",
        "                    self.grid[i] = self._make_grid(nx, ny).to(x[i].device)\n",
        "\n",
        "                y = x[i].sigmoid()\n",
        "                if self.inplace:\n",
        "                    y[..., 0:2] = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\n",
        "                    y[..., 2:4] = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i]  # wh\n",
        "                else:  # for YOLOv5 on AWS Inferentia https://github.com/ultralytics/yolov5/pull/2953\n",
        "                    xy = (y[..., 0:2] * 2. - 0.5 + self.grid[i]) * self.stride[i]  # xy\n",
        "                    wh = (y[..., 2:4] * 2) ** 2 * self.anchor_grid[i].view(1, self.na, 1, 1, 2)  # wh\n",
        "                    y = torch.cat((xy, wh, y[..., 4:]), -1)\n",
        "                z.append(y.view(bs, -1, self.no))\n",
        "\n",
        "        return x if self.training else (torch.cat(z, 1), x)\n",
        "\n",
        "    @staticmethod\n",
        "    def _make_grid(nx=20, ny=20):\n",
        "        yv, xv = torch.meshgrid([torch.arange(ny), torch.arange(nx)])\n",
        "        return torch.stack((xv, yv), 2).view((1, 1, ny, nx, 2)).float()\n",
        "\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, cfg='yolov5s.yaml', ch=3, nc=None, anchors=None):  # model, input channels, number of classes\n",
        "        super(Model, self).__init__()\n",
        "        if isinstance(cfg, dict):\n",
        "            self.yaml = cfg  # model dict\n",
        "        else:  # is *.yaml\n",
        "            import yaml  # for torch hub\n",
        "            self.yaml_file = Path(cfg).name\n",
        "            with open(cfg) as f:\n",
        "                self.yaml = yaml.safe_load(f)  # model dict\n",
        "\n",
        "        # Define model\n",
        "        ch = self.yaml['ch'] = self.yaml.get('ch', ch)  # input channels\n",
        "        if nc and nc != self.yaml['nc']:\n",
        "            logger.info(f\"Overriding model.yaml nc={self.yaml['nc']} with nc={nc}\")\n",
        "            self.yaml['nc'] = nc  # override yaml value\n",
        "        if anchors:\n",
        "            logger.info(f'Overriding model.yaml anchors with anchors={anchors}')\n",
        "            self.yaml['anchors'] = round(anchors)  # override yaml value\n",
        "        self.model, self.save = parse_model(deepcopy(self.yaml), ch=[ch])  # model, savelist\n",
        "        self.names = [str(i) for i in range(self.yaml['nc'])]  # default names\n",
        "        self.inplace = self.yaml.get('inplace', True)\n",
        "        # logger.info([x.shape for x in self.forward(torch.zeros(1, ch, 64, 64))])\n",
        "\n",
        "        # Build strides, anchors\n",
        "        m = self.model[-1]  # Detect()\n",
        "        if isinstance(m, Detect):\n",
        "            s = 256  # 2x min stride\n",
        "            m.inplace = self.inplace\n",
        "            m.stride = torch.tensor([s / x.shape[-2] for x in self.forward(torch.zeros(1, ch, s, s))])  # forward\n",
        "            m.anchors /= m.stride.view(-1, 1, 1)\n",
        "            check_anchor_order(m)\n",
        "            self.stride = m.stride\n",
        "            self._initialize_biases()  # only run once\n",
        "            # logger.info('Strides: %s' % m.stride.tolist())\n",
        "\n",
        "        # Init weights, biases\n",
        "        initialize_weights(self)\n",
        "        self.info()\n",
        "        logger.info('')\n",
        "\n",
        "    def forward(self, x, augment=False, profile=False):\n",
        "        if augment:\n",
        "            return self.forward_augment(x)  # augmented inference, None\n",
        "        else:\n",
        "            return self.forward_once(x, profile)  # single-scale inference, train\n",
        "\n",
        "    def forward_augment(self, x):\n",
        "        img_size = x.shape[-2:]  # height, width\n",
        "        s = [1, 0.83, 0.67]  # scales\n",
        "        f = [None, 3, None]  # flips (2-ud, 3-lr)\n",
        "        y = []  # outputs\n",
        "        for si, fi in zip(s, f):\n",
        "            xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))\n",
        "            yi = self.forward_once(xi)[0]  # forward\n",
        "            # cv2.imwrite(f'img_{si}.jpg', 255 * xi[0].cpu().numpy().transpose((1, 2, 0))[:, :, ::-1])  # save\n",
        "            yi = self._descale_pred(yi, fi, si, img_size)\n",
        "            y.append(yi)\n",
        "        return torch.cat(y, 1), None  # augmented inference, train\n",
        "\n",
        "    def forward_once(self, x, profile=False):\n",
        "        y, dt = [], []  # outputs\n",
        "        for m in self.model:\n",
        "            if m.f != -1:  # if not from previous layer\n",
        "                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # from earlier layers\n",
        "\n",
        "            if profile:\n",
        "                o = thop.profile(m, inputs=(x,), verbose=False)[0] / 1E9 * 2 if thop else 0  # FLOPS\n",
        "                t = time_synchronized()\n",
        "                for _ in range(10):\n",
        "                    _ = m(x)\n",
        "                dt.append((time_synchronized() - t) * 100)\n",
        "                if m == self.model[0]:\n",
        "                    logger.info(f\"{'time (ms)':>10s} {'GFLOPS':>10s} {'params':>10s}  {'module'}\")\n",
        "                logger.info(f'{dt[-1]:10.2f} {o:10.2f} {m.np:10.0f}  {m.type}')\n",
        "\n",
        "            x = m(x)  # run\n",
        "            y.append(x if m.i in self.save else None)  # save output\n",
        "\n",
        "        if profile:\n",
        "            logger.info('%.1fms total' % sum(dt))\n",
        "        return x\n",
        "\n",
        "    def _descale_pred(self, p, flips, scale, img_size):\n",
        "        # de-scale predictions following augmented inference (inverse operation)\n",
        "        if self.inplace:\n",
        "            p[..., :4] /= scale  # de-scale\n",
        "            if flips == 2:\n",
        "                p[..., 1] = img_size[0] - p[..., 1]  # de-flip ud\n",
        "            elif flips == 3:\n",
        "                p[..., 0] = img_size[1] - p[..., 0]  # de-flip lr\n",
        "        else:\n",
        "            x, y, wh = p[..., 0:1] / scale, p[..., 1:2] / scale, p[..., 2:4] / scale  # de-scale\n",
        "            if flips == 2:\n",
        "                y = img_size[0] - y  # de-flip ud\n",
        "            elif flips == 3:\n",
        "                x = img_size[1] - x  # de-flip lr\n",
        "            p = torch.cat((x, y, wh, p[..., 4:]), -1)\n",
        "        return p\n",
        "\n",
        "    def _initialize_biases(self, cf=None):  # initialize biases into Detect(), cf is class frequency\n",
        "        # https://arxiv.org/abs/1708.02002 section 3.3\n",
        "        # cf = torch.bincount(torch.tensor(np.concatenate(dataset.labels, 0)[:, 0]).long(), minlength=nc) + 1.\n",
        "        m = self.model[-1]  # Detect() module\n",
        "        for mi, s in zip(m.m, m.stride):  # from\n",
        "            b = mi.bias.view(m.na, -1)  # conv.bias(255) to (3,85)\n",
        "            b.data[:, 4] += math.log(8 / (640 / s) ** 2)  # obj (8 objects per 640 image)\n",
        "            b.data[:, 5:] += math.log(0.6 / (m.nc - 0.99)) if cf is None else torch.log(cf / cf.sum())  # cls\n",
        "            mi.bias = torch.nn.Parameter(b.view(-1), requires_grad=True)\n",
        "\n",
        "    def _print_biases(self):\n",
        "        m = self.model[-1]  # Detect() module\n",
        "        for mi in m.m:  # from\n",
        "            b = mi.bias.detach().view(m.na, -1).T  # conv.bias(255) to (3,85)\n",
        "            logger.info(\n",
        "                ('%6g Conv2d.bias:' + '%10.3g' * 6) % (mi.weight.shape[1], *b[:5].mean(1).tolist(), b[5:].mean()))\n",
        "\n",
        "    # def _print_weights(self):\n",
        "    #     for m in self.model.modules():\n",
        "    #         if type(m) is Bottleneck:\n",
        "    #             logger.info('%10.3g' % (m.w.detach().sigmoid() * 2))  # shortcut weights\n",
        "\n",
        "    def fuse(self):  # fuse model Conv2d() + BatchNorm2d() layers\n",
        "        logger.info('Fusing layers... ')\n",
        "        for m in self.model.modules():\n",
        "            if type(m) is Conv and hasattr(m, 'bn'):\n",
        "                m.conv = fuse_conv_and_bn(m.conv, m.bn)  # update conv\n",
        "                delattr(m, 'bn')  # remove batchnorm\n",
        "                m.forward = m.fuseforward  # update forward\n",
        "        self.info()\n",
        "        return self\n",
        "\n",
        "    def nms(self, mode=True):  # add or remove NMS module\n",
        "        present = type(self.model[-1]) is NMS  # last layer is NMS\n",
        "        if mode and not present:\n",
        "            logger.info('Adding NMS... ')\n",
        "            m = NMS()  # module\n",
        "            m.f = -1  # from\n",
        "            m.i = self.model[-1].i + 1  # index\n",
        "            self.model.add_module(name='%s' % m.i, module=m)  # add\n",
        "            self.eval()\n",
        "        elif not mode and present:\n",
        "            logger.info('Removing NMS... ')\n",
        "            self.model = self.model[:-1]  # remove\n",
        "        return self\n",
        "\n",
        "    def autoshape(self):  # add AutoShape module\n",
        "        logger.info('Adding AutoShape... ')\n",
        "        m = AutoShape(self)  # wrap model\n",
        "        copy_attr(m, self, include=('yaml', 'nc', 'hyp', 'names', 'stride'), exclude=())  # copy attributes\n",
        "        return m\n",
        "\n",
        "    def info(self, verbose=False, img_size=640):  # print model information\n",
        "        model_info(self, verbose, img_size)\n",
        "\n",
        "\n",
        "def parse_model(d, ch):  # model_dict, input_channels(3)\n",
        "    logger.info('\\n%3s%18s%3s%10s  %-40s%-30s' % ('', 'from', 'n', 'params', 'module', 'arguments'))\n",
        "    anchors, nc, gd, gw = d['anchors'], d['nc'], d['depth_multiple'], d['width_multiple']\n",
        "    na = (len(anchors[0]) // 2) if isinstance(anchors, list) else anchors  # number of anchors\n",
        "    no = na * (nc + 5)  # number of outputs = anchors * (classes + 5)\n",
        "\n",
        "    layers, save, c2 = [], [], ch[-1]  # layers, savelist, ch out\n",
        "    for i, (f, n, m, args) in enumerate(d['backbone'] + d['head']):  # from, number, module, args\n",
        "        m = eval(m) if isinstance(m, str) else m  # eval strings\n",
        "        for j, a in enumerate(args):\n",
        "            try:\n",
        "                args[j] = eval(a) if isinstance(a, str) else a  # eval strings\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        n = max(round(n * gd), 1) if n > 1 else n  # depth gain\n",
        "        if m in [Conv, GhostConv, Bottleneck, GhostBottleneck, SPP, DWConv, MixConv2d, Focus, CrossConv, BottleneckCSP,\n",
        "                 C3, C3TR]:\n",
        "            c1, c2 = ch[f], args[0]\n",
        "            if c2 != no:  # if not output\n",
        "                c2 = make_divisible(c2 * gw, 8)\n",
        "\n",
        "            args = [c1, c2, *args[1:]]\n",
        "            if m in [BottleneckCSP, C3, C3TR]:\n",
        "                args.insert(2, n)  # number of repeats\n",
        "                n = 1\n",
        "        elif m is nn.BatchNorm2d:\n",
        "            args = [ch[f]]\n",
        "        elif m is Concat:\n",
        "            c2 = sum([ch[x] for x in f])\n",
        "        elif m is Detect:\n",
        "            args.append([ch[x] for x in f])\n",
        "            if isinstance(args[1], int):  # number of anchors\n",
        "                args[1] = [list(range(args[1] * 2))] * len(f)\n",
        "        elif m is Contract:\n",
        "            c2 = ch[f] * args[0] ** 2\n",
        "        elif m is Expand:\n",
        "            c2 = ch[f] // args[0] ** 2\n",
        "        else:\n",
        "            c2 = ch[f]\n",
        "\n",
        "        m_ = nn.Sequential(*[m(*args) for _ in range(n)]) if n > 1 else m(*args)  # module\n",
        "        t = str(m)[8:-2].replace('__main__.', '')  # module type\n",
        "        np = sum([x.numel() for x in m_.parameters()])  # number params\n",
        "        m_.i, m_.f, m_.type, m_.np = i, f, t, np  # attach index, 'from' index, type, number params\n",
        "        logger.info('%3s%18s%3s%10.0f  %-40s%-30s' % (i, f, n, np, t, args))  # print\n",
        "        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # append to savelist\n",
        "        layers.append(m_)\n",
        "        if i == 0:\n",
        "            ch = []\n",
        "        ch.append(c2)\n",
        "    return nn.Sequential(*layers), sorted(save)\n",
        "\n",
        "\n",
        "# if __name__ == '__main__':\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     parser.add_argument('--cfg', type=str, default='yolov5s.yaml', help='model.yaml')\n",
        "#     parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "#     opt = parser.parse_args()\n",
        "#     opt.cfg = check_file(opt.cfg)  # check file\n",
        "#     set_logging()\n",
        "#     device = select_device(opt.device)\n",
        "\n",
        "#     # Create model\n",
        "#     model = Model(opt.cfg).to(device)\n",
        "#     model.train()\n",
        "\n",
        "#     # Profile\n",
        "#     # img = torch.rand(8 if torch.cuda.is_available() else 1, 3, 320, 320).to(device)\n",
        "#     # y = model(img, profile=True)\n",
        "\n",
        "#     # Tensorboard (not working https://github.com/ultralytics/yolov5/issues/2898)\n",
        "#     # from torch.utils.tensorboard import SummaryWriter\n",
        "#     # tb_writer = SummaryWriter('.')\n",
        "#     # logger.info(\"Run 'tensorboard --logdir=models' to view tensorboard at http://localhost:6006/\")\n",
        "#     # tb_writer.add_graph(torch.jit.trace(model, img, strict=False), [])  # add model graph\n",
        "#     # tb_writer.add_image('test', img[0], dataformats='CWH')  # add model to tensorboard\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dpRfCsqSE5xS"
      },
      "source": [
        "# Detector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUZRWNWQEPM9",
        "outputId": "3471dee3-34ff-4ac3-abff-bff4ce5b0305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31m\u001b[1mrequirements:\u001b[0m requirements.txt not found, check failed.\n"
          ]
        }
      ],
      "source": [
        "import argparse\n",
        "import time\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "# from models.experimental import attempt_load\n",
        "# from utils.datasets import letterbox\n",
        "# from utils.general import check_img_size, check_requirements, non_max_suppression, scale_coords\n",
        "# from utils.plots import colors, plot_one_box\n",
        "# from utils.torch_utils import select_device\n",
        "\n",
        "\n",
        "parser = argparse.ArgumentParser()\n",
        "parser.add_argument('--weights', nargs='+', type=str, default='weights\\Fruits.pt', help='model.pt path(s)')\n",
        "parser.add_argument('--img-size', type=int, default=640, help='inference size (pixels)')\n",
        "parser.add_argument('--conf-thres', type=float, default=0.25, help='object confidence threshold')\n",
        "parser.add_argument('--iou-thres', type=float, default=0.45, help='IOU threshold for NMS')\n",
        "parser.add_argument('--device', default='', help='cuda device, i.e. 0 or 0,1,2,3 or cpu')\n",
        "parser.add_argument('--save-conf', action='store_true', help='save confidences in --save-txt labels')\n",
        "parser.add_argument('--classes', nargs='+', type=int, help='filter by class: --class 0, or --class 0 2 3')\n",
        "parser.add_argument('--agnostic-nms', action='store_true', help='class-agnostic NMS')\n",
        "parser.add_argument('--project', default='runs/detect', help='save results to project/name')\n",
        "parser.add_argument('--name', default='exp', help='save results to project/name')\n",
        "parser.add_argument('--hide-conf', default=False, action='store_true', help='hide confidences')\n",
        "args = parser.parse_args(args=[])\n",
        "\n",
        "check_requirements(exclude=('tensorboard', 'pycocotools', 'thop'))\n",
        "\n",
        "\n",
        "class Detector:\n",
        "    def __init__(self):\n",
        "        weights, imgsz = args.weights, args.img_size\n",
        "        self.save_dir = 'output'\n",
        "        \n",
        "        # Load model\n",
        "        self.device = select_device(args.device)\n",
        "        self.model = attempt_load(weights, map_location=self.device)  # load FP32 model\n",
        "        stride = int(self.model.stride.max())  # model stride\n",
        "        imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
        "        self.names = self.model.module.names if hasattr(self.model, 'module') else self.model.names  # get class names\n",
        "    \n",
        "        if self.device.type != 'cpu':\n",
        "            self.model(torch.zeros(1, 3, imgsz, imgsz).to(self.device).type_as(next(self.model.parameters())))  # run once\n",
        "        \n",
        "    def detect(self, img):\n",
        "        # img = cv2.imread(file_name)\n",
        "\n",
        "        # file_name = Path(file_name).name\n",
        "        original_image = img.copy()\n",
        "\n",
        "        t0 = time.time()\n",
        "        \n",
        "        # img = cv2.resize(img, (416, 416))\n",
        "        img = letterbox(img)[0]\n",
        "        img = img[:, :, ::-1].transpose(2, 0, 1)  # BGR to RGB, to 3x416x416\n",
        "        img = np.ascontiguousarray(img)\n",
        "        img = torch.from_numpy(img).to(self.device)\n",
        "        img = img.float()  # uint8 to fp16/32\n",
        "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
        "        img = img.unsqueeze(0)\n",
        "\n",
        "        # Inference\n",
        "        pred = self.model(img, augment=False)[0]\n",
        "\n",
        "        # Apply NMS\n",
        "        pred = non_max_suppression(pred, args.conf_thres, args.iou_thres, classes=args.classes, agnostic=args.agnostic_nms)\n",
        "\n",
        "        # save_path = os.path.join(self.save_dir, file_name)   \n",
        "        # Process detections\n",
        "        for det in pred:  # detections per image\n",
        "            if len(det):\n",
        "                # Rescale boxes from img size to original_image size\n",
        "                det[:, :4] = scale_coords(img.shape[2:], det[:, :4], original_image.shape).round()\n",
        "\n",
        "                # Write results\n",
        "                for *xyxy, conf, cls in reversed(det):\n",
        "                    c = int(cls)  # integer class\n",
        "                    label = (self.names[c] if args.hide_conf else f'{self.names[c]} {conf:.2f}')\n",
        "                    plot_one_box(xyxy, original_image, label=label, color=colors(c, True), line_thickness=2)\n",
        "                        \n",
        "        # Save results (image with detections)\n",
        "        # cv2.imwrite(save_path, original_image)\n",
        "        print(f'Done. ({time.time() - t0:.3f}s)')\n",
        "        return original_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rl-7bm3bFawk",
        "outputId": "95bbb841-cb9d-4166-e3dd-a4a19891ebff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 519
        },
        "id": "JG3TvlwGH6_O",
        "outputId": "4bcfa4c6-e06f-46e6-a12d-f6ada9d69c4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "downloaded\n",
            "/content/drive/MyDrive/Fruits.pt\n"
          ]
        },
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/drive/MyDrive/Fruits.pt'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mC:\\Users\\TEOSHI~1\\AppData\\Local\\Temp/ipykernel_14024/2030453290.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mD\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[1;32mC:\\Users\\TEOSHI~1\\AppData\\Local\\Temp/ipykernel_14024/3873986684.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     39\u001b[0m         \u001b[1;31m# Load model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_device\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 41\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattempt_load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# load FP32 model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     42\u001b[0m         \u001b[0mstride\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# model stride\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m         \u001b[0mimgsz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_img_size\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimgsz\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstride\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# check img_size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mC:\\Users\\TEOSHI~1\\AppData\\Local\\Temp/ipykernel_14024/1024702148.py\u001b[0m in \u001b[0;36mattempt_load\u001b[1;34m(weights, map_location, inplace)\u001b[0m\n\u001b[0;32m    118\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"downloaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    119\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mckpt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mw\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmap_location\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# load\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"loaded\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    122\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'ema'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mckpt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ema'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m'model'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfuse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# FP32 model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Teo Shi Han\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[0;32m    789\u001b[0m         \u001b[0mpickle_load_args\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'encoding'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 791\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    792\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_is_zipfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    793\u001b[0m             \u001b[1;31m# The zipfile reader is going to advance the current file position.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Teo Shi Han\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m_open_file_like\u001b[1;34m(name_or_buffer, mode)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_open_file_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_is_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 271\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    272\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    273\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;34m'w'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\Teo Shi Han\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\serialization.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode)\u001b[0m\n\u001b[0;32m    250\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_opener\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    251\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 252\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    253\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/Fruits.pt'"
          ]
        }
      ],
      "source": [
        "D = Detector()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "MhRFUJjUBdbE",
        "HOJUWzzSBrHU",
        "AGu64KHTCjZz",
        "E6e2KP4jDG7T",
        "JMa18JJCDuPy",
        "lbzmYJzsD7Dr",
        "Dwk9cs_dEBfp",
        "nuJguHgavW04"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
